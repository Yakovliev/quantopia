{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1ac570",
   "metadata": {},
   "source": [
    "# Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "\n",
    "Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are both statistical tools used for **model selection**. They help you choose the best model from a set of candidate models by balancing two competing goals: **goodness of fit** and **model complexity**.\n",
    "\n",
    "Both criteria are particularly useful when comparing models that may have different numbers of parameters, as they penalize models for being more complex to prevent **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbf17b",
   "metadata": {},
   "source": [
    "## Akaike Information Criterion (AIC)\n",
    "\n",
    "AIC is an estimator of prediction error and is rooted in information theory. It estimates the relative amount of information a model loses when representing the process that generated the data. The model with the lowest AIC value is considered the best among the candidate models. \n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$AIC = -2 \\ln(\\hat{L}) + 2k$$\n",
    "\n",
    "* **$-2 \\ln(\\hat{L})$**: This is the **goodness-of-fit term**. It's derived from the maximum likelihood ($\\hat{L}$) of the model, which measures how well the model fits the data. A higher likelihood (and thus a smaller negative log-likelihood) indicates a better fit.\n",
    "* **$2k$**: This is the **penalty term**. It's a penalty for model complexity, where $k$ is the total number of parameters estimated by the model. A more complex model (one with more parameters) gets a higher penalty.\n",
    "\n",
    "$k$ is equal to the number of structural parameters in the model's function $f(x_i, \\beta)$ (e.g., the $\\beta$ coefficients in a regression model) if the error variance $\\sigma^2$ is known. However, if the error variance $\\sigma^2$ is unknown, it must be estimated from the data, which means it counts as an additional estimated parameter and $k = p + 1$. We will review this in detail below.\n",
    "\n",
    "The AIC's goal is to find the model that best approximates the unknown data-generating process, even if that process isn't one of the candidate models. It's focused on **predictive accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3b3de",
   "metadata": {},
   "source": [
    "## Bayesian Information Criterion (BIC)\n",
    "\n",
    "BIC, also known as the Schwarz Information Criterion (SIC), is a criterion for model selection derived from a Bayesian perspective. Like AIC, it balances goodness of fit and complexity, but it does so more aggressively. The model with the lowest BIC value is the one preferred. \n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$BIC = -2 \\ln(\\hat{L}) + k \\ln(n)$$\n",
    "\n",
    "* **$-2 \\ln(\\hat{L})$**: This is the same goodness-of-fit term as in AIC.\n",
    "* **$k \\ln(n)$**: This is the **penalty term**. The penalty for complexity is stronger than AIC's because it includes the natural logarithm of the number of data points ($n$).\n",
    "\n",
    "Because of the $\\ln(n)$ factor, BIC applies a much heavier penalty for additional parameters, especially as the sample size grows. This means BIC tends to favor **simpler models** more strongly than AIC. It assumes that one of the candidate models is the \"true\" model, and its goal is to find that true model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df0282",
   "metadata": {},
   "source": [
    "### Key Differences and When to Use Which\n",
    "\n",
    "| Feature | Akaike Information Criterion (AIC) | Bayesian Information Criterion (BIC) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Penalty for Complexity** | Penalizes with $2k$. | Penalizes with $k \\ln(n)$. |\n",
    "| **Focus** | Finds the best **approximating** model for predictive accuracy. | Finds the \"true\" model. |\n",
    "| **Sample Size** | The penalty is constant with respect to sample size. | The penalty increases with sample size, favoring simpler models. |\n",
    "| **Behavior** | Tends to select more complex models than BIC. | Tends to select more simpler models than AIC. |\n",
    "\n",
    "NOTE: The terms \"approximating model\" and \"true model\" refer to their underlying assumptions about the reality you are trying to model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6982e55e",
   "metadata": {},
   "source": [
    "**AIC assumes that the \"true\" data-generating process is infinitely complex and therefore unknowable.** We can only ever hope to approximate it with our models.\n",
    "\n",
    "* **Goal**: AIC's goal is to find the model that provides the best trade-off between bias (how far your model is from the true process) and variance (how much your model would change with different data). In simpler terms, it's about building the model that will give you the most accurate predictions on a new, unseen dataset.\n",
    "* **Best for**: This makes AIC ideal for tasks where **prediction** is the primary goal, such as forecasting future stock prices, predicting customer behavior, or machine learning applications. You're not trying to discover the fundamental laws of the universe, you're just trying to make the most useful prediction you can.\n",
    "* **Mathematical Justification**: AIC's penalty term ($2k$) is derived from the Kullback-Leibler (KL) divergence, which is a measure of the information lost when approximating reality with a given model. By minimizing AIC, you are minimizing this information loss.\n",
    "\n",
    "**BIC assumes that there is a \"true\" model that generated the data, and this model exists within your set of candidate models.** This is often a good assumption for scientific fields where we believe there are underlying, fixed physical laws governing a system.\n",
    "\n",
    "* **Goal**: BIC's goal is to select the model that is most likely to be this \"true\" model. Because of its heavier penalty for more parameters ($k\\ln(n)$), BIC is more likely to choose simpler, more parsimonious models. It essentially bets that the simplest model that explains the data well is the correct one.\n",
    "* **Best for**: This makes BIC more suitable for tasks of **explanation and discovery**, where you want to find the most fundamental, elegant, and concise model that describes a phenomenon. This is common in fields like physics, biology, and some social sciences.\n",
    "* **Mathematical Justification**: BIC is derived from Bayesian inference and is an approximation of the posterior probability of a model being the true model, given the data. By minimizing BIC, you are maximizing this posterior probability.\n",
    "\n",
    "In essence:\n",
    "\n",
    "* **AIC** asks: \"Which model will give me the most accurate predictions for the future?\"\n",
    "* **BIC** asks: \"Which model is most likely to be the true explanation for the data I have?\"\n",
    "\n",
    "This is why AIC often chooses a slightly more complex model than BIC. AIC is willing to accept a little extra complexity if it improves predictive accuracy, while BIC is more conservative, preferring a simpler model unless the evidence for a more complex one is overwhelming.\n",
    "\n",
    "In practice, if your goal is to find the model with the highest predictive power, AIC is often preferred. If your goal is to select the most efficient or fundamental model that is likely to be the \"true\" one, BIC might be a better choice.\n",
    "\n",
    "When you compare two models, you find AIC Difference $\\Delta \\text{AIC}$ and BIC Difference $\\Delta \\text{BIC}$ for these modesl. Here are some general guidelines for interpreting these differences, based on conventions in the scientific community:\n",
    "\n",
    "* **AIC Difference ($\\Delta \\text{AIC}$)**: A difference of more than 2 between two models is often considered significant. If the difference is between 0 and 2, the models are considered to have a similar level of support from the data. A model with a $\\Delta$AIC of 10 or more is considered to have very little support compared to the best model.\n",
    "* **BIC Difference ($\\Delta \\text{BIC}$)**: Because BIC has a stronger penalty for complexity, its differences are interpreted more stringently. A difference of 0-2 is considered as weak evidence for the model with the lower BIC, a difference of 2-6 is considered positive evidence, a difference of 6-10 is strong evidence, and a difference of more than 10 is considered very strong evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55128a",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a powerful and flexible method for estimating the parameters of a statistical model. It's used to find the parameter values that make the observed data most probable. The **likelihood function** quantifies how probable the observed data is for a given set of parameters. The **log-likelihood function** is simply the natural logarithm of the likelihood function.\n",
    "\n",
    "The core idea of MLE is to find the parameter values that maximize the likelihood function, $L(\\theta|x)$, where $\\theta$ represents the parameters and $x$ is the data. The likelihood function is often a product of probability density functions (or probability mass functions) for each data point. For a set of independent and identically distributed (i.i.d.) observations, the likelihood is:\n",
    "\n",
    "$$L(\\theta|x) = \\prod_{i=1}^{n} f(x_i|\\theta)$$\n",
    "\n",
    "Working with this product can be difficult, especially with many data points, as multiplying many small numbers can lead to computational underflow. This is where the **log-likelihood** comes in. By taking the natural logarithm, the product is transformed into a sum, which is much more computationally stable and easier to differentiate.\n",
    "\n",
    "$$\\ln L(\\theta|x) = \\ln \\left( \\prod_{i=1}^{n} f(x_i|\\theta) \\right) = \\sum_{i=1}^{n} \\ln f(x_i|\\theta)$$\n",
    "\n",
    "Since the logarithm is a monotonic function, maximizing the log-likelihood function yields the **exact same parameter estimates** as maximizing the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d713e40",
   "metadata": {},
   "source": [
    "The connection between Maximum Likelihood and regression modeling becomes clear when we make assumptions about the distribution of the errors. While Ordinary Least Squares (OLS) regression minimizes the sum of squared residuals, MLE provides a more general framework that can lead to the same result under specific conditions.\n",
    "\n",
    "**For OLS:**\n",
    "The OLS method finds the coefficients that minimize the **sum of squared residuals** (or errors). That is, it minimizes the following:\n",
    "\n",
    "$$\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1x_i))^2$$\n",
    "\n",
    "**Relation to MLE:**\n",
    "The OLS solution is the Maximum Likelihood Estimate for a linear regression model **if we assume that the error terms ($\\epsilon_i$) are independently and identically distributed (i.i.d.) according to a normal distribution with a mean of zero and a constant variance ($\\sigma^2$).**\n",
    "\n",
    "When we make this assumption, the probability of observing a particular data point $y_i$ for a given $x_i$ is a normal distribution centered at the predicted value, $\\beta_0 + \\beta_1x_i$. The log-likelihood function for this model can be written as:\n",
    "\n",
    "$$\\ln L(\\beta_0, \\beta_1, \\sigma^2|x,y) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1x_i))^2$$\n",
    "\n",
    "NOTE: We will derive a similar formula below but in a bit more general form.\n",
    "\n",
    "Notice the last term in the equation. To maximize the log-likelihood function, we need to maximize this entire expression. The first term is a constant with respect to the $\\beta$ coefficients, and the second term is negative. Therefore, to make the overall value as large as possible, we must make the negative term as small as possible. This is equivalent to minimizing the **sum of squared residuals**, which is exactly what OLS does.\n",
    "\n",
    "**Why is this important?** This connection shows that OLS is not just an arbitrary method for fitting a line; it has a probabilistic foundation under the assumption of normally distributed errors. For other types of regression, such as logistic regression, there is no OLS equivalent. Instead, the parameters are always estimated using Maximum Likelihood, where the log-likelihood function is based on the appropriate distribution for the outcome variable (e.g., a Bernoulli distribution for binary outcomes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501ba8e",
   "metadata": {},
   "source": [
    "## Derivation of the Log-Likelihood for Least Squares\n",
    "\n",
    "NOTE: $\\ln(L)$ denotes log-likelihood function. $\\ln(\\hat{L})$ denotes the specific value of the log-likelihood function after the model has been fitted to the data and this specific value is maximized log-likelihood.\n",
    "\n",
    "Here is the derivation (note that we derive all the formulas in this section under the assumption that we know exact values of $\\sigma_i$ or $\\sigma$):\n",
    "\n",
    "1.  **Start with the probability of one data point.** We assume that the probability of observing a single data point $(x_i, y_i)$ given the model's prediction $f(x_i, \\beta)$ is described by a normal distribution with a mean of $f(x_i, \\beta)$ and a standard deviation of $\\sigma_i$. This is equivalent to the assumption that the error terms $\\epsilon_i$ are independently distributed according to a normal distribution with a mean of zero and a variance $\\sigma_i^2$, so $\\epsilon_i \\sim N(0, \\sigma_i^2)$ (remember that $y_i = f(x_i, \\beta) + \\epsilon_i$). The probability density function (PDF) for a single point is:\n",
    "\n",
    "    $$p(y_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{(y_i - f(x_i, \\beta))^2}{2\\sigma_i^2}}$$\n",
    "\n",
    "2.  **Form the likelihood function.** Assuming each data point is an independent measurement, the total likelihood ($L$) of observing all $n$ data points is the product of their individual probabilities:\n",
    "\n",
    "    $$L = \\prod_{i=1}^{n} p(y_i) = \\prod_{i=1}^{n} \\left( \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{(y_i - f(x_i, \\beta))^2}{2\\sigma_i^2}} \\right)$$\n",
    "\n",
    "3.  **Take the natural logarithm.** To simplify the product and make the calculations more manageable, we take the natural logarithm of the likelihood function. A product becomes a sum, and the exponential term simplifies.\n",
    "\n",
    "    $$\\ln(L) = \\ln \\left( \\prod_{i=1}^{n} p(y_i) \\right) = \\sum_{i=1}^{n} \\ln(p(y_i))$$   \n",
    "    \n",
    "    $$\\ln(L) = \\sum_{i=1}^{n} \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{(y_i - f(x_i, \\beta))^2}{2\\sigma_i^2}} \\right)$$\n",
    "\n",
    "4.  **Simplify the expression.** Using logarithm rules ($\\ln(a \\cdot b) = \\ln(a) + \\ln(b)$ and $\\ln(e^x) = x$), we can expand the sum:\n",
    "\n",
    "    $$\\ln(L) = \\sum_{i=1}^{n} \\left( \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\right) - \\frac{(y_i - f(x_i, \\beta))^2}{2\\sigma_i^2} \\right)$$   \n",
    "    \n",
    "    $$\\ln(L) = - \\sum_{i=1}^{n} \\ln(\\sqrt{2\\pi\\sigma_i^2}) - \\frac{1}{2} \\sum_{i=1}^{n} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma_i^2}$$\n",
    "\n",
    "5. The second term in this equation is directly related to our Chi-Squared ($\\chi^2$) statistic. Since $\\chi^2 = \\sum_{i=1}^{N} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma_i^2}$, we can rewrite the equation as:\n",
    "\n",
    "    $$\\ln(L) = - \\frac{1}{2}\\chi^2 - \\sum_{i=1}^{n} \\ln(\\sqrt{2\\pi\\sigma_i^2})$$\n",
    "\n",
    "6. When comparing different models on the same dataset, the second term, which depends on the known measurement uncertainties ($\\sigma_i$), is a constant and doesn't affect which model is selected. Therefore, for our purposes, we can write a simplified and more practical relationship:\n",
    "\n",
    "    $$\\ln(L) \\propto - \\frac{1}{2}\\chi^2$$\n",
    "\n",
    "    This means that maximizing the log-likelihood is equivalent to **minimizing the chi-squared statistic**.\n",
    "\n",
    "7.  If $\\sigma_i = \\sigma$ for all $i$ (assumption of homoscedasticity), we can simplify this expression even more:\n",
    "\n",
    "    $$\\ln(L) = - \\sum_{i=1}^{n} \\ln(\\sqrt{2\\pi\\sigma^2}) - \\frac{1}{2} \\sum_{i=1}^{n} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma^2}$$\n",
    "\n",
    "    $$\\ln(L) = - n \\ln(\\sqrt{2\\pi\\sigma^2}) - \\frac{1}{2} \\sum_{i=1}^{n} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma^2}$$\n",
    "\n",
    "    $$\\ln(L) = - \\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2} \\sum_{i=1}^{n} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma^2}$$\n",
    "\n",
    "    $$\\ln(L) = - \\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2} \\sum_{i=1}^{n} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma^2}$$\n",
    "\n",
    "    $$\\ln(L) = - \\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2} \\chi^2$$\n",
    "\n",
    "### How to Use This for AIC and BIC\n",
    "\n",
    "Since the AIC and BIC formulas depend on the maximized log-likelihood $-2\\ln(\\hat{L})$, we can use the results of our derivation. When a fitting procedure like `curve_fit` finds the optimal parameters $\\hat{\\beta}$, it does so by minimizing the $\\chi^2$ statistic. This is the best-fit of $\\chi^2$. Let's call this minimized value $\\chi^2_{min}$.\n",
    "\n",
    "Because maximizing the log-likelihood is equivalent to minimizing the chi-squared statistic, the maximized log-likelihood, $\\ln(\\hat{L})$, is given by:\n",
    "\n",
    "$$\\ln(\\hat{L}) \\propto - \\frac{1}{2}\\chi^2_{min}$$\n",
    "\n",
    "Now we can calculate the information criteria using this **minimized** $\\chi^2$ value from the fit:\n",
    "\n",
    "* **For AIC:**\n",
    "\n",
    "    $$-2\\ln(\\hat{L}) = -2 \\left( - \\frac{1}{2}\\chi^2_{min} + \\text{constant} \\right) = \\chi^2_{min} + \\text{constant}'$$\n",
    "\n",
    "    Since the constants don't affect model ranking, we can simply use the proportional formula:\n",
    "\n",
    "    $$AIC \\propto \\chi^2_{min} + 2k$$\n",
    "\n",
    "    NOTE: This last formula is valid for model comparison only (not valid to calculate the exact value of AIC). Also, this last formula is valid only if $\\sigma_i$ values are known.\n",
    "\n",
    "* **For BIC:**\n",
    "\n",
    "    $$BIC \\propto \\chi^2_{min} + k\\ln(n)$$\n",
    "\n",
    "    NOTE: This last formula is valid for model comparison only (not valid to calculate the exact value of AIC). Also, this last formula is valid only if $\\sigma_i$ values are known.\n",
    "\n",
    "Remember: $\\chi^2_{min}$ is the final chi-squared value calculated from the Weighted Least Squares, $k$ is the number of parameters, and $n$ is the number of data points. By calculating these values for different models fitted to the same data, you can choose the model with the lowest AIC or BIC as the one that provides the best balance of fit and parsimony.\n",
    "\n",
    "If the standard deviations $\\sigma_i$ are the same for all the data points and are equal to $\\sigma$ (homoscedasticity), then the maximized log-likelihood to be used in the AIC and BIC formulas is:\n",
    "\n",
    "$$\\ln(\\hat{L}) = - \\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2} \\chi^2_{min}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88af61",
   "metadata": {},
   "source": [
    "## The Case of Unknown Variance: Ordinary Least Squares (OLS)\n",
    "\n",
    "In the previous chapter, we operated under the assumption that the standard deviations of the errors, $\\sigma_i$, were known. This is common in the physical sciences where measurement uncertainties can be well-defined. However, in many other fields, these uncertainties are not known beforehand.\n",
    "\n",
    "The most common scenario is to assume that while the variance is unknown, it is constant for all data points. This is the assumption of **homoscedasticity** ($\\sigma_i = \\sigma$ for all *i*), which is fundamental to **Ordinary Least Squares (OLS)**. When $\\sigma$ is unknown, it becomes another parameter that we must estimate from the data.\n",
    "\n",
    "### Estimating the Unknown Variance $\\sigma^2$\n",
    "\n",
    "Our goal is to maximize the log-likelihood function with respect to all model parameters, which now include the $\\beta$ parameters *and* the unknown variance $\\sigma^2$.\n",
    "\n",
    "We start with the log-likelihood function for the homoscedastic case, as derived in Step 7 of the previous chapter:\n",
    "\n",
    "$$\\ln(L(\\beta, \\sigma^2)) = - \\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - f(x_i, \\beta))^2$$\n",
    "\n",
    "This expression is often written in terms of the **Residual Sum of Squares (RSS)**, where $RSS = \\sum_{i=1}^{n} (y_i - f(x_i, \\beta))^2$.\n",
    "\n",
    "$$\\ln(L(\\beta, \\sigma^2)) = - \\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{RSS}{2\\sigma^2}$$\n",
    "\n",
    "To find the values of $\\beta$ and $\\sigma^2$ that maximize this function, we can use a two-step process:\n",
    "\n",
    "1.  For any given value of $\\sigma^2$, maximizing the log-likelihood is equivalent to minimizing the `RSS` term. This is exactly what the standard least-squares fitting procedure does to find the optimal parameters $\\beta$.\n",
    "2.  Once the optimal $\\beta$ parameters are found and the `RSS` is minimized, we can find the **Maximum Likelihood Estimate (MLE)** for the variance, denoted as $\\hat{\\sigma}^2$, by taking the partial derivative of the log-likelihood function with respect to $\\sigma^2$ and setting it to zero.\n",
    "\n",
    "Let's perform the second step. Treating `RSS` as a constant (since $\\beta$ is now fixed), we differentiate with respect to $\\sigma^2$:\n",
    "\n",
    "$$\\frac{\\partial \\ln(L)}{\\partial \\sigma^2} = - \\frac{n}{2\\sigma^2} + \\frac{RSS}{2(\\sigma^2)^2}$$\n",
    "\n",
    "Setting the derivative to zero to find the maximum:\n",
    "\n",
    "$$\\frac{n}{2\\hat{\\sigma}^2} = \\frac{RSS}{2(\\hat{\\sigma}^2)^2}$$\n",
    "\n",
    "Solving for $\\hat{\\sigma}^2$ gives us the MLE for the variance:\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{RSS}{n}$$\n",
    "\n",
    "> This is a fundamentally important result: the maximum likelihood estimate for the unknown variance is the residual sum of squares divided by the number of data points.\n",
    "\n",
    "There are several important notes regarding this results:\n",
    "1. The Maximum Likelihood Estimate (MLE) for the variance, $\\hat{\\sigma}^2 = RSS/n$, is a **biased estimator**.\n",
    "2. The unbiased estimator, $\\hat{\\sigma}^2 = RSS/(n-p)$, is what is typically used for inference and is reported in most statistical software as the \"Mean Squared Error\" (MSE). Here, $p$ is the number of model parameters.\n",
    "3. **For calculating the log-likelihood and the information criteria (AIC, BIC) derived from it, we MUST use the biased Maximum Likelihood Estimate ($RSS/n$).**\n",
    "\n",
    "Let's review the last statement in more details.\n",
    "\n",
    "Our primary goal was to find the parameter values that maximize the log-likelihood function. We found that the value of $\\sigma^2$ that maximizes this function (after `RSS` has been minimized by finding the best $\\hat{\\beta}$ parameters) is:\n",
    "\n",
    "$$\\hat{\\sigma}^2_{MLE} = \\frac{RSS}{n} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - f(x_i, \\hat{\\beta}))^2$$\n",
    "\n",
    "This is the **Maximum Likelihood Estimate (MLE)** for the variance. While it is the correct value for maximizing the likelihood, it is a **biased estimator** of the true population variance $\\sigma^2$. On average, it will slightly underestimate the true variance.\n",
    "\n",
    "In applied statistics, the goal is often to get the most accurate possible estimate of the true population variance. For this, we use the **unbiased estimator**, which corrects for the bias mentioned above by adjusting the denominator. If we have estimated $k$ model parameters (i.e., the number of coefficients in $\\beta$), the unbiased estimate is:\n",
    "\n",
    "$$\\hat{\\sigma}^2_{unbiased} = \\frac{RSS}{n-p}$$\n",
    "\n",
    "This is the **Mean Squared Error (MSE)**, and its square root is the **Residual Standard Error (RSE)** reported in the output of virtually all OLS regression software. The denominator, $n-p$, represents the residual **degrees of freedom**. By dividing by a smaller number, we increase the value of the estimate, correcting for the downward bias of the MLE.\n",
    "\n",
    "**Which estimator should we use for the log-likelihood?**\n",
    "\n",
    "The answer lies in the fundamental definition of AIC and BIC: they are based on the **maximized value of the log-likelihood function**.\n",
    "\n",
    "The derivation of AIC and BIC starts with $\\ln(\\hat{L})$, which is the value of $\\ln(L)$ when *all* parameters have been set to their Maximum Likelihood Estimates. Therefore, to be mathematically consistent, we **must** use the MLE for the variance, $\\hat{\\sigma}^2_{MLE} = RSS/n$, when calculating the log-likelihood for AIC and BIC.\n",
    "\n",
    "Plugging the unbiased estimate $RSS/(n-p)$ into the log-likelihood formula would result in a value that is *not* the true maximum likelihood, and the theoretical foundation of AIC and BIC would no longer hold.\n",
    "\n",
    "**In summary:**\n",
    "*   For **calculating AIC and BIC**, use the **biased** MLE variance: $\\hat{\\sigma}^2 = RSS/n$.\n",
    "*   For **reporting the model's error variance or standard error for inference**, use the **unbiased** variance: $\\hat{\\sigma}^2 = RSS/(n-p)$.\n",
    "\n",
    "### The Maximized Log-Likelihood\n",
    "\n",
    "Now we can substitute this estimate $\\hat{\\sigma}^2$ back into the log-likelihood equation to get the *maximized* log-likelihood, $\\ln(\\hat{L})$. This value represents the highest possible likelihood given the data and the model form.\n",
    "\n",
    "$$\\ln(\\hat{L}) = - \\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln\\left(\\frac{RSS}{n}\\right) - \\frac{RSS}{2\\left(\\frac{RSS}{n}\\right)}$$\n",
    "\n",
    "Simplifying the last term:\n",
    "\n",
    "$$\\ln(\\hat{L}) = - \\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln\\left(\\frac{RSS}{n}\\right) - \\frac{n}{2}$$\n",
    "\n",
    "Using the logarithm rule $\\ln(a/b) = \\ln(a) - \\ln(b)$:\n",
    "\n",
    "$$\\ln(\\hat{L}) = - \\frac{n}{2} \\left( \\ln(2\\pi) + \\ln(RSS) - \\ln(n) + 1 \\right)$$\n",
    "\n",
    "This is the final expression for the maximized log-likelihood when $\\sigma^2$ is unknown.\n",
    "\n",
    "### Calculating AIC and BIC with Unknown Variance\n",
    "\n",
    "With the maximized log-likelihood, $\\ln(\\hat{L})$, we can now calculate the information criteria. A crucial point is that we have estimated an additional parameter: the variance $\\sigma^2$. Therefore, the total number of estimated parameters, $k$, becomes:\n",
    "\n",
    "* **Number of Parameters $k$:** If your model has $p$ parameters for the function $f(x,\\beta)$, then the total number of estimated parameters is $k = p + 1$.\n",
    "* **Example:** For instance, in a simple linear regression model $y = \\beta_0 + \\beta_1 x$, there are two $\\beta$ parameters ($p=2$). When we also estimate the variance, the total number of parameters for the AIC/BIC calculation becomes $k = p + 1 = 3$.\n",
    "\n",
    "The general formulas for AIC and BIC are:\n",
    "\n",
    "$$AIC = -2\\ln(\\hat{L}) + 2k$$\n",
    "$$BIC = -2\\ln(\\hat{L}) + k\\ln(n)$$\n",
    "\n",
    "We will use our expression for the maximized log-likelihood:\n",
    "$\\ln(\\hat{L}) = - \\frac{n}{2} \\left( \\ln(2\\pi) + \\ln(RSS) - \\ln(n) + 1 \\right)$\n",
    "\n",
    "**For AIC:**\n",
    "\n",
    "Substituting the expression for $\\ln(\\hat{L})$ into the AIC formula gives:\n",
    "\n",
    "$$AIC = -2 \\left[ - \\frac{n}{2} \\left( \\ln(2\\pi) + \\ln(RSS) - \\ln(n) + 1 \\right) \\right] + 2k$$\n",
    "$$AIC = n \\left( \\ln(2\\pi) + \\ln(RSS) - \\ln(n) + 1 \\right) + 2k$$\n",
    "\n",
    "For the purpose of comparing different models on the same dataset, any terms that do not depend on the model can be dropped. The terms $n$, $\\ln(2\\pi)$, $\\ln(n)$, and 1 are constant across all models. This leaves us with a simplified, proportional formula:\n",
    "\n",
    "$$AIC \\propto n \\ln(RSS) + 2k$$\n",
    "\n",
    "Here, $k = p + 1$ is the total number of estimated parameters (including the variance).\n",
    "\n",
    "**For BIC:**\n",
    "\n",
    "We follow the exact same logic for BIC. Substituting the expression for $\\ln(\\hat{L})$ into the BIC formula gives:\n",
    "\n",
    "$$BIC = -2 \\left[ - \\frac{n}{2} \\left( \\ln(2\\pi) + \\ln(RSS) - \\ln(n) + 1 \\right) \\right] + k\\ln(n)$$\n",
    "$$BIC = n \\left( \\ln(2\\pi) + \\ln(RSS) - \\ln(n) + 1 \\right) + k\\ln(n)$$\n",
    "\n",
    "Again, for model comparison, we can drop the same terms that are constant for a given dataset, resulting in the simplified, proportional formula for BIC:\n",
    "\n",
    "$$BIC \\propto n \\ln(RSS) + k\\ln(n)$$\n",
    "\n",
    "Here, $k = p + 1$ is the total number of estimated parameters (including the variance).\n",
    "\n",
    "It is important to remember that these simplified formulas produce values that are not the true AIC or BIC, but rather values that maintain the same differences between models as the full formulas would. Since model selection depends only on these differences (e.g., finding the model with the minimum AIC), these simplified versions are sufficient and computationally convenient for that purpose. However, if reporting the absolute AIC or BIC value is required, the full formula should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3be4bd",
   "metadata": {},
   "source": [
    "## Modification for Small Sample Size: The Corrected AIC (AICc)\n",
    "\n",
    "The Akaike Information Criterion (AIC) is derived in the limit of a large sample size $n$. When the sample size is not large relative to the number of estimated parameters $k$, AIC can perform poorly. Specifically, it has a tendency to favor models with too many parameters, a phenomenon known as overfitting.\n",
    "\n",
    "To address this, the **Corrected Akaike Information Criterion (AICc)** was developed. AICc includes a second-order correction term that increases the penalty for extra parameters, with the size of this correction being larger for smaller sample sizes.\n",
    "\n",
    "The AICc is defined as the AIC with an added penalty term:\n",
    "\n",
    "$$AICc = AIC + \\frac{2k(k+1)}{n - k - 1}$$\n",
    "\n",
    "Let's derive the more common form of this equation. We start with the full definition of AIC:\n",
    "\n",
    "$$AIC = -2\\ln(\\hat{L}) + 2k$$\n",
    "\n",
    "Now, we add the correction term:\n",
    "\n",
    "$$AICc = \\left( -2\\ln(\\hat{L}) + 2k \\right) + \\frac{2k(k+1)}{n - k - 1}$$\n",
    "\n",
    "We can combine the two terms that include $k$ by factoring out $2k$:\n",
    "\n",
    "$$AICc = -2\\ln(\\hat{L}) + 2k \\left( 1 + \\frac{k+1}{n - k - 1} \\right)$$\n",
    "\n",
    "To simplify the expression in the parenthesis, we find a common denominator:\n",
    "\n",
    "$$1 + \\frac{k+1}{n - k - 1} = \\frac{(n - k - 1) + (k+1)}{n - k - 1} = \\frac{n}{n - k - 1}$$\n",
    "\n",
    "By substituting this simplified fraction back into the equation, we arrive at the most common and elegant formula for AICc:\n",
    "\n",
    "$$AICc = -2\\ln(\\hat{L}) + 2k \\left( \\frac{n}{n - k - 1} \\right)$$\n",
    "\n",
    "This form clearly shows that the standard AIC penalty $2k$ is being multiplied by a correction factor $n / (n - k - 1)$.\n",
    "\n",
    "Let's examine the correction factor $n / (n - k - 1)$:\n",
    "\n",
    "*   **When $n$ is large:** As $n$ becomes much larger than $k$, the fraction $n / (n - k - 1)$ approaches $n/n$, which is 1. In this case, AICc converges to AIC. This is exactly what we want; for large samples, the correction is negligible.\n",
    "*   **When $n$ is small:** When $n$ is close to $k$, the denominator $n - k - 1$ becomes very small, making the correction factor large. This imposes a much heavier penalty on more complex models (those with a larger $k$), helping to prevent overfitting.\n",
    "\n",
    "Given its properties, it is often recommended to **use AICc by default**, rather than AIC, especially if you are not in a \"big data\" context. A common rule of thumb is to use AICc when the ratio $n/k$ is less than 40.\n",
    "\n",
    "*   **Number of Parameters $k$:** As before, $k$ must include all estimated parameters. In the OLS case with $p$ model coefficients, $k = p + 1$ to account for the estimated variance.\n",
    "\n",
    "We can write the proportional formula for AICc in the OLS case (unknown variance) by starting with the proportional formula for AIC and adding the correction term:\n",
    "\n",
    "$$AICc \\propto n \\ln(RSS) + 2k + \\frac{2k(k+1)}{n - k - 1}$$\n",
    "\n",
    "Or, using the multiplicative correction factor:\n",
    "\n",
    "$$AICc \\propto n \\ln(RSS) + 2k \\left( \\frac{n}{n - k - 1} \\right)$$\n",
    "\n",
    "NOTE: There is an obvious restriction: $n$ should be greater than $k+1$ ($n > k + 1$) to avoid the situation when denominator is zero or negative and AICc cannot be calculated.\n",
    "\n",
    "Is There a Small-Sample Correction for BIC? While AIC has a widely accepted and commonly used correction, **there is no standard, universally adopted small-sample correction for BIC.**\n",
    "\n",
    "The reasons for this are rooted in their different theoretical origins:\n",
    "1.  **Different Foundations:** AIC is derived from principles of information theory and aims to find the model that best approximates the true data-generating process with minimal information loss. Its derivation allows for a more straightforward second-order correction.\n",
    "2.  **Asymptotic Nature of BIC:** BIC is derived from a Bayesian probability framework. It is an asymptotic approximation to the full Bayesian model evidence, which is used to calculate the Bayes factor. The $k \\ln(n)$ penalty term arises naturally from this approximation.\n",
    "3.  **Stronger Inherent Penalty:** The penalty term in BIC, $k \\ln(n)$, is already much stronger than AIC's $2k$ penalty (for any $n > 7$). This strong penalty, which scales with sample size, already makes BIC less prone to the kind of overfitting in small samples that necessitated the creation of AICc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda4bc9",
   "metadata": {},
   "source": [
    "## Additional Materials\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "* https://www.quantstart.com/articles/Maximum-Likelihood-Estimation-for-Linear-Regression/\n",
    "* https://en.wikipedia.org/wiki/Akaike_information_criterion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}