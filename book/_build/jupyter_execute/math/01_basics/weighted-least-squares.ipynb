{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4b4898",
   "metadata": {},
   "source": [
    "# Weighted Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a99129",
   "metadata": {},
   "source": [
    "## What is Weighted Least Squares (WLS)?\n",
    "\n",
    "**Weighted Least Squares (WLS)** is a variation of the Ordinary Least Squares (OLS) method. In OLS, it's assumed that the variance of the errors (residuals) is constant across all observations. This assumption is called **homoscedasticity**. However, in many real-world scenarios, this assumption doesn't hold; the errors might be larger for some observations than for others. This situation is called **heteroscedasticity**.\n",
    "\n",
    "When heteroscedasticity is present, OLS gives equal weight to all data points. This can lead to inefficient parameter estimates (meaning the estimates are not the most precise possible) and incorrect standard errors, which in turn affect the reliability of confidence intervals and hypothesis tests.\n",
    "\n",
    "WLS addresses this by assigning different **weights** to each data point in the regression. The goal of WLS is to minimize the sum of the *weighted* squared residuals:\n",
    "\n",
    "$$\\sum_{i=1}^{n} w_i (y_i - f(x_i, \\beta))^2$$\n",
    "\n",
    "where:\n",
    "* $w_i$ is the weight for the $i$-th data point.\n",
    "* $y_i$ is the observed dependent variable for the $i$-th point.\n",
    "* $f(x_i, \\beta)$ is the predicted value from the model for the $i$-th point, with parameters $\\beta$.\n",
    "\n",
    "**How are weights determined?**\n",
    "The weights are typically inversely proportional to the variance of the errors for each observation. If $\\sigma_i^2$ is the variance of the error for the $i$-th observation, then the weight $w_i$ is usually $1/\\sigma_i^2$. This means:\n",
    "* Observations with smaller errors (lower variance) get larger weights, influencing the fit more.\n",
    "* Observations with larger errors (higher variance) get smaller weights, influencing the fit less.\n",
    "\n",
    "NOTE: For additional statistical context, Weighted Least Squares is a special case of a broader method known as Generalized Least Squares (GLS). GLS is designed for situations where the errors are either heteroscedastic (have non-constant variance) or are correlated with each other. WLS simplifies the GLS framework by assuming that the errors, while having different variances, are not correlated with one another. This means the covariance matrix of the errors is a diagonal matrix, which makes the calculations more straightforward than in the full GLS approach.\n",
    "\n",
    "## Standard deviation\n",
    "\n",
    "In experimental sciences and data analysis, observations are inherently subject to **measurement uncertainty**. This uncertainty reflects the lack of perfect knowledge about the true value of a quantity due to limitations of instruments, environmental variations, or inherent stochastic processes. In our case of the regression analysis and statistics, we may also call it as an \"error\".\n",
    "\n",
    "The **standard deviation ($\\sigma$)** is the most common statistical measure used to quantify the spread or dispersion of a set of data points around their mean. In the context of individual experimental measurements, the standard deviation of a measurement (or its uncertainty) refers to the expected variability if that measurement were to be repeated multiple times under identical conditions.\n",
    "\n",
    "If a measurement $Y$ is reported as $Y \\pm \\delta Y$, where $\\delta Y$ represents the uncertainty, this $\\delta Y$ is frequently taken to be the **standard deviation** of that measurement. It implies that approximately 68.3% of repeated measurements would fall within the range $[Y - \\delta Y, Y + \\delta Y]$, assuming a normal distribution of errors.\n",
    "\n",
    "### Interpretation of Individual Data Points in Weighted Least Squares\n",
    "\n",
    "When performing regression analysis, especially Weighted Least Squares (WLS), each data point $(x_i, y_i)$ is treated as follows:\n",
    "* **$x_i$ (Independent Variable):** Typically assumed to be known precisely, or to have negligible uncertainty compared to $y_i$.\n",
    "* **$y_i$ (Dependent Variable):** This value is considered the **best estimate** (or the sample mean) of the true underlying value of the dependent variable at $x_i$. This implies that if multiple independent measurements of $y$ were taken at $x_i$, $y_i$ would represent their average, aiming to minimize random errors.\n",
    "* **$\\sigma_i$ (Uncertainty/Standard Deviation of $y_i$):** This parameter, provided to the fitting algorithm (e.g., via the `sigma` argument in `scipy.optimize.curve_fit`), quantifies the **standard deviation of the measurement $y_i$**. It reflects the precision with which $y_i$ was determined. A smaller $\\sigma_i$ indicates a more precise (less uncertain) measurement, and vice-versa.\n",
    "\n",
    "### Role in Weighted Least Squares (WLS)\n",
    "\n",
    "The `scipy.optimize.curve_fit` function, when provided with the `sigma` array and `absolute_sigma=True`, performs a Weighted Least Squares minimization.\n",
    "\n",
    "* **Weight Calculation:** For each data point $(x_i, y_i)$ with associated standard deviation $\\sigma_i$, a weight $w_i$ is calculated as the inverse of the variance: $w_i = \\frac{1}{\\sigma_i^2}$.\n",
    "* **Minimization Objective:** The Levenberg-Marquardt algorithm (the default for `curve_fit`) then seeks to minimize the **weighted residual sum of squares (WRSS)**:\n",
    "\n",
    "    $$RSS_w = \\sum_{i=1}^{N} w_i (y_i - f(x_i, \\beta))^2 = \\sum_{i=1}^{N} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma_i^2}$$\n",
    "    \n",
    "    where $RSS_w$ is WRSS, $f(x_i, \\beta)$ is the model's predicted value and $\\beta$ represents the model parameters.\n",
    "* **Impact of Weights:** Measurements with smaller $\\sigma_i$ (higher precision) receive larger weights ($w_i$), thus exerting a greater influence on the determination of the fitted parameters. Conversely, measurements with larger $\\sigma_i$ (lower precision) receive smaller weights, having less impact on the fit. This ensures that the fitting process prioritizes minimizing deviations for the more reliable data points.\n",
    "\n",
    "By incorporating these standard deviations, WLS provides more statistically efficient (more precise) estimates of the model parameters when the assumption of constant error variance (homoscedasticity) is violated, as is the case when different data points have different known uncertainties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b153194",
   "metadata": {},
   "source": [
    "## What if each point has the same standard deviation?\n",
    "\n",
    "If each data point has the same standard deviation, the homoscedasticity is not violated. However, we can use standard deviation data to improve our results.\n",
    "\n",
    "**Homoscedasticity** is the statistical assumption that the **variance of the errors** is constant across all levels of the independent variable. In our notation, this means $\\sigma_i^2 = C$, where $C$ is a constant for all data points $i$.\n",
    "\n",
    "Since standard deviation is the square root of variance ($\\sigma = \\sqrt{\\sigma^2}$), a constant variance implies a constant standard deviation. If every data point has the same standard deviation, then the assumption of homoscedasticity is **satisfied and not violated**.\n",
    "\n",
    "This leads to a crucial and interesting point about the relationship between Weighted Least Squares (WLS) and Ordinary Least Squares (OLS).\n",
    "\n",
    "1.  **WLS Objective:** WLS minimizes the weighted sum of squared residuals:\n",
    "\n",
    "    $$\\sum_{i=1}^{N} w_i (y_i - f(x_i, \\beta))^2$$\n",
    "\n",
    "    where the weight $w_i = 1/\\sigma_i^2$.\n",
    "\n",
    "2.  **When Homoscedasticity is Satisfied:** If every data point has the same standard deviation, let's call it $\\sigma_{const}$. Then, for all $i$, we have $\\sigma_i = \\sigma_{const}$. This means all the weights are also the same:\n",
    "\n",
    "    $$w_i = \\frac{1}{\\sigma_i^2} = \\frac{1}{\\sigma_{const}^2} = C$$\n",
    "\n",
    "    where $C$ is a constant.\n",
    "\n",
    "3.  **Mathematical Equivalence:** The WLS minimization objective then becomes:\n",
    "\n",
    "    $$\\sum_{i=1}^{N} C (y_i - f(x_i, \\beta))^2 = C \\sum_{i=1}^{N} (y_i - f(x_i, \\beta))^2$$\n",
    "\n",
    "    Since $C$ is just a positive constant, minimizing this expression is mathematically identical to minimizing the expression without the constant:\n",
    "\n",
    "    $$\\sum_{i=1}^{N} (y_i - f(x_i, \\beta))^2$$\n",
    "    \n",
    "    This is exactly the objective of **Ordinary Least Squares (OLS)**.\n",
    "\n",
    "If the assumption of homoscedasticity holds true (i.e., every data point has the same standard deviation), then **Weighted Least Squares and Ordinary Least Squares will produce the exact same parameter estimates**.\n",
    "\n",
    "However, WLS can still be valuable even in this situation if you have a known, constant `sigma` and use `absolute_sigma=True`, because it will provide you with the correct uncertainties (standard errors) for your fitted parameters. OLS would simply assume the errors are scaled by the goodness of fit, which might not be an accurate reflection of the true experimental uncertainties.\n",
    "\n",
    "### Scenario A: Using a constant `sigma` array with `absolute_sigma=True`\n",
    "\n",
    "Let's assume your known, constant standard deviation is $\\sigma_{known} = 0.5$. Your `sigma` array would be `[0.5, 0.5, 0.5, ...]`\n",
    "\n",
    "* **Fitted Parameters (`popt`):** The optimal values for parameters A and B will be **identical** to the case where you don't use `sigma`. As we discussed, the minimization is mathematically equivalent to OLS, and the location of the minimum of the objective function is the same.\n",
    "* **Covariance Matrix (`pcov`) and Parameter Errors (`perr`):** This is where the difference lies. By providing `sigma` and setting `absolute_sigma=True`, you are telling the algorithm: \"My measurements have an absolute standard deviation of 0.5. Calculate the parameter uncertainties based on this known fact.\" The covariance matrix (`pcov`) and the standard errors (`perr`) derived from it will directly reflect the propagated uncertainty from your measurements.\n",
    "\n",
    "This is the **statistically correct** approach when you have known measurement uncertainties. The resulting parameter errors are a more accurate representation of the true uncertainty in your fitted parameters, grounded in the physical reality of your experiment.\n",
    "\n",
    "### Scenario B: Skipping the `sigma` parameter\n",
    "\n",
    "In this case, you simply call `curve_fit` without specifying `sigma`.\n",
    "\n",
    "* **Fitted Parameters (`popt`):** The optimal values for parameters A and B will be **identical** to Scenario A.\n",
    "* **Covariance Matrix (`pcov`) and Parameter Errors (`perr`):** The `curve_fit` function handles this differently. It performs a standard OLS fit (which is equivalent to WLS with constant weights), but then it **scales the covariance matrix by the reduced chi-squared value (the goodness of fit)**. This is the behavior of `absolute_sigma=False` (the default).\n",
    "\n",
    "    * The standard errors you get are based on the **observed scatter of your data points around the fitted curve**, not on any known measurement uncertainty.\n",
    "    * If your data points are very close to the fitted curve, the reduced chi-squared value will be small, and the calculated `perr` will be artificially small.\n",
    "    * If your data points are very scattered around the fitted curve, the reduced chi-squared value will be large, and the calculated `perr` will be artificially large.\n",
    "\n",
    "Summary:\n",
    "\n",
    "| Feature                  | Using `sigma` with `absolute_sigma=True` (constant `sigma`) | Skipping `sigma` (OLS)                                    |\n",
    "| ------------------------ | ------------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| **Fitted Parameters (`popt`)** | **Identical** | **Identical** |\n",
    "| **Parameter Errors (`perr`)** | **More Accurate.** Based on your known measurement uncertainty. | **Less Accurate.** Based on the observed scatter of the data. |\n",
    "| **Statistical Assumption** | You know the absolute uncertainty of your measurements.         | You don't know the absolute uncertainty, and the uncertainty is constant. |\n",
    "| **Purpose** | To find the parameters and their uncertainties based on your experimental knowledge. | To find the parameters and their uncertainties based on the model's goodness of fit. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c5152",
   "metadata": {},
   "source": [
    "## Weighted Residual Sum of Squares (WRSS)\n",
    "\n",
    "The objective of any \"least squares\" method is to find the model parameters that minimize the sum of the squared residuals. In Weighted Least Squares (WLS), this objective function is the **Weighted Residual Sum of Squares (WRSS)**. A small WRSS indicates a tight fit of the model to the data.\n",
    "\n",
    "The formula for the WRSS is:\n",
    "\n",
    "$$ RSS_w = \\sum_{i=1}^{N} w_i (y_i - f(x_i, \\beta))^2 $$\n",
    "\n",
    "where:\n",
    "* $RSS_w$ is WRSS.\n",
    "* $y_i$ is the observed value for the i-th data point.\n",
    "* $f(x_i, \\beta)$ is the value predicted by the model for the i-th data point.\n",
    "* $(y_i - f(x_i, \\beta))$ is the residual (the error) for the i-th data point.\n",
    "* $w_i$ is the weight assigned to the i-th data point.\n",
    "\n",
    "The crucial difference from the ordinary Residual Sum of Squares (RSS) is the inclusion of the weight, $w_i$. This weight ensures that not all squared residuals contribute equally to the final sum. As defined in your earlier sections, the weight is typically the inverse of the error variance ($w_i = 1/\\sigma_i^2$), giving more influence to more precise data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6f380",
   "metadata": {},
   "source": [
    "### The Connection Between WRSS and the Chi-Squared (χ²) Statistic\n",
    "\n",
    "In the context of curve fitting where meaningful, known uncertainties ($\\sigma_i$) are provided for each data point, the WRSS takes on a profound statistical meaning. In this case, the WRSS value is identical to the **chi-squared statistic (χ²)**.\n",
    "\n",
    "$$ \\chi^2 = \\sum_{i=1}^{N} \\left( \\frac{y_i - f(x_i, \\beta)}{\\sigma_i} \\right)^2 = \\sum_{i=1}^{N} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma_i^2} = WRSS $$\n",
    "\n",
    "This is more than just a notational change; it recasts the WRSS as a goodness-of-fit statistic. By comparing the calculated $\\chi^2$ value with the theoretical chi-squared distribution for a given number of degrees of freedom, one can quantitatively assess how well the model describes the data. More details about $\\chi^2$ stasitics see [here](../01_basics/goodness-of-fit-and-chi-squared.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30f2107",
   "metadata": {},
   "source": [
    "### Calculating Goodness-of-Fit Metrics with WLS\n",
    "\n",
    "Standard metrics like RMSE and R-squared should be adapted to use the weights in a WLS context to be consistent with the weighted regression.\n",
    "\n",
    "**Root Mean Squared Error (RMSE) and Standard Error of the Residuals (SER)**\n",
    "\n",
    "In WLS, both the RMSE and the SER (also known as the Residual Standard Error) are calculated using the WRSS to properly reflect the weighted nature of the fit.\n",
    "\n",
    "*   **Weighted Root Mean Squared Error (WRMSE):** A common formulation for the weighted RMSE is:\n",
    "\n",
    "    $$ RMSE_w = \\sqrt{\\frac{\\sum_{i=1}^{N} w_i (y_i - f(x_i, \\beta))^2}{\\sum_{i=1}^{N} w_i}} = \\sqrt{\\frac{RSS_w}{\\sum w_i}} $$\n",
    "\n",
    "    This calculates a weighted average of the squared errors. Note that some definitions might use $N$ (the number of data points) in the denominator instead of the sum of the weights.\n",
    "\n",
    "*   **Standard Error of the Residuals (SER) for WLS:** The SER is an unbiased estimator of the error variance, adjusted for the number of parameters ($p$) in the model. Its formula is:\n",
    "\n",
    "    $$ \\text{SER} = \\sqrt{\\frac{\\text{WRSS}}{N - p}} $$\n",
    "\n",
    "    This value quantifies the typical deviation of the data points from the fitted line in the weighted space. A smaller SER indicates that the model's predictions are closer to the actual observations.\n",
    "\n",
    "#### **R-squared (R²) and Adjusted R-squared in WLS**\n",
    "\n",
    "The interpretation of R-squared in WLS is more complex than in OLS. The standard R-squared formula is $R² = 1 - RSS/TSS$, where TSS is the Total Sum of Squares around the mean. In WLS, both the RSS and TSS must be weighted to be meaningful.\n",
    "\n",
    "*   **Weighted R-squared (R²_w):** A widely accepted approach is to calculate a weighted version of both the residual sum of squares (which is WRSS) and the total sum of squares (WTSS).\n",
    "\n",
    "    The **Weighted Total Sum of Squares (WTSS)** is defined as:\n",
    "\n",
    "    $$ \\text{WTSS} = \\sum_{i=1}^{N} w_i (y_i - \\bar{y}_w)^2 $$\n",
    "\n",
    "    where $ȳ_w$ is the **weighted mean** of the dependent variable $y$, calculated as $ȳ_w = (\\sigma w_i * y_i) / (\\sigma w_i)$.\n",
    "\n",
    "    The weighted R-squared is then:\n",
    "\n",
    "    $$ R^2_w = 1 - \\frac{\\text{WRSS}}{\\text{WTSS}} $$\n",
    "\n",
    "    This $R²_w$ represents the proportion of the total weighted variance in the dependent variable that is explained by the weighted model. It is important to note that different statistical packages might use slightly different formulations, which can sometimes lead to confusion when comparing results.\n",
    "\n",
    "*   **Adjusted Weighted R-squared:** The adjusted R² is modified similarly, using the weighted R-squared and accounting for the number of data points ($N$) and the number of predictors ($p$):\n",
    "\n",
    "    $$ \\text{Adjusted } R^2_w = 1 - (1 - R^2_w) \\frac{N-1}{N-p-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203caab7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f34aa7f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e09faf08",
   "metadata": {},
   "source": [
    "## Additional Materials\n",
    "\n",
    "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n",
    "* https://online.stat.psu.edu/stat501/book/export/html/990\n",
    "* https://www.stat.uchicago.edu/~yibi/teaching/stat224/L14.pdf\n",
    "* https://ms.mcmaster.ca/canty/teaching/stat3a03/Lectures7.pdf\n",
    "* https://en.wikipedia.org/wiki/Reduced_chi-squared_statistic\n",
    "* https://en.wikipedia.org/wiki/Weighted_arithmetic_mean\n",
    "* https://stats.stackexchange.com/questions/51442/weighted-variance-one-more-time\n",
    "* https://stats.stackexchange.com/questions/61225/correct-equation-for-weighted-unbiased-sample-covariance/61298#61298\n",
    "* https://stats.stackexchange.com/questions/330548/difference-in-r-squared-observed-from-statsmodels-when-wls-is-used\n",
    "* https://stats.stackexchange.com/questions/439590/how-does-r-compute-r-squared-for-weighted-least-squares\n",
    "* https://en.wikipedia.org/wiki/Pseudo-R-squared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}