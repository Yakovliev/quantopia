{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf22ae86",
   "metadata": {},
   "source": [
    "# Orthogonal Distance Regression\n",
    "\n",
    "Orthogonal Distance Regression (ODR) is a regression method that finds the best-fit line or curve by minimizing the sum of the squared **orthogonal distances** from the data points to the model. Unlike ordinary least squares (OLS) regression, which minimizes the vertical distances (errors in the y-direction), ODR accounts for errors in **both the x and y variables**.\n",
    "\n",
    "This makes ODR particularly useful when both independent and dependent variables have measurement errors. The \"orthogonal\" part refers to the fact that the distances are measured perpendicular to the fitted curve, not vertically or horizontally.\n",
    "\n",
    "## General Principles of ODR\n",
    "\n",
    "In a typical OLS regression, you assume that the independent variable ($x$) is known without error, and all the error is in the dependent variable ($y$). The goal is to minimize the sum of squared vertical distances from each data point $(x_i, y_i)$ to the fitted line $y = f(x_i)$.\n",
    "\n",
    "ODR, however, treats both $x$ and $y$ as having errors. The method involves finding a point $(\\hat{x}_i, \\hat{y}_i)$ on the curve $y = f(x)$ that is closest to the observed data point $(x_i, y_i)$. The \"distance\" that ODR minimizes is the perpendicular distance between the observed point $(x_i, y_i)$ and the point on the curve $(\\hat{x}_i, \\hat{y}_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fac6eb",
   "metadata": {},
   "source": [
    "The objective function that ODR minimizes is:\n",
    "\n",
    "$$\\sum_{i=1}^{n} \\left( w_{x,i}^2(x_i - \\hat{x}_i)^2 + w_{y,i}^2(y_i - \\hat{y}_i)^2 \\right)$$\n",
    "\n",
    "A more common and statistically grounded formulation, especially when using standard deviations, is to define the weights as the inverse of the variances:\n",
    "\n",
    "$$S = \\sum_{i=1}^{n} \\left( \\frac{(x_i - \\hat{x}_i)^2}{\\sigma_{x,i}^2} + \\frac{(y_i - \\hat{y}_i)^2}{\\sigma_{y,i}^2} \\right)$$\n",
    "\n",
    "where:\n",
    "* $S$ is the sum of squared weighted orthogonal distances.\n",
    "* $(x_i, y_i)$ are the observed data points.\n",
    "* $(\\hat{x}_i, \\hat{y}_i)$ are the corresponding points on the fitted curve.\n",
    "* The weights $w_{x,i}$ and $w_{y,i}$ are used to properly scale the errors in the objective function. They represent the inverse of the variance (the square of the standard deviation) of the errors for each variable.\n",
    "* $\\sigma_{x,i}$ and $\\sigma_{y,i}$ are the **standard deviations** of the errors for the $i$-th data point in the x and y directions, respectively.\n",
    "* The terms $\\frac{1}{\\sigma_{x,i}^2}$ and $\\frac{1}{\\sigma_{y,i}^2}$ are the actual weights used in the minimization. They represent the **inverse variance**, a standard way to weight observations in statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad1137",
   "metadata": {},
   "source": [
    "The core idea is that the regression should be more influenced by data points with smaller uncertainties and less by those with larger uncertainties.\n",
    "\n",
    "* **$w_{x,i}$ (x-axis weights):** A data point with a small error in the x-direction (a small standard deviation) will have a large $w_x$ value. This means the algorithm will penalize deviations in the x-direction more heavily, forcing the fitted curve to be closer to that point horizontally.\n",
    "* **$w_{y,i}$ (y-axis weights):** Similarly, a data point with a small error in the y-direction will have a large $w_y$ value. This pushes the curve to be closer to that point vertically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ff217",
   "metadata": {},
   "source": [
    "## ODR with Non-linear Functions\n",
    "\n",
    "ODR is especially powerful for non-linear regression because it can handle cases where both variables have errors and the relationship isn't a straight line. The process involves an iterative numerical optimization algorithm to find the parameters of the non-linear function that minimize the objective function described above.\n",
    "\n",
    "For your specific case with a non-linear function and errors for both the x-axis and y-axis, you will need to:\n",
    "\n",
    "1.  **Define your non-linear model function**, e.g., $y = f(x; \\beta_1, \\beta_2, ...)$ where $\\beta_i$ are the parameters you want to estimate.\n",
    "2.  **Provide your data** ($x_i, y_i$).\n",
    "3.  **Specify the errors** or weights for both variables.\n",
    "4.  **Use an ODR-specific software library or package** (e.g., `scipy.odr` in Python) to perform the regression. The solver will then iteratively adjust the parameters of your function until the sum of the squared orthogonal distances is minimized. ODR typically uses trust-region Levenberg-Marquardt algorithms.\n",
    "\n",
    "The output will be the best-fit parameters for your non-linear function, along with their standard errors, which can be used to assess the uncertainty of the estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab84630",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efee2a5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9938130",
   "metadata": {},
   "source": [
    "Assessing the goodness of fit for Orthogonal Distance Regression (ODR), especially with non-linear models, is more complex than with Ordinary Least Squares (OLS). Since ODR accounts for errors in both x and y variables, traditional metrics like R-squared can be misleading or inappropriate.\n",
    "\n",
    "The most suitable and standard goodness-of-fit parameter for ODR is the **reduced chi-squared ($\\chi_{red}^2$)** statistic.\n",
    "\n",
    "### Reduced Chi-Squared ($\\chi_{red}^2$)\n",
    "\n",
    "The reduced chi-squared is a powerful and statistically rigorous measure for ODR because it directly incorporates the uncertainties (weights) of both the x and y data points. It is defined as the sum of squared weighted residuals divided by the number of degrees of freedom.\n",
    "\n",
    "$$\\chi_{red}^2 = \\frac{\\sum_{i=1}^{n} \\left( \\frac{(x_i - \\hat{x}_i)^2}{\\sigma_{x,i}^2} + \\frac{(y_i - \\hat{y}_i)^2}{\\sigma_{y,i}^2} \\right)}{n - p}$$\n",
    "\n",
    "* $n$ is the number of data points.\n",
    "* $p$ is the number of parameters in the model.\n",
    "* $\\sigma_{x,i}$ and $\\sigma_{y,i}$ are the known standard deviations of the errors for each data point $(x_i, y_i)$.\n",
    "\n",
    "#### Interpretation of Reduced Chi-Squared\n",
    "\n",
    "The value of $\\chi_{red}^2$ tells you how well the model fits the data relative to the expected measurement errors.\n",
    "\n",
    "* A value of **$\\chi_{red}^2 \\approx 1$** indicates an excellent fit. The model's residuals are consistent with the measurement uncertainties you provided. The observed scatter is what you'd expect.\n",
    "* A value of **$\\chi_{red}^2 \\gg 1$** (significantly greater than 1) suggests a poor fit. This means the model does not adequately describe the data, or you have underestimated your measurement uncertainties. The observed scatter is much larger than expected.\n",
    "* A value of **$\\chi_{red}^2 < 1$** may indicate that the model is \"overfitting\" the data, or that you have overestimated your measurement uncertainties.\n",
    "\n",
    "---\n",
    "\n",
    "### R-squared and ODR\n",
    "\n",
    "While R-squared is a common metric for OLS, its application to ODR is problematic. R-squared is based on the idea of explaining the variance in the y-variable, assuming the x-variable is known without error. This assumption is explicitly violated in ODR. Calculating a pseudo R-squared value for ODR can be done, but it often lacks a clear statistical interpretation and can be insensitive to the order of the variables (swapping x and y). For these reasons, **R-squared and adjusted R-squared are generally not recommended** for evaluating ODR results.\n",
    "\n",
    "### Non-linear Regression and Other Parameters\n",
    "\n",
    "For both linear and non-linear regression, other metrics and methods are valuable for assessing the fit:\n",
    "\n",
    "* **Residual Plots:** These are crucial. Plotting the residuals (the distances from the points to the fitted curve) against the fitted values can reveal systematic patterns (e.g., a \"U\" shape) that indicate a poor model choice, even if other statistical parameters look good.\n",
    "* **Standard Errors of Parameters:** The standard errors of the fitted parameters provide a measure of their uncertainty. If these are very large, it suggests that the data are not sufficient to accurately determine the model's parameters.\n",
    "* **Confidence and Prediction Bands:** Visualizing the confidence and prediction bands around the fitted curve is an excellent way to see the range of uncertainty in your model and its predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}