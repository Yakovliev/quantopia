
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Least Squares Regression, RSS, RMSE, R-squared &#8212; Quantopia&#39;:&#39; Physics, Python and Pi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'math/01_basics/least-squares-regression';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Least Squares Regression - Code Examples" href="least-squares-regression-code.html" />
    <link rel="prev" title="Fourier Transform" href="fourier-transform-01.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Quantopia':' Physics, Python and Pi - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Quantopia':' Physics, Python and Pi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Quantopia: Physics, Python, and Pi (Alpha Version)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">MATH</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gradient-operator.html">Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradient-directional-derivative.html">Directional Derivative</a></li>
<li class="toctree-l1"><a class="reference internal" href="divergence.html">Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier-transform-01.html">Fourier Transform</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Least Squares Regression, RSS, RMSE, R-squared</a></li>
<li class="toctree-l1"><a class="reference internal" href="least-squares-regression-code.html">Least Squares Regression - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares.html">Ordinary Least Squares (OLS) Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares-code.html">Ordinary Least Squares (OLS) Regression - Code Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="variance-covariance.html">Variance and Covariance</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares.html">Weighted Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-1.html">WLS - Code Examples Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-2.html">WLS - Code Examples Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="goodness-of-fit-and-chi-squared.html">Goodness of Fit and Chi-Squared Statistic</a></li>
<li class="toctree-l1"><a class="reference internal" href="aic-and-bic.html">Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-3.html">WLS - Code Examples Part 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="orthogonal-distance-regression.html">Orthogonal Distance Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PHYSICS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../physics/continuity-equation-01.html">The Continuity Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/continuity-equation-02.html">The Continuity Equation: One-Dimensional Advection of a Density Profile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/ensemble.html">Statistical Ensembles and Liouville’s Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/microcanonical-ensemble.html">Microcanonical Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/fokker-planck-equation-example.html">Fokker-Planck Equation - Example Analysis (preview)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DATA SCIENCE AND MACHINE LEARNING</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../data-science/knn-algorithm.html">K-Nearest Neighbors (KNN) Algorithm (preview)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data-science/naive-bayes.html">Naive Bayes Method (preview)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data-science/logistic-regression.html">Logistic Regression (preview)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/Yakovliev/quantopia/blob/main/book/math/01_basics/least-squares-regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia/issues/new?title=Issue%20on%20page%20%2Fmath/01_basics/least-squares-regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/math/01_basics/least-squares-regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Least Squares Regression, RSS, RMSE, R-squared</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-core-idea-minimizing-differences">The Core Idea: Minimizing Differences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-regression">Least Squares Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-sum-of-squares-rss">Residual Sum of Squares (RSS)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-coefficient-of-determination">R-squared (Coefficient of Determination)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-squared-r-2">Adjusted R-squared <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-for-non-linear-models">R-squared for Non-Linear Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse-and-standard-error-of-the-regression-ser">Root Mean Squared Error (RMSE) and Standard Error of the Regression (SER)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-levenberg-marquardt-algorithm-lma">What is the Levenberg-Marquardt Algorithm (LMA)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-it-related-to-least-squares-regression">How is it related to Least Squares Regression?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scipy-optimize-curve-fit"><code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code></a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="least-squares-regression-rss-rmse-r-squared">
<h1>Least Squares Regression, RSS, RMSE, R-squared<a class="headerlink" href="#least-squares-regression-rss-rmse-r-squared" title="Link to this heading">#</a></h1>
<p>When you have a set of experimental data points <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, where <span class="math notranslate nohighlight">\(i\)</span> ranges from 1 to <span class="math notranslate nohighlight">\(n\)</span> (the number of data points), and you want to find a mathematical function that best describes the relationship between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, you are performing <strong>curve fitting</strong> or <strong>regression analysis</strong>. The goal is to find the parameters of a chosen function that make the function’s output as close as possible to your observed <span class="math notranslate nohighlight">\(y\)</span> values for the corresponding <span class="math notranslate nohighlight">\(x\)</span> values.</p>
<p>Let’s review some specific non-linear case, and consider that we want to approximate the data with the function:</p>
<div class="math notranslate nohighlight">
\[f(x; A, B) = A \cdot (e^{-B \cdot x} - 1) + 100\]</div>
<p>Here, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are the parameters that we need to determine from the data points. The ‘100’ is a constant offset in this specific function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As of now, we consider all experimental points with no errors. However, if <span class="math notranslate nohighlight">\(y\)</span> values or both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> values have some errors, we need to apply different algorithms. We will consider such algorihms later.</p>
</div>
<section id="the-core-idea-minimizing-differences">
<h2>The Core Idea: Minimizing Differences<a class="headerlink" href="#the-core-idea-minimizing-differences" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>The fundamental idea behind most curve fitting methods is to minimize the “difference” between your experimental <span class="math notranslate nohighlight">\(y_i\)</span> values and the <span class="math notranslate nohighlight">\(y\)</span> values predicted by your chosen function, <span class="math notranslate nohighlight">\(f(x_i; A, B)\)</span>. This “difference” is often called the <strong>residual</strong>.</p>
</div></blockquote>
<p>For each data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, the residual, <span class="math notranslate nohighlight">\(e_i\)</span>, is defined as:</p>
<p><span class="math notranslate nohighlight">\(e_i = y_i - f(x_i; A, B)\)</span></p>
<p>Our goal is to find the values of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> that make these residuals, collectively, as small as possible.</p>
<p>There are various methods for approximating data, but for continuous functions and without explicit error bars on individual points (as you specified initially), the most common and widely used method is <strong>Least Squares Regression</strong>.</p>
</section>
<section id="least-squares-regression">
<h2>Least Squares Regression<a class="headerlink" href="#least-squares-regression" title="Link to this heading">#</a></h2>
<p>The principle of least squares is to find the parameters (in our case, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>) that <strong>minimize the sum of the squares of the residuals</strong>. Why squares?</p>
<ul class="simple">
<li><p>Squaring the residuals ensures that positive and negative differences don’t cancel each other out.</p></li>
<li><p>It penalizes larger errors more heavily than smaller errors, which is often desirable.</p></li>
</ul>
<p>So, we want to minimize the following quantity, which is the <strong>Residual Sum of Squares (RSS)</strong>:</p>
<div class="math notranslate nohighlight">
\[ RSS(A, B) = \sum_{i=1}^{n} (y_i - f(x_i; A, B))^2 \]</div>
<p>Substituting our specific function:</p>
<div class="math notranslate nohighlight">
\[ RSS(A, B) = \sum_{i=1}^{n} (y_i - (A \cdot (e^{-B \cdot x_i} - 1) + 100))^2 \]</div>
<p>To find the values of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> that minimize <span class="math notranslate nohighlight">\(RSS\)</span>, we typically use calculus. We take the partial derivatives of <span class="math notranslate nohighlight">\(RSS\)</span> with respect to each parameter (<span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>), set them equal to zero, and solve the resulting system of equations.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial RSS}{\partial A} = 0\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial RSS}{\partial B} = 0\]</div>
<p>For linear regression, these equations are linear and have a direct analytical solution. However, for non-linear functions like ours (due to the <span class="math notranslate nohighlight">\(e^{-B \cdot x}\)</span> term), these equations are often non-linear and require iterative numerical optimization algorithms (like the Levenberg-Marquardt algorithm, which is commonly used in <code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code> in Python). We won’t derive the specific partial derivatives for the aforementioned function here, as it gets quite involved and typically handled by computational tools. The core idea remains the same: find <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> that make the slope of the <span class="math notranslate nohighlight">\(RSS\)</span> surface zero.</p>
<p>Once you’ve found the best-fit parameters <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, you need to evaluate how “good” your approximation is. This is where metrics like RSS, RMSE, and R-squared come in.</p>
</section>
<section id="residual-sum-of-squares-rss">
<h2>Residual Sum of Squares (RSS)<a class="headerlink" href="#residual-sum-of-squares-rss" title="Link to this heading">#</a></h2>
<p>As derived above, RSS is:</p>
<div class="math notranslate nohighlight">
\[RSS = \sum_{i=1}^{n} (y_i - f(x_i; A, B))^2\]</div>
<p>RSS is a direct measure of the total discrepancy between your observed data points and your fitted function. A smaller RSS indicates a better fit to the data.</p>
<p><strong>Understanding:</strong></p>
<ul class="simple">
<li><p>It’s always non-negative.</p></li>
<li><p>Its units are the square of the units of <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>It’s absolute: you CANNOT compare RSS directly between different datasets or models with different numbers of data points or vastly different scales of <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
</section>
<section id="r-squared-coefficient-of-determination">
<h2>R-squared (Coefficient of Determination)<a class="headerlink" href="#r-squared-coefficient-of-determination" title="Link to this heading">#</a></h2>
<p>R-squared is a very popular metric because it provides a standardized measure of how well your model explains the variability in the dependent variable <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>First, let’s re-state the key sums of squares:</p>
<ol class="arabic">
<li><p><strong>Total Sum of Squares (TSS):</strong> This measures the total variability in the observed <span class="math notranslate nohighlight">\(y\)</span> values around their mean <span class="math notranslate nohighlight">\(\bar{y}\)</span>. It represents how much the <span class="math notranslate nohighlight">\(y\)</span> values vary in total, without considering any model.</p>
<div class="math notranslate nohighlight">
\[TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> are the observed data points and <span class="math notranslate nohighlight">\(\bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i\)</span> is the mean of the observed <span class="math notranslate nohighlight">\(y\)</span> values. TSS represents the total variability in the observed <span class="math notranslate nohighlight">\(y\)</span> values around their mean. It’s the sum of squared differences if you were to approximate all <span class="math notranslate nohighlight">\(y_i\)</span> with their mean <span class="math notranslate nohighlight">\(\bar{y}\)</span> (which is essentially a horizontal line).</p>
</li>
<li><p><strong>Residual Sum of Squares (RSS) / Sum of Squared Residuals (SSR) / Sum of Squares Error (SSE):</strong> This measures the variability in the observed <span class="math notranslate nohighlight">\(y\)</span> values that is <em>not</em> explained by the regression model. It’s the sum of the squared differences between the observed <span class="math notranslate nohighlight">\(y_i\)</span> and the predicted <span class="math notranslate nohighlight">\(f(x_i)\)</span> (often denoted as <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>).</p>
<div class="math notranslate nohighlight">
\[RSS = \sum_{i=1}^{n} (y_i - f(x_i))^2\]</div>
<p>where <span class="math notranslate nohighlight">\(f(x_i)\)</span> (or <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>) are the predicted values from your model.</p>
</li>
<li><p><strong>Sum of Squares due to Regression (SSR) / Explained Sum of Squares (ESS):</strong> This measures the variability in the dependent variable (<span class="math notranslate nohighlight">\(y\)</span>) that <em>is</em> explained by the regression model. It’s the sum of the squared differences between the predicted values <span class="math notranslate nohighlight">\(f(x_i)\)</span> and the mean of the observed <span class="math notranslate nohighlight">\(y\)</span> values <span class="math notranslate nohighlight">\(\bar{y}\)</span>.</p>
<div class="math notranslate nohighlight">
\[ESS = \sum_{i=1}^{n} (f(x_i) - \bar{y})^2\]</div>
<p>NOTE: There can be some confusion with the acronym “SSR” as it may refer to “Sum of Squares due to Regression” or “Sum of Squared Residuals”. For these materials, we will use Residual Sum of Squares (RSS) and Explained Sum of Squares (ESS) to avoid any confusion.</p>
</li>
</ol>
<p>R-squared is defined as:</p>
<div class="math notranslate nohighlight">
\[R^2 = 1 - \frac{RSS}{TSS}\]</div>
<p><strong>Meaning:</strong> R-squared tells you the proportion of the variance in the dependent variable (<span class="math notranslate nohighlight">\(y\)</span>) that is predictable from the independent variable (<span class="math notranslate nohighlight">\(x\)</span>) using your regression model. In simpler terms, it indicates how much of the variation in <span class="math notranslate nohighlight">\(y\)</span> can be explained by your chosen function.</p>
<p><strong>Understanding:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R^2\)</span> ranges from 0 to 1 (or 0% to 100%) for ordinary least squares with an intercept.</p></li>
<li><p>An <span class="math notranslate nohighlight">\(R^2\)</span> of 1 (or 100%) means that your model perfectly explains all the variability in <span class="math notranslate nohighlight">\(y\)</span>. The residuals are all zero, and the function passes through every data point. This is rare in experimental data.</p></li>
<li><p>An <span class="math notranslate nohighlight">\(R^2\)</span> of 0 means that your model explains none of the variability in <span class="math notranslate nohighlight">\(y\)</span>. In this case, your model performs no better than simply predicting the mean of <span class="math notranslate nohighlight">\(y\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> values.</p></li>
<li><p>A higher <span class="math notranslate nohighlight">\(R^2\)</span> generally indicates a better fit.</p></li>
<li><p>NOTE: <span class="math notranslate nohighlight">\(R^2\)</span> may be less than zero or greater than one for non-linear regression or for linear models without an intercept.</p></li>
</ul>
<p><strong>Interpretation Caveats:</strong></p>
<ul class="simple">
<li><p>A high <span class="math notranslate nohighlight">\(R^2\)</span> doesn’t necessarily mean the model is “correct” or that the chosen function is the true underlying relationship. It just means it explains a lot of the variance.</p></li>
<li><p>Adding more parameters to a model will generally increase <span class="math notranslate nohighlight">\(R^2\)</span>, even if those parameters don’t significantly improve the model’s predictive power (this is why <strong>adjusted R-squared</strong> is sometimes used, which penalizes for added complexity).</p></li>
<li><p><strong>IMPORTANT</strong>: <span class="math notranslate nohighlight">\(R^2\)</span> is most appropriate for linear models. For non-linear models, using of <span class="math notranslate nohighlight">\(R^2\)</span> is not recommended as it’s often misleading and shouldn’t be the primary metric.</p></li>
<li><p>It’s possible to have a low <span class="math notranslate nohighlight">\(R^2\)</span> for a valid model if the inherent variability in the data (noise) is very high, even if the model captures the underlying trend.</p></li>
</ul>
</section>
<section id="adjusted-r-squared-r-2">
<h2>Adjusted R-squared <span class="math notranslate nohighlight">\(R^2\)</span><a class="headerlink" href="#adjusted-r-squared-r-2" title="Link to this heading">#</a></h2>
<p>The standard R-squared <span class="math notranslate nohighlight">\(R^2\)</span> measures the proportion of variance in the dependent variable that is explained by the independent variables in a regression model. While useful, <span class="math notranslate nohighlight">\(R^2\)</span> has a significant drawback: <strong>it always increases or stays the same when you add more independent parameters to your model, even if those new variables do not genuinely improve the model’s explanatory power.</strong> This can lead to misleading conclusions, as a more complex model might appear better simply because it has more terms, not because it’s truly a better fit to the underlying phenomenon.</p>
<p><strong>Adjusted R-squared</strong> addresses this issue by penalizing the inclusion of unnecessary parameters. It adjusts the <span class="math notranslate nohighlight">\(R^2\)</span> value based on the number of estimated parameters in the model and the number of data points.</p>
<p>Imagine you have a model with a certain <span class="math notranslate nohighlight">\(R^2\)</span>. If you add a new parameter that genuinely helps explain the variance in <span class="math notranslate nohighlight">\(y\)</span>, the <span class="math notranslate nohighlight">\(RSS\)</span> will decrease significantly, and the <span class="math notranslate nohighlight">\(R^2\)</span> will increase. However, if you add a new parameter that is irrelevant (e.g., random noise), <span class="math notranslate nohighlight">\(RSS\)</span> will still decrease slightly (due to random chance or fitting noise), causing <span class="math notranslate nohighlight">\(R^2\)</span> to increase, but this increase is not meaningful. Adjusted <span class="math notranslate nohighlight">\(R^2\)</span> accounts for this by considering the degrees of freedom.</p>
<p>The formula for adjusted R-squared <span class="math notranslate nohighlight">\(R^2_{adj}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[R^2_{adj} = 1 - \frac{RSS / (n - k)}{TSS / (n - 1)}\]</div>
<p>Let’s break down the components and explain why this formula works:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span>: The number of data points (observations).</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span>: The number of estimated parameters in the model.</p></li>
<li><p><span class="math notranslate nohighlight">\(RSS\)</span>: Residual Sum of Squares (unexplained variation).</p></li>
<li><p><span class="math notranslate nohighlight">\(TSS\)</span>: Total Sum of Squares (total variation).</p></li>
</ul>
<p>To understand the division terms, we need to introduce the concept of <strong>degrees of freedom (df)</strong>:</p>
<ul class="simple">
<li><p><strong>Degrees of freedom for residuals (<span class="math notranslate nohighlight">\(df_{res}\)</span>):</strong> This is the number of data points minus the number of parameters estimated by the model <span class="math notranslate nohighlight">\(df_{res} = n - k\)</span>.</p></li>
<li><p><strong>Degrees of freedom for total variation (<span class="math notranslate nohighlight">\(df_{tot}\)</span>):</strong> This is the number of data points minus 1 (because the mean <span class="math notranslate nohighlight">\(\bar{y}\)</span> is estimated from the data). So, <span class="math notranslate nohighlight">\(df_{tot} = n - 1\)</span>.</p></li>
</ul>
<p>Now, let’s rewrite the formula using degrees of freedom:</p>
<div class="math notranslate nohighlight">
\[R^2_{adj} = 1 - \frac{RSS / df_{res}}{TSS / df_{tot}}\]</div>
<p>This can also be expressed in terms of <strong>Mean Squared Error (MSE)</strong>:</p>
<ul class="simple">
<li><p><strong>Mean Squared Error of Residuals <span class="math notranslate nohighlight">\(MSE\)</span>:</strong> This is the average squared residual.
<span class="math notranslate nohighlight">\(MSE = \frac{RSS}{n - k}\)</span></p></li>
<li><p><strong>Mean Squared Total <span class="math notranslate nohighlight">\(MST\)</span>:</strong> This is the sample variance of <span class="math notranslate nohighlight">\(y\)</span>.
<span class="math notranslate nohighlight">\(MST = \frac{TSS}{n - 1}\)</span></p></li>
</ul>
<p>Substituting these into the adjusted R-squared formula:</p>
<div class="math notranslate nohighlight">
\[R^2_{adj} = 1 - \frac{MSE}{MST}\]</div>
<p><strong>Why this adjustment works:</strong></p>
<ul class="simple">
<li><p><strong>Penalizing Complexity:</strong> When you add an irrelevant parameter, <span class="math notranslate nohighlight">\(RSS\)</span> will decrease only slightly, but <span class="math notranslate nohighlight">\(k\)</span> (the number of estimated parameters) increases by 1. This means <span class="math notranslate nohighlight">\(n - k\)</span> (the denominator for <span class="math notranslate nohighlight">\(MSE\)</span>) decreases. If the decrease in <span class="math notranslate nohighlight">\(RSS\)</span> is not substantial enough to offset the decrease in <span class="math notranslate nohighlight">\(n - k\)</span>, then <span class="math notranslate nohighlight">\(MSE\)</span> might actually <em>increase</em>. If <span class="math notranslate nohighlight">\(MSE\)</span> increases, <span class="math notranslate nohighlight">\(R^2_{adj}\)</span> will decrease. This is the “penalty” for adding useless variables.</p></li>
<li><p><strong>Fair Comparison:</strong> Adjusted <span class="math notranslate nohighlight">\(R^2\)</span> allows for a more fair comparison between models with different numbers of parameters. A model with a higher adjusted <span class="math notranslate nohighlight">\(R^2\)</span> is generally preferred, as it suggests a better fit that is not merely a result of adding more terms.</p></li>
</ul>
<p>You can also derive adjusted R-squared from standard R-squared:</p>
<div class="math notranslate nohighlight">
\[R^2 = 1 - \frac{RSS}{TSS}\]</div>
<div class="math notranslate nohighlight">
\[\frac{RSS}{TSS} = 1 - R^2\]</div>
<div class="math notranslate nohighlight">
\[RSS = (1 - R^2) \cdot TSS\]</div>
<p>Substitute <span class="math notranslate nohighlight">\(RSS\)</span> into the adjusted R-squared formula:</p>
<div class="math notranslate nohighlight">
\[R^2_{adj} = 1 - \frac{(1 - R^2) \cdot TSS / (n - k)}{TSS / (n - 1)}\]</div>
<p>Cancel out <span class="math notranslate nohighlight">\(TSS\)</span>:</p>
<div class="math notranslate nohighlight">
\[R^2_{adj} = 1 - (1 - R^2) \frac{n - 1}{n - k}\]</div>
<p>This form clearly shows how <span class="math notranslate nohighlight">\(R^2_{adj}\)</span> relates to <span class="math notranslate nohighlight">\(R^2\)</span> and the degrees of freedom.</p>
</section>
<section id="r-squared-for-non-linear-models">
<h2>R-squared for Non-Linear Models<a class="headerlink" href="#r-squared-for-non-linear-models" title="Link to this heading">#</a></h2>
<p><strong>Adjusted R-squared can be used for both linear and non-linear functions. However, it should be used with caution non-linear functions/models</strong></p>
<ul class="simple">
<li><p><strong>Linear Functions:</strong> Adjusted R-squared is very commonly used in linear regression. It’s the preferred metric over standard <span class="math notranslate nohighlight">\(R^2\)</span> when comparing linear models with different numbers of parameters, as it helps identify models that are parsimonious (simple yet effective).</p></li>
<li><p><strong>Non-Linear Functions:</strong> The principles behind adjusted R-squared (penalizing for model complexity and providing a more robust measure of explained variance) apply equally to non-linear models, especially when they are fitted using the least squares method (for example, using <code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code>).</p></li>
</ul>
<p>Technically, we can compute adjusted R-squared for any model fitted using least squares methods, including non-linear least squares.</p>
<p>However, for non-linear models, adjusted R-squared as well as R-squared can behave unexpectedly:</p>
<ul class="simple">
<li><p>It may not represent the proportion of variance explained in the same intuitive way</p></li>
<li><p>It can sometimes be negative or exceed 1</p></li>
<li><p>The penalty for additional parameters may not adequately capture model complexity in non-linear cases</p></li>
</ul>
<p>Therefore, when you are comparing different non-linear models, or trying to decide if adding another parameter to your non-linear function is truly beneficial, adjusted R-squared is a more appropriate metric than the standard R-squared. However, the general recommendation is to use adjusted R-squared with caution for non-linear models.</p>
<p>For a linear model that includes an intercept term (e.g., <span class="math notranslate nohighlight">\(f(x) = \beta_0 + \beta_1 x\)</span>), the following identity is always true:</p>
<div class="math notranslate nohighlight">
\[ \sum_{i=1}^{n} (y_i - \bar{y})^2 = \sum_{i=1}^{n} (f(x_i) - \bar{y})^2 + \sum_{i=1}^{n} (y_i - f(x_i))^2 \]</div>
<p>Or, more simply:</p>
<div class="math notranslate nohighlight">
\[ TSS = ESS + RSS \]</div>
<p>(Total Sum of Squares = Explained Sum of Squares + Residual Sum of Squares)</p>
<p>This identity holds because, in linear OLS regression, the residuals <span class="math notranslate nohighlight">\(e_i = y_i - f(x_i)\)</span> are mathematically guaranteed to be uncorrelated with the predicted values <span class="math notranslate nohighlight">\(f(x_i)\)</span>. This leads to a key cross-product term being exactly zero. Let’s show this:</p>
<p>Start with the definition of TSS:</p>
<div class="math notranslate nohighlight">
\[ TSS = \sum (y_i - \bar{y})^2 \]</div>
<p>We can add and subtract the predicted value <span class="math notranslate nohighlight">\(f(x_i)\)</span> inside the parentheses:</p>
<div class="math notranslate nohighlight">
\[ TSS = \sum (y_i - f(x_i) + f(x_i) - \bar{y})^2 \]</div>
<p>Group the terms:</p>
<div class="math notranslate nohighlight">
\[ TSS = \sum ( (y_i - f(x_i)) + (f(x_i) - \bar{y}) )^2 \]</div>
<p>Expand the square:</p>
<div class="math notranslate nohighlight">
\[ TSS = \sum (y_i - f(x_i))^2 + \sum (f(x_i) - \bar{y})^2 + 2 \sum (y_i - f(x_i))(f(x_i) - \bar{y}) \]</div>
<div class="math notranslate nohighlight">
\[ TSS = RSS + ESS + 2 \sum e_i (f(x_i) - \bar{y}) \]</div>
<p>For this identity to hold, the final cross-product term must be zero. In OLS linear regression, the method of minimizing the RSS ensures that</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sum e_i f(x_i) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum e_i = 0\)</span></p></li>
<li><p>Since <span class="math notranslate nohighlight">\(\bar{y}\)</span> is a constant, the cross-product term <span class="math notranslate nohighlight">\(2 \sum (y_i - f(x_i))(f(x_i) - \bar{y})\)</span> simplifies to <span class="math notranslate nohighlight">\(2 (\sum e_i f(x_i) - \bar{y} \sum e_i) = 2 (0 - \bar{y} \cdot 0) = 0\)</span>.</p></li>
</ul>
<p>Thus, for linear OLS, we have the clean decomposition: <span class="math notranslate nohighlight">\( TSS = ESS + RSS \)</span>.</p>
<p>This allows <span class="math notranslate nohighlight">\(R^2\)</span> to be interpreted as the proportion of variance explained:</p>
<div class="math notranslate nohighlight">
\[ R^2 = \frac{ESS}{TSS} = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS} \]</div>
<p><strong>Why This Fails for Non-Linear Regression</strong></p>
<p>When you fit a non-linear model using an iterative method like Levenberg-Marquardt, the algorithm minimizes the RSS. However, it <strong>does not guarantee</strong> that the resulting residuals will be uncorrelated with the predicted values in the same way. The cross-product term <span class="math notranslate nohighlight">\(\sum (y_i - f(x_i))(f(x_i) - \bar{y})\)</span> is generally <strong>not zero</strong>.</p>
<p>As a result, the neat identity breaks down:</p>
<div class="math notranslate nohighlight">
\[ TSS \neq ESS + RSS \quad (\text{for non-linear models}) \]</div>
<p><strong>Consequences for R-squared Interpretation:</strong></p>
<ol class="arabic simple">
<li><p><strong>Proportion of Variance is Lost:</strong> Since <span class="math notranslate nohighlight">\(TSS\)</span> no longer neatly partitions into ESS and RSS, the formula <span class="math notranslate nohighlight">\(R^2 = 1 - RSS/TSS\)</span> can no longer be interpreted as the “proportion of variance explained.” It is simply a statement comparing the model’s error <span class="math notranslate nohighlight">\(RSS\)</span> to the error of a baseline model (a horizontal line, <span class="math notranslate nohighlight">\(TSS\)</span>).</p></li>
<li><p><strong>R-squared Can Be Negative:</strong> If a non-linear model provides a worse fit to the data than a simple horizontal line at the mean <span class="math notranslate nohighlight">\(\bar{y}\)</span>, the <span class="math notranslate nohighlight">\(RSS\)</span> can be larger than the TSS. If <span class="math notranslate nohighlight">\(RSS &gt; TSS\)</span>, then <span class="math notranslate nohighlight">\(RSS/TSS &gt; 1\)</span>, resulting in a <strong>negative R-squared</strong>.</p></li>
</ol>
<p>A negative <span class="math notranslate nohighlight">\(R^2\)</span> is a clear signal that the chosen non-linear model is a very poor fit for the data, performing worse than the most basic baseline model. For this reason, while you can calculate a value for <span class="math notranslate nohighlight">\(R^2\)</span> for any model, its interpretation must be handled with extreme care outside of linear regression. It is better viewed as a comparative metric rather than an absolute measure of explained variance.</p>
<p>If we’re going to use <span class="math notranslate nohighlight">\(R^2\)</span> for the non-linear regression, then we need to select between these two formulas:</p>
<div class="math notranslate nohighlight">
\[R^2 = \frac{ESS}{TSS}\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[R^2 = 1 - \frac{RSS}{TSS}\]</div>
<p>First formula cannot be negative but can exceed 1. Second formula cannot exceed 1 but can be negative. While the usage of <span class="math notranslate nohighlight">\(R^2\)</span> is not recommended for non-linear regression, if you still decide to use this metric, the recommendation is to use the second formula:</p>
<div class="math notranslate nohighlight">
\[R^2 = 1 - \frac{RSS}{TSS}\]</div>
<p>This formula is directly tied to the quantity (<span class="math notranslate nohighlight">\(RSS\)</span>) that is minimized during the regression. It compares the sum of the squared errors of your model (<span class="math notranslate nohighlight">\(RSS\)</span>) to the sum of the squared errors you would get from a very simple model that just predicts the mean of the data (<span class="math notranslate nohighlight">\(TSS\)</span>). This formula gives a clear and intuitive meaning to <span class="math notranslate nohighlight">\(R^2\)</span>. It represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). An <span class="math notranslate nohighlight">\(R^2\)</span> of 1 means your model has zero residual error, while an <span class="math notranslate nohighlight">\(R^2\)</span> of 0 means your model is no better than simply predicting the mean. Also, this formula ensures that <span class="math notranslate nohighlight">\(R^2\)</span> will not exceed 1. While it can become negative if the model is a very poor fit (i.e., <span class="math notranslate nohighlight">\(RSS &gt; TSS\)</span>), this is a valid and meaningful result that indicates the model is worse than a simple horizontal line at the mean.</p>
</section>
<section id="root-mean-squared-error-rmse-and-standard-error-of-the-regression-ser">
<h2>Root Mean Squared Error (RMSE) and Standard Error of the Regression (SER)<a class="headerlink" href="#root-mean-squared-error-rmse-and-standard-error-of-the-regression-ser" title="Link to this heading">#</a></h2>
<p>RMSE is derived directly from RSS and is often more interpretable:</p>
<div class="math notranslate nohighlight">
\[RMSE = \sqrt{\frac{RSS}{n}}\]</div>
<p>RMSE represents the typical or average magnitude of the residuals. It gives you a sense of the average “error” your model makes in predicting <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p><strong>Understanding:</strong></p>
<ul class="simple">
<li><p>It’s in the same units as your dependent variable <span class="math notranslate nohighlight">\(y\)</span>. This makes it easier to interpret: “On average, our prediction is off by RMSE units of <span class="math notranslate nohighlight">\(y\)</span>.”</p></li>
<li><p>It’s sensitive to outliers because of the squaring of errors. Large errors contribute disproportionately to RMSE.</p></li>
<li><p>Like RSS, a smaller RMSE indicates a better fit.</p></li>
<li><p>You can compare RMSE values between different models <strong>on the same dataset</strong> to see which one performs better, provided the models have a similar number of parameters. Comparing RMSE across different datasets or datasets with vastly different scales of <span class="math notranslate nohighlight">\(y\)</span> can still be misleading.</p></li>
</ul>
<p>While the Root Mean Squared Error (RMSE) provides a direct and intuitive measure of the average error magnitude, you may encounter a closely related metric known as the <strong>Standard Error of the Regression (SER)</strong>, also called the <em>Residual Standard Error</em> or <em>Standard Error of the Estimate</em>.</p>
<p>The formula for the SER is:</p>
<div class="math notranslate nohighlight">
\[ SER = \sqrt{\frac{RSS}{n - k}} \]</div>
<p>Let’s break down the components and the reasoning behind this formula:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(RSS\)</span> (Residual Sum of Squares):</strong> As before, this is the sum of the squared differences between the observed and predicted values.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(n\)</span>:</strong> The number of data points.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(k\)</span>:</strong> The number of estimated parameters in the model.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(n - k\)</span>:</strong> This term represents the <strong>degrees of freedom</strong> of the residuals.</p></li>
</ul>
<p><strong>Derivation and Rationale: Why <span class="math notranslate nohighlight">\(n - k\)</span>?</strong></p>
<p>The concept of degrees of freedom is central to understanding the SER. When we estimate the <span class="math notranslate nohighlight">\(k\)</span> parameters of our model from the data, we “use up” <span class="math notranslate nohighlight">\(k\)</span> pieces of information. This leaves <span class="math notranslate nohighlight">\((n - k)\)</span> independent pieces of information (the residuals) to estimate the variance of the underlying error in our model.</p>
<p>The term <span class="math notranslate nohighlight">\(RSS / (n - k)\)</span> is the <strong>Mean Squared Error (MSE)</strong>, which is considered an <strong>unbiased estimator</strong> of the variance of the random errors <span class="math notranslate nohighlight">\(\sigma^2\)</span> in the data. An unbiased estimator is one whose expected value is equal to the true population parameter it is trying to estimate. We discuss unbiased estimators in more details <a class="reference internal" href="variance-covariance.html"><span class="std std-doc">here</span></a>.</p>
<p>Let’s formalize this. Assume our model is fundamentally correct and that the observed values <span class="math notranslate nohighlight">\(y_i\)</span> are composed of the true function value plus a random error term <span class="math notranslate nohighlight">\(\epsilon_i\)</span>, which is assumed to have a mean of 0 and a variance of <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[ y_i = f(x_i; \beta_1, ..., \beta_k) + \epsilon_i \]</div>
<p>The RSS is the sum of the squared <em>sample</em> residuals, not the true errors <span class="math notranslate nohighlight">\(\epsilon_i\)</span>. It can be shown mathematically that the expected value of the RSS is:</p>
<div class="math notranslate nohighlight">
\[ E[RSS] = (n - k)\sigma^2 \]</div>
<p>NOTE: We do not show this derivation in this material, however, you can validate derivation of a similar case <a class="reference internal" href="variance-covariance.html"><span class="std std-doc">here</span></a>.</p>
<p>Therefore, to get an unbiased estimate of the true error variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, we must divide RSS by the degrees of freedom:</p>
<div class="math notranslate nohighlight">
\[ \hat{\sigma}^2 = MSE = \frac{RSS}{n - k} \]</div>
<p>The Standard Error of the Regression (SER) is simply the square root of this unbiased variance estimate:</p>
<div class="math notranslate nohighlight">
\[ SER = \hat{\sigma} = \sqrt{\frac{RSS}{n - k}} \]</div>
<p><strong>Comparison: RMSE vs. SER</strong></p>
<ul>
<li><p><strong>RMSE:</strong></p>
<div class="math notranslate nohighlight">
\[ RMSE = \sqrt{\frac{RSS}{n}} \]</div>
<ul class="simple">
<li><p><strong>Interpretation:</strong> The average magnitude of the residual. It answers: “By how much is our model typically wrong in its predictions?”</p></li>
<li><p><strong>Use Case:</strong> Best for comparing the predictive accuracy of different models on the <em>same dataset</em>, especially in machine learning contexts.</p></li>
</ul>
</li>
<li><p><strong>SER:</strong></p>
<div class="math notranslate nohighlight">
\[ SER = \sqrt{\frac{RSS}{n - k}} \]</div>
<ul class="simple">
<li><p><strong>Interpretation:</strong> An estimate of the standard deviation of the underlying, unobservable errors. It answers: “What is the typical size of the random noise in the data that our model cannot explain?”</p></li>
<li><p><strong>Use Case:</strong> Rooted in statistical inference. It is used for calculating confidence intervals and p-values for the model parameters.</p></li>
</ul>
</li>
</ul>
<p>For large datasets (where <span class="math notranslate nohighlight">\(n\)</span> is much larger than <span class="math notranslate nohighlight">\(k\)</span>), the two values will be very close.</p>
</section>
<section id="what-is-the-levenberg-marquardt-algorithm-lma">
<h2>What is the Levenberg-Marquardt Algorithm (LMA)?<a class="headerlink" href="#what-is-the-levenberg-marquardt-algorithm-lma" title="Link to this heading">#</a></h2>
<p>The Levenberg-Marquardt Algorithm (LMA), also known as the damped least-squares (DLS) method, is a powerful and widely used iterative algorithm for solving <strong>non-linear least squares problems</strong>.</p>
<p>Imagine you have a set of data points and you want to fit a mathematical model to them. If the model is non-linear with respect to its parameters, finding the best parameters to minimize the difference between the model’s predictions and the actual data can be challenging. That’s where LMA comes in.</p>
<p>LMA is a hybrid optimization algorithm that cleverly combines two other optimization methods:</p>
<ol class="arabic simple">
<li><p><strong>Gradient Descent (or Steepest Descent):</strong> This method takes steps in the direction opposite to the gradient of the objective function. It’s good at finding a solution when you are far from the minimum but can be slow to converge as you get closer.</p></li>
<li><p><strong>Gauss-Newton Algorithm (GNA):</strong> This method uses the Jacobian matrix (first derivatives) to approximate the objective function as a quadratic and then finds the minimum of that quadratic approximation. It converges very fast when you are close to the minimum, but it can struggle or even diverge if the initial guess is far from the solution or if the Jacobian matrix is singular (not invertible).</p></li>
</ol>
<p><strong>How LMA combines them:</strong></p>
<p>The LMA introduces a “damping factor” (<span class="math notranslate nohighlight">\(\lambda\)</span>) that controls the blend between these two methods:</p>
<ul class="simple">
<li><p><strong>When <span class="math notranslate nohighlight">\(\lambda\)</span> is large:</strong> The algorithm behaves more like <strong>gradient descent</strong>. This makes it more robust when the current parameters are far from the optimal solution, as it ensures a decrease in the error, even if it’s a small step. This helps it avoid getting stuck or diverging in difficult regions of the parameter space.</p></li>
<li><p><strong>When <span class="math notranslate nohighlight">\(\lambda\)</span> is small:</strong> The algorithm behaves more like the <strong>Gauss-Newton method</strong>. As the algorithm approaches the minimum, <span class="math notranslate nohighlight">\(\lambda\)</span> is decreased, allowing for faster convergence.</p></li>
</ul>
<p>The LMA adaptively adjusts this damping factor at each iteration. If a step leads to a significant reduction in the sum of squares, <span class="math notranslate nohighlight">\(\lambda\)</span> is decreased, moving towards the faster Gauss-Newton method. If a step increases the sum of squares (meaning it overshot or moved in the wrong direction), <span class="math notranslate nohighlight">\(\lambda\)</span> is increased, making it more like gradient descent to take smaller, more cautious steps.</p>
<p>This adaptive nature makes LMA very robust and efficient for a wide range of non-linear problems.</p>
<section id="how-is-it-related-to-least-squares-regression">
<h3>How is it related to Least Squares Regression?<a class="headerlink" href="#how-is-it-related-to-least-squares-regression" title="Link to this heading">#</a></h3>
<p>Least squares regression, in its essence, is about finding the parameters of a model that minimize the sum of the squared differences (residuals) between the observed data and the values predicted by the model.</p>
<ul class="simple">
<li><p><strong>Linear Least Squares:</strong> When the model is linear in its parameters (e.g., <span class="math notranslate nohighlight">\(y = ax + b\)</span>), the problem can be solved directly using linear algebra (e.g., normal equations). However, LMA can also resolve this problem but it will not be so resource efficient. If the performance is not so important, then you can use either of algorithms. The next subpage contains code examples for non-linear problem and comparison of various methods for the same linear problem. You can review to see what other options we have to resolve linear problems.</p></li>
<li><p><strong>Non-linear Least Squares:</strong> When the model is non-linear in its parameters (e.g., <span class="math notranslate nohighlight">\(y = A \cdot e^{-Bx} + C\)</span>), there’s no direct analytical solution. This is where iterative optimization algorithms like the Levenberg-Marquardt algorithm come into play.</p></li>
</ul>
<p><strong>The LMA is specifically designed to solve non-linear least squares problems.</strong> It iteratively refines the model parameters by minimizing the sum of squared residuals, which is the core objective of least squares regression.</p>
<p>The objective function that LMA minimizes is typically of the form:</p>
<div class="math notranslate nohighlight">
\[S(\beta) = \sum_{i=1}^{n} (y_i - f(x_i, \beta))^2\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> are the observed data points.</p></li>
<li><p><span class="math notranslate nohighlight">\(f(x_i, \beta)\)</span> is the model function that predicts the dependent variable based on the independent variable <span class="math notranslate nohighlight">\(x_i\)</span> and the parameter vector <span class="math notranslate nohighlight">\(\beta\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p></li>
</ul>
<p>The LMA tries to find the <span class="math notranslate nohighlight">\(\beta\)</span> that minimizes <span class="math notranslate nohighlight">\(S(\beta)\)</span>.</p>
</section>
<section id="scipy-optimize-curve-fit">
<h3><code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code><a class="headerlink" href="#scipy-optimize-curve-fit" title="Link to this heading">#</a></h3>
<p>The Levenberg-Marquardt algorithm <strong>is the default method for <code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code> for unconstrained problems.</strong></p>
<p>NOTE: In the context of optimization, an <strong>unconstrained problem</strong> is an optimization problem where there are <strong>no restrictions or limitations on the values that the parameters (or variables) can take.</strong> The goal is to find the minimum (or maximum) of an objective function over the entire domain of the variables, which is typically the set of all real numbers (<span class="math notranslate nohighlight">\(R^n\)</span> for <span class="math notranslate nohighlight">\(n\)</span> parameters).</p>
<p><strong>Examples of unconstrained problems:</strong></p>
<ul class="simple">
<li><p>Minimizing <span class="math notranslate nohighlight">\(f(x) = x^2 - 4x + 5\)</span>. Here, <span class="math notranslate nohighlight">\(x\)</span> can be any real number.</p></li>
<li><p>Fitting a non-linear curve to data where the parameters of the curve can take any real value (e.g., an amplitude, decay rate, or phase that isn’t physically limited to a specific range).</p></li>
</ul>
<p><strong>In contrast, a constrained problem</strong> has limitations or bounds on the parameter values. These constraints can be:</p>
<ul class="simple">
<li><p><strong>Box constraints (bounds):</strong> Parameters must lie within a specific range (e.g., <span class="math notranslate nohighlight">\(0 \le x \le 10\)</span>, or <span class="math notranslate nohighlight">\(-5 \le y \le 5\)</span>).</p></li>
<li><p><strong>Equality constraints:</strong> Parameters must satisfy certain equations (e.g., <span class="math notranslate nohighlight">\(x + y = 1\)</span>).</p></li>
<li><p><strong>Inequality constraints:</strong> Parameters must satisfy certain inequalities (e.g., <span class="math notranslate nohighlight">\(x^2 + y^2 \le 1\)</span>).</p></li>
</ul>
<p>The Levenberg-Marquardt algorithm, in its original formulation, is designed for unconstrained optimization. When bounds or other constraints are introduced, other algorithms like Trust Region Reflective (<code class="docutils literal notranslate"><span class="pre">trf</span></code>) or Dogbox (<code class="docutils literal notranslate"><span class="pre">dogbox</span></code>) are typically employed, as they are specifically designed to handle such constraints efficiently. This is why <code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code> switches to <code class="docutils literal notranslate"><span class="pre">trf</span></code> when bounds are provided.</p>
<p>Link: <a class="reference external" href="https://docs.scipy.org/doc/scipy-1.16.0/reference/generated/scipy.optimize.curve_fit.html">https://docs.scipy.org/doc/scipy-1.16.0/reference/generated/scipy.optimize.curve_fit.html</a></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./math\01_basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="fourier-transform-01.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Fourier Transform</p>
      </div>
    </a>
    <a class="right-next"
       href="least-squares-regression-code.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Least Squares Regression - Code Examples</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-core-idea-minimizing-differences">The Core Idea: Minimizing Differences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-regression">Least Squares Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-sum-of-squares-rss">Residual Sum of Squares (RSS)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-coefficient-of-determination">R-squared (Coefficient of Determination)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-squared-r-2">Adjusted R-squared <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-for-non-linear-models">R-squared for Non-Linear Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse-and-standard-error-of-the-regression-ser">Root Mean Squared Error (RMSE) and Standard Error of the Regression (SER)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-levenberg-marquardt-algorithm-lma">What is the Levenberg-Marquardt Algorithm (LMA)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-it-related-to-least-squares-regression">How is it related to Least Squares Regression?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scipy-optimize-curve-fit"><code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Vladyslav Yakovliev
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>