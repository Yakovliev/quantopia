
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) &#8212; Quantopia&#39;:&#39; Physics, Python and Pi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'math/01_basics/aic-and-bic';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="WLS - Code Examples Part 3" href="weighted-least-squares-code-3.html" />
    <link rel="prev" title="Goodness of Fit and Chi-Squared Statistic" href="goodness-of-fit-and-chi-squared.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Quantopia':' Physics, Python and Pi - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Quantopia':' Physics, Python and Pi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Quantopia: Physics, Python, and Pi (Alpha Version)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">MATH</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gradient-operator.html">Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradient-directional-derivative.html">Directional Derivative</a></li>
<li class="toctree-l1"><a class="reference internal" href="divergence.html">Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier-transform-01.html">Fourier Transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="least-squares-regression.html">Least Squares Regression, RSS, RMSE, R-squared</a></li>
<li class="toctree-l1"><a class="reference internal" href="least-squares-regression-code.html">Least Squares Regression - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares.html">Ordinary Least Squares (OLS) Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares-code.html">Ordinary Least Squares (OLS) Regression - Code Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="variance-covariance.html">Variance and Covariance</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares.html">Weighted Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-1.html">WLS - Code Examples Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-2.html">WLS - Code Examples Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="goodness-of-fit-and-chi-squared.html">Goodness of Fit and Chi-Squared Statistic</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-3.html">WLS - Code Examples Part 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="orthogonal-distance-regression.html">Orthogonal Distance Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PHYSICS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../physics/continuity-equation-01.html">The Continuity Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/continuity-equation-02.html">The Continuity Equation: One-Dimensional Advection of a Density Profile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/ensemble.html">Statistical Ensembles and Liouville’s Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/microcanonical-ensemble.html">Microcanonical Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/fokker-planck-equation-example.html">Fokker-Planck Equation - Example Analysis (preview)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DATA SCIENCE AND MACHINE LEARNING</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../data-science/knn-algorithm.html">K-Nearest Neighbors (KNN) Algorithm (preview)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data-science/naive-bayes.html">Naive Bayes Method (preview)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data-science/logistic-regression.html">Logistic Regression (preview)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/Yakovliev/quantopia/blob/main/book/math/01_basics/aic-and-bic.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia/issues/new?title=Issue%20on%20page%20%2Fmath/01_basics/aic-and-bic.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/math/01_basics/aic-and-bic.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#akaike-information-criterion-aic">Akaike Information Criterion (AIC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-information-criterion-bic">Bayesian Information Criterion (BIC)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-and-when-to-use-which">Key Differences and When to Use Which</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-the-log-likelihood-for-least-squares">Derivation of the Log-Likelihood for Least Squares</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-this-for-aic-and-bic">How to Use This for AIC and BIC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-case-of-unknown-variance-ordinary-least-squares-ols">The Case of Unknown Variance: Ordinary Least Squares (OLS)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-unknown-variance-sigma-2">Estimating the Unknown Variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-maximized-log-likelihood">The Maximized Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-aic-and-bic-with-unknown-variance">Calculating AIC and BIC with Unknown Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modification-for-small-sample-size-the-corrected-aic-aicc">Modification for Small Sample Size: The Corrected AIC (AICc)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional Materials</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="akaike-information-criterion-aic-and-bayesian-information-criterion-bic">
<h1>Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)<a class="headerlink" href="#akaike-information-criterion-aic-and-bayesian-information-criterion-bic" title="Link to this heading">#</a></h1>
<p>Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are both statistical tools used for <strong>model selection</strong>. They help you choose the best model from a set of candidate models by balancing two competing goals: <strong>goodness of fit</strong> and <strong>model complexity</strong>.</p>
<p>Both criteria are particularly useful when comparing models that may have different numbers of parameters, as they penalize models for being more complex to prevent <strong>overfitting</strong>.</p>
<section id="akaike-information-criterion-aic">
<h2>Akaike Information Criterion (AIC)<a class="headerlink" href="#akaike-information-criterion-aic" title="Link to this heading">#</a></h2>
<p>AIC is an estimator of prediction error and is rooted in information theory. It estimates the relative amount of information a model loses when representing the process that generated the data. The model with the lowest AIC value is considered the best among the candidate models.</p>
<p>The formula is:</p>
<div class="math notranslate nohighlight">
\[AIC = -2 \ln(\hat{L}) + 2k\]</div>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(-2 \ln(\hat{L})\)</span></strong>: This is the <strong>goodness-of-fit term</strong>. It’s derived from the maximum likelihood (<span class="math notranslate nohighlight">\(\hat{L}\)</span>) of the model, which measures how well the model fits the data. A higher likelihood (and thus a smaller negative log-likelihood) indicates a better fit.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(2k\)</span></strong>: This is the <strong>penalty term</strong>. It’s a penalty for model complexity, where <span class="math notranslate nohighlight">\(k\)</span> is the total number of parameters estimated by the model. A more complex model (one with more parameters) gets a higher penalty.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(k\)</span> is equal to the number of structural parameters in the model’s function <span class="math notranslate nohighlight">\(f(x_i, \beta)\)</span> (e.g., the <span class="math notranslate nohighlight">\(\beta\)</span> coefficients in a regression model) if the error variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> is known. However, if the error variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> is unknown, it must be estimated from the data, which means it counts as an additional estimated parameter and <span class="math notranslate nohighlight">\(k = p + 1\)</span>. We will review this in detail below.</p>
<p>The AIC’s goal is to find the model that best approximates the unknown data-generating process, even if that process isn’t one of the candidate models. It’s focused on <strong>predictive accuracy</strong>.</p>
</section>
<section id="bayesian-information-criterion-bic">
<h2>Bayesian Information Criterion (BIC)<a class="headerlink" href="#bayesian-information-criterion-bic" title="Link to this heading">#</a></h2>
<p>BIC, also known as the Schwarz Information Criterion (SIC), is a criterion for model selection derived from a Bayesian perspective. Like AIC, it balances goodness of fit and complexity, but it does so more aggressively. The model with the lowest BIC value is the one preferred.</p>
<p>The formula is:</p>
<div class="math notranslate nohighlight">
\[BIC = -2 \ln(\hat{L}) + k \ln(n)\]</div>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(-2 \ln(\hat{L})\)</span></strong>: This is the same goodness-of-fit term as in AIC.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(k \ln(n)\)</span></strong>: This is the <strong>penalty term</strong>. The penalty for complexity is stronger than AIC’s because it includes the natural logarithm of the number of data points (<span class="math notranslate nohighlight">\(n\)</span>).</p></li>
</ul>
<p>Because of the <span class="math notranslate nohighlight">\(\ln(n)\)</span> factor, BIC applies a much heavier penalty for additional parameters, especially as the sample size grows. This means BIC tends to favor <strong>simpler models</strong> more strongly than AIC. It assumes that one of the candidate models is the “true” model, and its goal is to find that true model.</p>
<section id="key-differences-and-when-to-use-which">
<h3>Key Differences and When to Use Which<a class="headerlink" href="#key-differences-and-when-to-use-which" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Feature</p></th>
<th class="head text-left"><p>Akaike Information Criterion (AIC)</p></th>
<th class="head text-left"><p>Bayesian Information Criterion (BIC)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Penalty for Complexity</strong></p></td>
<td class="text-left"><p>Penalizes with <span class="math notranslate nohighlight">\(2k\)</span>.</p></td>
<td class="text-left"><p>Penalizes with <span class="math notranslate nohighlight">\(k \ln(n)\)</span>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Focus</strong></p></td>
<td class="text-left"><p>Finds the best <strong>approximating</strong> model for predictive accuracy.</p></td>
<td class="text-left"><p>Finds the “true” model.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Sample Size</strong></p></td>
<td class="text-left"><p>The penalty is constant with respect to sample size.</p></td>
<td class="text-left"><p>The penalty increases with sample size, favoring simpler models.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Behavior</strong></p></td>
<td class="text-left"><p>Tends to select more complex models than BIC.</p></td>
<td class="text-left"><p>Tends to select more simpler models than AIC.</p></td>
</tr>
</tbody>
</table>
</div>
<p>NOTE: The terms “approximating model” and “true model” refer to their underlying assumptions about the reality you are trying to model.</p>
<p><strong>AIC assumes that the “true” data-generating process is infinitely complex and therefore unknowable.</strong> We can only ever hope to approximate it with our models.</p>
<ul class="simple">
<li><p><strong>Goal</strong>: AIC’s goal is to find the model that provides the best trade-off between bias (how far your model is from the true process) and variance (how much your model would change with different data). In simpler terms, it’s about building the model that will give you the most accurate predictions on a new, unseen dataset.</p></li>
<li><p><strong>Best for</strong>: This makes AIC ideal for tasks where <strong>prediction</strong> is the primary goal, such as forecasting future stock prices, predicting customer behavior, or machine learning applications. You’re not trying to discover the fundamental laws of the universe, you’re just trying to make the most useful prediction you can.</p></li>
<li><p><strong>Mathematical Justification</strong>: AIC’s penalty term (<span class="math notranslate nohighlight">\(2k\)</span>) is derived from the Kullback-Leibler (KL) divergence, which is a measure of the information lost when approximating reality with a given model. By minimizing AIC, you are minimizing this information loss.</p></li>
</ul>
<p><strong>BIC assumes that there is a “true” model that generated the data, and this model exists within your set of candidate models.</strong> This is often a good assumption for scientific fields where we believe there are underlying, fixed physical laws governing a system.</p>
<ul class="simple">
<li><p><strong>Goal</strong>: BIC’s goal is to select the model that is most likely to be this “true” model. Because of its heavier penalty for more parameters (<span class="math notranslate nohighlight">\(k\ln(n)\)</span>), BIC is more likely to choose simpler, more parsimonious models. It essentially bets that the simplest model that explains the data well is the correct one.</p></li>
<li><p><strong>Best for</strong>: This makes BIC more suitable for tasks of <strong>explanation and discovery</strong>, where you want to find the most fundamental, elegant, and concise model that describes a phenomenon. This is common in fields like physics, biology, and some social sciences.</p></li>
<li><p><strong>Mathematical Justification</strong>: BIC is derived from Bayesian inference and is an approximation of the posterior probability of a model being the true model, given the data. By minimizing BIC, you are maximizing this posterior probability.</p></li>
</ul>
<p>In essence:</p>
<ul class="simple">
<li><p><strong>AIC</strong> asks: “Which model will give me the most accurate predictions for the future?”</p></li>
<li><p><strong>BIC</strong> asks: “Which model is most likely to be the true explanation for the data I have?”</p></li>
</ul>
<p>This is why AIC often chooses a slightly more complex model than BIC. AIC is willing to accept a little extra complexity if it improves predictive accuracy, while BIC is more conservative, preferring a simpler model unless the evidence for a more complex one is overwhelming.</p>
<p>In practice, if your goal is to find the model with the highest predictive power, AIC is often preferred. If your goal is to select the most efficient or fundamental model that is likely to be the “true” one, BIC might be a better choice.</p>
<p>When you compare two models, you find AIC Difference <span class="math notranslate nohighlight">\(\Delta \text{AIC}\)</span> and BIC Difference <span class="math notranslate nohighlight">\(\Delta \text{BIC}\)</span> for these modesl. Here are some general guidelines for interpreting these differences, based on conventions in the scientific community:</p>
<ul class="simple">
<li><p><strong>AIC Difference (<span class="math notranslate nohighlight">\(\Delta \text{AIC}\)</span>)</strong>: A difference of more than 2 between two models is often considered significant. If the difference is between 0 and 2, the models are considered to have a similar level of support from the data. A model with a <span class="math notranslate nohighlight">\(\Delta\)</span>AIC of 10 or more is considered to have very little support compared to the best model.</p></li>
<li><p><strong>BIC Difference (<span class="math notranslate nohighlight">\(\Delta \text{BIC}\)</span>)</strong>: Because BIC has a stronger penalty for complexity, its differences are interpreted more stringently. A difference of 0-2 is considered as weak evidence for the model with the lower BIC, a difference of 2-6 is considered positive evidence, a difference of 6-10 is strong evidence, and a difference of more than 10 is considered very strong evidence.</p></li>
</ul>
</section>
</section>
<section id="maximum-likelihood-estimation">
<h2>Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Link to this heading">#</a></h2>
<p>Maximum Likelihood Estimation (MLE) is a powerful and flexible method for estimating the parameters of a statistical model. It’s used to find the parameter values that make the observed data most probable. The <strong>likelihood function</strong> quantifies how probable the observed data is for a given set of parameters. The <strong>log-likelihood function</strong> is simply the natural logarithm of the likelihood function.</p>
<p>The core idea of MLE is to find the parameter values that maximize the likelihood function, <span class="math notranslate nohighlight">\(L(\theta|x)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> represents the parameters and <span class="math notranslate nohighlight">\(x\)</span> is the data. The likelihood function is often a product of probability density functions (or probability mass functions) for each data point. For a set of independent and identically distributed (i.i.d.) observations, the likelihood is:</p>
<div class="math notranslate nohighlight">
\[L(\theta|x) = \prod_{i=1}^{n} f(x_i|\theta)\]</div>
<p>Working with this product can be difficult, especially with many data points, as multiplying many small numbers can lead to computational underflow. This is where the <strong>log-likelihood</strong> comes in. By taking the natural logarithm, the product is transformed into a sum, which is much more computationally stable and easier to differentiate.</p>
<div class="math notranslate nohighlight">
\[\ln L(\theta|x) = \ln \left( \prod_{i=1}^{n} f(x_i|\theta) \right) = \sum_{i=1}^{n} \ln f(x_i|\theta)\]</div>
<p>Since the logarithm is a monotonic function, maximizing the log-likelihood function yields the <strong>exact same parameter estimates</strong> as maximizing the likelihood function.</p>
<p>The connection between Maximum Likelihood and regression modeling becomes clear when we make assumptions about the distribution of the errors. While Ordinary Least Squares (OLS) regression minimizes the sum of squared residuals, MLE provides a more general framework that can lead to the same result under specific conditions.</p>
<p><strong>For OLS:</strong>
The OLS method finds the coefficients that minimize the <strong>sum of squared residuals</strong> (or errors). That is, it minimizes the following:</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1x_i))^2\]</div>
<p><strong>Relation to MLE:</strong>
The OLS solution is the Maximum Likelihood Estimate for a linear regression model <strong>if we assume that the error terms (<span class="math notranslate nohighlight">\(\epsilon_i\)</span>) are independently and identically distributed (i.i.d.) according to a normal distribution with a mean of zero and a constant variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>).</strong></p>
<p>When we make this assumption, the probability of observing a particular data point <span class="math notranslate nohighlight">\(y_i\)</span> for a given <span class="math notranslate nohighlight">\(x_i\)</span> is a normal distribution centered at the predicted value, <span class="math notranslate nohighlight">\(\beta_0 + \beta_1x_i\)</span>. The log-likelihood function for this model can be written as:</p>
<div class="math notranslate nohighlight">
\[\ln L(\beta_0, \beta_1, \sigma^2|x,y) = -\frac{n}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1x_i))^2\]</div>
<p>NOTE: We will derive a similar formula below but in a bit more general form.</p>
<p>Notice the last term in the equation. To maximize the log-likelihood function, we need to maximize this entire expression. The first term is a constant with respect to the <span class="math notranslate nohighlight">\(\beta\)</span> coefficients, and the second term is negative. Therefore, to make the overall value as large as possible, we must make the negative term as small as possible. This is equivalent to minimizing the <strong>sum of squared residuals</strong>, which is exactly what OLS does.</p>
<p><strong>Why is this important?</strong> This connection shows that OLS is not just an arbitrary method for fitting a line; it has a probabilistic foundation under the assumption of normally distributed errors. For other types of regression, such as logistic regression, there is no OLS equivalent. Instead, the parameters are always estimated using Maximum Likelihood, where the log-likelihood function is based on the appropriate distribution for the outcome variable (e.g., a Bernoulli distribution for binary outcomes).</p>
</section>
<section id="derivation-of-the-log-likelihood-for-least-squares">
<h2>Derivation of the Log-Likelihood for Least Squares<a class="headerlink" href="#derivation-of-the-log-likelihood-for-least-squares" title="Link to this heading">#</a></h2>
<p>NOTE: <span class="math notranslate nohighlight">\(\ln(L)\)</span> denotes log-likelihood function. <span class="math notranslate nohighlight">\(\ln(\hat{L})\)</span> denotes the specific value of the log-likelihood function after the model has been fitted to the data and this specific value is maximized log-likelihood.</p>
<p>Here is the derivation (note that we derive all the formulas in this section under the assumption that we know exact values of <span class="math notranslate nohighlight">\(\sigma_i\)</span> or <span class="math notranslate nohighlight">\(\sigma\)</span>):</p>
<ol class="arabic">
<li><p><strong>Start with the probability of one data point.</strong> We assume that the probability of observing a single data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> given the model’s prediction <span class="math notranslate nohighlight">\(f(x_i, \beta)\)</span> is described by a normal distribution with a mean of <span class="math notranslate nohighlight">\(f(x_i, \beta)\)</span> and a standard deviation of <span class="math notranslate nohighlight">\(\sigma_i\)</span>. This is equivalent to the assumption that the error terms <span class="math notranslate nohighlight">\(\epsilon_i\)</span> are independently distributed according to a normal distribution with a mean of zero and a variance <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>, so <span class="math notranslate nohighlight">\(\epsilon_i \sim N(0, \sigma_i^2)\)</span> (remember that <span class="math notranslate nohighlight">\(y_i = f(x_i, \beta) + \epsilon_i\)</span>). The probability density function (PDF) for a single point is:</p>
<div class="math notranslate nohighlight">
\[p(y_i) = \frac{1}{\sqrt{2\pi\sigma_i^2}} e^{-\frac{(y_i - f(x_i, \beta))^2}{2\sigma_i^2}}\]</div>
</li>
<li><p><strong>Form the likelihood function.</strong> Assuming each data point is an independent measurement, the total likelihood (<span class="math notranslate nohighlight">\(L\)</span>) of observing all <span class="math notranslate nohighlight">\(n\)</span> data points is the product of their individual probabilities:</p>
<div class="math notranslate nohighlight">
\[L = \prod_{i=1}^{n} p(y_i) = \prod_{i=1}^{n} \left( \frac{1}{\sqrt{2\pi\sigma_i^2}} e^{-\frac{(y_i - f(x_i, \beta))^2}{2\sigma_i^2}} \right)\]</div>
</li>
<li><p><strong>Take the natural logarithm.</strong> To simplify the product and make the calculations more manageable, we take the natural logarithm of the likelihood function. A product becomes a sum, and the exponential term simplifies.</p>
<div class="math notranslate nohighlight">
\[\ln(L) = \ln \left( \prod_{i=1}^{n} p(y_i) \right) = \sum_{i=1}^{n} \ln(p(y_i))\]</div>
<div class="math notranslate nohighlight">
\[\ln(L) = \sum_{i=1}^{n} \ln \left( \frac{1}{\sqrt{2\pi\sigma_i^2}} e^{-\frac{(y_i - f(x_i, \beta))^2}{2\sigma_i^2}} \right)\]</div>
</li>
<li><p><strong>Simplify the expression.</strong> Using logarithm rules (<span class="math notranslate nohighlight">\(\ln(a \cdot b) = \ln(a) + \ln(b)\)</span> and <span class="math notranslate nohighlight">\(\ln(e^x) = x\)</span>), we can expand the sum:</p>
<div class="math notranslate nohighlight">
\[\ln(L) = \sum_{i=1}^{n} \left( \ln\left(\frac{1}{\sqrt{2\pi\sigma_i^2}}\right) - \frac{(y_i - f(x_i, \beta))^2}{2\sigma_i^2} \right)\]</div>
<div class="math notranslate nohighlight">
\[\ln(L) = - \sum_{i=1}^{n} \ln(\sqrt{2\pi\sigma_i^2}) - \frac{1}{2} \sum_{i=1}^{n} \frac{(y_i - f(x_i, \beta))^2}{\sigma_i^2}\]</div>
</li>
<li><p>The second term in this equation is directly related to our Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) statistic. Since <span class="math notranslate nohighlight">\(\chi^2 = \sum_{i=1}^{N} \frac{(y_i - f(x_i, \beta))^2}{\sigma_i^2}\)</span>, we can rewrite the equation as:</p>
<div class="math notranslate nohighlight">
\[\ln(L) = - \frac{1}{2}\chi^2 - \sum_{i=1}^{n} \ln(\sqrt{2\pi\sigma_i^2})\]</div>
</li>
<li><p>When comparing different models on the same dataset, the second term, which depends on the known measurement uncertainties (<span class="math notranslate nohighlight">\(\sigma_i\)</span>), is a constant and doesn’t affect which model is selected. Therefore, for our purposes, we can write a simplified and more practical relationship:</p>
<div class="math notranslate nohighlight">
\[\ln(L) \propto - \frac{1}{2}\chi^2\]</div>
<p>This means that maximizing the log-likelihood is equivalent to <strong>minimizing the chi-squared statistic</strong>.</p>
</li>
<li><p>If <span class="math notranslate nohighlight">\(\sigma_i = \sigma\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> (assumption of homoscedasticity), we can simplify this expression even more:</p>
<div class="math notranslate nohighlight">
\[\ln(L) = - \sum_{i=1}^{n} \ln(\sqrt{2\pi\sigma^2}) - \frac{1}{2} \sum_{i=1}^{n} \frac{(y_i - f(x_i, \beta))^2}{\sigma^2}\]</div>
<div class="math notranslate nohighlight">
\[\ln(L) = - n \ln(\sqrt{2\pi\sigma^2}) - \frac{1}{2} \sum_{i=1}^{n} \frac{(y_i - f(x_i, \beta))^2}{\sigma^2}\]</div>
<div class="math notranslate nohighlight">
\[\ln(L) = - \frac{n}{2} \ln(2\pi\sigma^2) - \frac{1}{2} \sum_{i=1}^{n} \frac{(y_i - f(x_i, \beta))^2}{\sigma^2}\]</div>
<div class="math notranslate nohighlight">
\[\ln(L) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\sigma^2) - \frac{1}{2} \sum_{i=1}^{n} \frac{(y_i - f(x_i, \beta))^2}{\sigma^2}\]</div>
<div class="math notranslate nohighlight">
\[\ln(L) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\sigma^2) - \frac{1}{2} \chi^2\]</div>
</li>
</ol>
<section id="how-to-use-this-for-aic-and-bic">
<h3>How to Use This for AIC and BIC<a class="headerlink" href="#how-to-use-this-for-aic-and-bic" title="Link to this heading">#</a></h3>
<p>Since the AIC and BIC formulas depend on the maximized log-likelihood <span class="math notranslate nohighlight">\(-2\ln(\hat{L})\)</span>, we can use the results of our derivation. When a fitting procedure like <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> finds the optimal parameters <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, it does so by minimizing the <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic. This is the best-fit of <span class="math notranslate nohighlight">\(\chi^2\)</span>. Let’s call this minimized value <span class="math notranslate nohighlight">\(\chi^2_{min}\)</span>.</p>
<p>Because maximizing the log-likelihood is equivalent to minimizing the chi-squared statistic, the maximized log-likelihood, <span class="math notranslate nohighlight">\(\ln(\hat{L})\)</span>, is given by:</p>
<div class="math notranslate nohighlight">
\[\ln(\hat{L}) \propto - \frac{1}{2}\chi^2_{min}\]</div>
<p>Now we can calculate the information criteria using this <strong>minimized</strong> <span class="math notranslate nohighlight">\(\chi^2\)</span> value from the fit:</p>
<ul>
<li><p><strong>For AIC:</strong></p>
<div class="math notranslate nohighlight">
\[-2\ln(\hat{L}) = -2 \left( - \frac{1}{2}\chi^2_{min} + \text{constant} \right) = \chi^2_{min} + \text{constant}'\]</div>
<p>Since the constants don’t affect model ranking, we can simply use the proportional formula:</p>
<div class="math notranslate nohighlight">
\[AIC \propto \chi^2_{min} + 2k\]</div>
<p>NOTE: This last formula is valid for model comparison only (not valid to calculate the exact value of AIC). Also, this last formula is valid only if <span class="math notranslate nohighlight">\(\sigma_i\)</span> values are known.</p>
</li>
<li><p><strong>For BIC:</strong></p>
<div class="math notranslate nohighlight">
\[BIC \propto \chi^2_{min} + k\ln(n)\]</div>
<p>NOTE: This last formula is valid for model comparison only (not valid to calculate the exact value of AIC). Also, this last formula is valid only if <span class="math notranslate nohighlight">\(\sigma_i\)</span> values are known.</p>
</li>
</ul>
<p>Remember: <span class="math notranslate nohighlight">\(\chi^2_{min}\)</span> is the final chi-squared value calculated from the Weighted Least Squares, <span class="math notranslate nohighlight">\(k\)</span> is the number of parameters, and <span class="math notranslate nohighlight">\(n\)</span> is the number of data points. By calculating these values for different models fitted to the same data, you can choose the model with the lowest AIC or BIC as the one that provides the best balance of fit and parsimony.</p>
<p>If the standard deviations <span class="math notranslate nohighlight">\(\sigma_i\)</span> are the same for all the data points and are equal to <span class="math notranslate nohighlight">\(\sigma\)</span> (homoscedasticity), then the maximized log-likelihood to be used in the AIC and BIC formulas is:</p>
<div class="math notranslate nohighlight">
\[\ln(\hat{L}) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\sigma^2) - \frac{1}{2} \chi^2_{min}\]</div>
</section>
</section>
<section id="the-case-of-unknown-variance-ordinary-least-squares-ols">
<h2>The Case of Unknown Variance: Ordinary Least Squares (OLS)<a class="headerlink" href="#the-case-of-unknown-variance-ordinary-least-squares-ols" title="Link to this heading">#</a></h2>
<p>In the previous chapter, we operated under the assumption that the standard deviations of the errors, <span class="math notranslate nohighlight">\(\sigma_i\)</span>, were known. This is common in the physical sciences where measurement uncertainties can be well-defined. However, in many other fields, these uncertainties are not known beforehand.</p>
<p>The most common scenario is to assume that while the variance is unknown, it is constant for all data points. This is the assumption of <strong>homoscedasticity</strong> (<span class="math notranslate nohighlight">\(\sigma_i = \sigma\)</span> for all <em>i</em>), which is fundamental to <strong>Ordinary Least Squares (OLS)</strong>. When <span class="math notranslate nohighlight">\(\sigma\)</span> is unknown, it becomes another parameter that we must estimate from the data.</p>
<section id="estimating-the-unknown-variance-sigma-2">
<h3>Estimating the Unknown Variance <span class="math notranslate nohighlight">\(\sigma^2\)</span><a class="headerlink" href="#estimating-the-unknown-variance-sigma-2" title="Link to this heading">#</a></h3>
<p>Our goal is to maximize the log-likelihood function with respect to all model parameters, which now include the <span class="math notranslate nohighlight">\(\beta\)</span> parameters <em>and</em> the unknown variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>We start with the log-likelihood function for the homoscedastic case, as derived in Step 7 of the previous chapter:</p>
<div class="math notranslate nohighlight">
\[\ln(L(\beta, \sigma^2)) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - f(x_i, \beta))^2\]</div>
<p>This expression is often written in terms of the <strong>Residual Sum of Squares (RSS)</strong>, where <span class="math notranslate nohighlight">\(RSS = \sum_{i=1}^{n} (y_i - f(x_i, \beta))^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[\ln(L(\beta, \sigma^2)) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\sigma^2) - \frac{RSS}{2\sigma^2}\]</div>
<p>To find the values of <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> that maximize this function, we can use a two-step process:</p>
<ol class="arabic simple">
<li><p>For any given value of <span class="math notranslate nohighlight">\(\sigma^2\)</span>, maximizing the log-likelihood is equivalent to minimizing the <code class="docutils literal notranslate"><span class="pre">RSS</span></code> term. This is exactly what the standard least-squares fitting procedure does to find the optimal parameters <span class="math notranslate nohighlight">\(\beta\)</span>.</p></li>
<li><p>Once the optimal <span class="math notranslate nohighlight">\(\beta\)</span> parameters are found and the <code class="docutils literal notranslate"><span class="pre">RSS</span></code> is minimized, we can find the <strong>Maximum Likelihood Estimate (MLE)</strong> for the variance, denoted as <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>, by taking the partial derivative of the log-likelihood function with respect to <span class="math notranslate nohighlight">\(\sigma^2\)</span> and setting it to zero.</p></li>
</ol>
<p>Let’s perform the second step. Treating <code class="docutils literal notranslate"><span class="pre">RSS</span></code> as a constant (since <span class="math notranslate nohighlight">\(\beta\)</span> is now fixed), we differentiate with respect to <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \ln(L)}{\partial \sigma^2} = - \frac{n}{2\sigma^2} + \frac{RSS}{2(\sigma^2)^2}\]</div>
<p>Setting the derivative to zero to find the maximum:</p>
<div class="math notranslate nohighlight">
\[\frac{n}{2\hat{\sigma}^2} = \frac{RSS}{2(\hat{\sigma}^2)^2}\]</div>
<p>Solving for <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> gives us the MLE for the variance:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}^2 = \frac{RSS}{n}\]</div>
<blockquote>
<div><p>This is a fundamentally important result: the maximum likelihood estimate for the unknown variance is the residual sum of squares divided by the number of data points.</p>
</div></blockquote>
<p>There are several important notes regarding this results:</p>
<ol class="arabic simple">
<li><p>The Maximum Likelihood Estimate (MLE) for the variance, <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = RSS/n\)</span>, is a <strong>biased estimator</strong>.</p></li>
<li><p>The unbiased estimator, <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = RSS/(n-p)\)</span>, is what is typically used for inference and is reported in most statistical software as the “Mean Squared Error” (MSE). Here, <span class="math notranslate nohighlight">\(p\)</span> is the number of model parameters.</p></li>
<li><p><strong>For calculating the log-likelihood and the information criteria (AIC, BIC) derived from it, we MUST use the biased Maximum Likelihood Estimate (<span class="math notranslate nohighlight">\(RSS/n\)</span>).</strong></p></li>
</ol>
<p>Let’s review the last statement in more details.</p>
<p>Our primary goal was to find the parameter values that maximize the log-likelihood function. We found that the value of <span class="math notranslate nohighlight">\(\sigma^2\)</span> that maximizes this function (after <code class="docutils literal notranslate"><span class="pre">RSS</span></code> has been minimized by finding the best <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> parameters) is:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}^2_{MLE} = \frac{RSS}{n} = \frac{1}{n}\sum_{i=1}^{n} (y_i - f(x_i, \hat{\beta}))^2\]</div>
<p>This is the <strong>Maximum Likelihood Estimate (MLE)</strong> for the variance. While it is the correct value for maximizing the likelihood, it is a <strong>biased estimator</strong> of the true population variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. On average, it will slightly underestimate the true variance.</p>
<p>In applied statistics, the goal is often to get the most accurate possible estimate of the true population variance. For this, we use the <strong>unbiased estimator</strong>, which corrects for the bias mentioned above by adjusting the denominator. If we have estimated <span class="math notranslate nohighlight">\(k\)</span> model parameters (i.e., the number of coefficients in <span class="math notranslate nohighlight">\(\beta\)</span>), the unbiased estimate is:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}^2_{unbiased} = \frac{RSS}{n-p}\]</div>
<p>This is the <strong>Mean Squared Error (MSE)</strong>, and its square root is the <strong>Residual Standard Error (RSE)</strong> reported in the output of virtually all OLS regression software. The denominator, <span class="math notranslate nohighlight">\(n-p\)</span>, represents the residual <strong>degrees of freedom</strong>. By dividing by a smaller number, we increase the value of the estimate, correcting for the downward bias of the MLE.</p>
<p><strong>Which estimator should we use for the log-likelihood?</strong></p>
<p>The answer lies in the fundamental definition of AIC and BIC: they are based on the <strong>maximized value of the log-likelihood function</strong>.</p>
<p>The derivation of AIC and BIC starts with <span class="math notranslate nohighlight">\(\ln(\hat{L})\)</span>, which is the value of <span class="math notranslate nohighlight">\(\ln(L)\)</span> when <em>all</em> parameters have been set to their Maximum Likelihood Estimates. Therefore, to be mathematically consistent, we <strong>must</strong> use the MLE for the variance, <span class="math notranslate nohighlight">\(\hat{\sigma}^2_{MLE} = RSS/n\)</span>, when calculating the log-likelihood for AIC and BIC.</p>
<p>Plugging the unbiased estimate <span class="math notranslate nohighlight">\(RSS/(n-p)\)</span> into the log-likelihood formula would result in a value that is <em>not</em> the true maximum likelihood, and the theoretical foundation of AIC and BIC would no longer hold.</p>
<p><strong>In summary:</strong></p>
<ul class="simple">
<li><p>For <strong>calculating AIC and BIC</strong>, use the <strong>biased</strong> MLE variance: <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = RSS/n\)</span>.</p></li>
<li><p>For <strong>reporting the model’s error variance or standard error for inference</strong>, use the <strong>unbiased</strong> variance: <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = RSS/(n-p)\)</span>.</p></li>
</ul>
</section>
<section id="the-maximized-log-likelihood">
<h3>The Maximized Log-Likelihood<a class="headerlink" href="#the-maximized-log-likelihood" title="Link to this heading">#</a></h3>
<p>Now, we can substitute this estimate <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> back into the log-likelihood equation to get the <em>maximized</em> log-likelihood, <span class="math notranslate nohighlight">\(\ln(\hat{L})\)</span>. This value represents the highest possible likelihood given the data and the model form.</p>
<div class="math notranslate nohighlight">
\[\ln(\hat{L}) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln\left(\frac{RSS}{n}\right) - \frac{RSS}{2\left(\frac{RSS}{n}\right)}\]</div>
<p>Simplifying the last term:</p>
<div class="math notranslate nohighlight">
\[\ln(\hat{L}) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln\left(\frac{RSS}{n}\right) - \frac{n}{2}\]</div>
<p>Using the logarithm rule <span class="math notranslate nohighlight">\(\ln(a/b) = \ln(a) - \ln(b)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\ln(\hat{L}) = - \frac{n}{2} \left( \ln(2\pi) + \ln(RSS) - \ln(n) + 1 \right)\]</div>
<p>This is the final expression for the maximized log-likelihood when <span class="math notranslate nohighlight">\(\sigma^2\)</span> is unknown.</p>
</section>
<section id="calculating-aic-and-bic-with-unknown-variance">
<h3>Calculating AIC and BIC with Unknown Variance<a class="headerlink" href="#calculating-aic-and-bic-with-unknown-variance" title="Link to this heading">#</a></h3>
<p>With the maximized log-likelihood, <span class="math notranslate nohighlight">\(\ln(\hat{L})\)</span>, we can now calculate the information criteria. A crucial point is that we have estimated an additional parameter: the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Therefore, the total number of estimated parameters, <span class="math notranslate nohighlight">\(k\)</span>, becomes:</p>
<ul class="simple">
<li><p><strong>Number of Parameters <span class="math notranslate nohighlight">\(k\)</span>:</strong> If your model has <span class="math notranslate nohighlight">\(p\)</span> parameters for the function <span class="math notranslate nohighlight">\(f(x,\beta)\)</span>, then the total number of estimated parameters is <span class="math notranslate nohighlight">\(k = p + 1\)</span>.</p></li>
<li><p><strong>Example:</strong> For instance, in a simple linear regression model <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x\)</span>, there are two <span class="math notranslate nohighlight">\(\beta\)</span> parameters (<span class="math notranslate nohighlight">\(p=2\)</span>). When we also estimate the variance, the total number of parameters for the AIC/BIC calculation becomes <span class="math notranslate nohighlight">\(k = p + 1 = 3\)</span>.</p></li>
</ul>
<p>The general formulas for AIC and BIC are:</p>
<div class="math notranslate nohighlight">
\[AIC = -2\ln(\hat{L}) + 2k\]</div>
<div class="math notranslate nohighlight">
\[BIC = -2\ln(\hat{L}) + k\ln(n)\]</div>
<p>We will use our expression for the maximized log-likelihood:
<span class="math notranslate nohighlight">\(\ln(\hat{L}) = - \frac{n}{2} \left( \ln(2\pi) + \ln(RSS) - \ln(n) + 1 \right)\)</span></p>
<p><strong>For AIC:</strong></p>
<p>Substituting the expression for <span class="math notranslate nohighlight">\(\ln(\hat{L})\)</span> into the AIC formula gives:</p>
<div class="math notranslate nohighlight">
\[AIC = -2 \left[ - \frac{n}{2} \left( \ln(2\pi) + \ln(RSS) - \ln(n) + 1 \right) \right] + 2k\]</div>
<div class="math notranslate nohighlight">
\[AIC = n \left( \ln(2\pi) + \ln(RSS) - \ln(n) + 1 \right) + 2k\]</div>
<p>For the purpose of comparing different models on the same dataset, any terms that do not depend on the model can be dropped. The terms <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\ln(2\pi)\)</span>, <span class="math notranslate nohighlight">\(\ln(n)\)</span>, and 1 are constant across all models. This leaves us with a simplified, proportional formula:</p>
<div class="math notranslate nohighlight">
\[AIC \propto n \ln(RSS) + 2k\]</div>
<p>Here, <span class="math notranslate nohighlight">\(k = p + 1\)</span> is the total number of estimated parameters (including the variance).</p>
<p><strong>For BIC:</strong></p>
<p>We follow the exact same logic for BIC. Substituting the expression for <span class="math notranslate nohighlight">\(\ln(\hat{L})\)</span> into the BIC formula gives:</p>
<div class="math notranslate nohighlight">
\[BIC = -2 \left[ - \frac{n}{2} \left( \ln(2\pi) + \ln(RSS) - \ln(n) + 1 \right) \right] + k\ln(n)\]</div>
<div class="math notranslate nohighlight">
\[BIC = n \left( \ln(2\pi) + \ln(RSS) - \ln(n) + 1 \right) + k\ln(n)\]</div>
<p>Again, for model comparison, we can drop the same terms that are constant for a given dataset, resulting in the simplified, proportional formula for BIC:</p>
<div class="math notranslate nohighlight">
\[BIC \propto n \ln(RSS) + k\ln(n)\]</div>
<p>Here, <span class="math notranslate nohighlight">\(k = p + 1\)</span> is the total number of estimated parameters (including the variance).</p>
<p>It is important to remember that these simplified formulas produce values that are not the true AIC or BIC, but rather values that maintain the same differences between models as the full formulas would. Since model selection depends only on these differences (e.g., finding the model with the minimum AIC), these simplified versions are sufficient and computationally convenient for that purpose. However, if reporting the absolute AIC or BIC value is required, the full formula should be used.</p>
</section>
</section>
<section id="modification-for-small-sample-size-the-corrected-aic-aicc">
<h2>Modification for Small Sample Size: The Corrected AIC (AICc)<a class="headerlink" href="#modification-for-small-sample-size-the-corrected-aic-aicc" title="Link to this heading">#</a></h2>
<p>The Akaike Information Criterion (AIC) is derived in the limit of a large sample size <span class="math notranslate nohighlight">\(n\)</span>. When the sample size is not large relative to the number of estimated parameters <span class="math notranslate nohighlight">\(k\)</span>, AIC can perform poorly. Specifically, it has a tendency to favor models with too many parameters, a phenomenon known as overfitting.</p>
<p>To address this, the <strong>Corrected Akaike Information Criterion (AICc)</strong> was developed. AICc includes a second-order correction term that increases the penalty for extra parameters, with the size of this correction being larger for smaller sample sizes.</p>
<p>The AICc is defined as the AIC with an added penalty term:</p>
<div class="math notranslate nohighlight">
\[AICc = AIC + \frac{2k(k+1)}{n - k - 1}\]</div>
<p>Let’s derive the more common form of this equation. We start with the full definition of AIC:</p>
<div class="math notranslate nohighlight">
\[AIC = -2\ln(\hat{L}) + 2k\]</div>
<p>Now, we add the correction term:</p>
<div class="math notranslate nohighlight">
\[AICc = \left( -2\ln(\hat{L}) + 2k \right) + \frac{2k(k+1)}{n - k - 1}\]</div>
<p>We can combine the two terms that include <span class="math notranslate nohighlight">\(k\)</span> by factoring out <span class="math notranslate nohighlight">\(2k\)</span>:</p>
<div class="math notranslate nohighlight">
\[AICc = -2\ln(\hat{L}) + 2k \left( 1 + \frac{k+1}{n - k - 1} \right)\]</div>
<p>To simplify the expression in the parenthesis, we find a common denominator:</p>
<div class="math notranslate nohighlight">
\[1 + \frac{k+1}{n - k - 1} = \frac{(n - k - 1) + (k+1)}{n - k - 1} = \frac{n}{n - k - 1}\]</div>
<p>By substituting this simplified fraction back into the equation, we arrive at the most common and elegant formula for AICc:</p>
<div class="math notranslate nohighlight">
\[AICc = -2\ln(\hat{L}) + 2k \left( \frac{n}{n - k - 1} \right)\]</div>
<p>This form clearly shows that the standard AIC penalty <span class="math notranslate nohighlight">\(2k\)</span> is being multiplied by a correction factor <span class="math notranslate nohighlight">\(n / (n - k - 1)\)</span>.</p>
<p>Let’s examine the correction factor <span class="math notranslate nohighlight">\(n / (n - k - 1)\)</span>:</p>
<ul class="simple">
<li><p><strong>When <span class="math notranslate nohighlight">\(n\)</span> is large:</strong> As <span class="math notranslate nohighlight">\(n\)</span> becomes much larger than <span class="math notranslate nohighlight">\(k\)</span>, the fraction <span class="math notranslate nohighlight">\(n / (n - k - 1)\)</span> approaches <span class="math notranslate nohighlight">\(n/n\)</span>, which is 1. In this case, AICc converges to AIC. This is exactly what we want; for large samples, the correction is negligible.</p></li>
<li><p><strong>When <span class="math notranslate nohighlight">\(n\)</span> is small:</strong> When <span class="math notranslate nohighlight">\(n\)</span> is close to <span class="math notranslate nohighlight">\(k\)</span>, the denominator <span class="math notranslate nohighlight">\(n - k - 1\)</span> becomes very small, making the correction factor large. This imposes a much heavier penalty on more complex models (those with a larger <span class="math notranslate nohighlight">\(k\)</span>), helping to prevent overfitting.</p></li>
</ul>
<p>Given its properties, it is often recommended to <strong>use AICc by default</strong>, rather than AIC, especially if you are not in a “big data” context. A common rule of thumb is to use AICc when the ratio <span class="math notranslate nohighlight">\(n/k\)</span> is less than 40.</p>
<ul class="simple">
<li><p><strong>Number of Parameters <span class="math notranslate nohighlight">\(k\)</span>:</strong> As before, <span class="math notranslate nohighlight">\(k\)</span> must include all estimated parameters. In the OLS case with <span class="math notranslate nohighlight">\(p\)</span> model coefficients, <span class="math notranslate nohighlight">\(k = p + 1\)</span> to account for the estimated variance.</p></li>
</ul>
<p>We can write the proportional formula for AICc in the OLS case (unknown variance) by starting with the proportional formula for AIC and adding the correction term:</p>
<div class="math notranslate nohighlight">
\[AICc \propto n \ln(RSS) + 2k + \frac{2k(k+1)}{n - k - 1}\]</div>
<p>Or, using the multiplicative correction factor:</p>
<div class="math notranslate nohighlight">
\[AICc \propto n \ln(RSS) + 2k \left( \frac{n}{n - k - 1} \right)\]</div>
<p>NOTE: There is an obvious restriction: <span class="math notranslate nohighlight">\(n\)</span> should be greater than <span class="math notranslate nohighlight">\(k+1\)</span> (<span class="math notranslate nohighlight">\(n &gt; k + 1\)</span>) to avoid the situation when denominator is zero or negative and AICc cannot be calculated.</p>
<p>Is There a Small-Sample Correction for BIC? While AIC has a widely accepted and commonly used correction, <strong>there is no standard, universally adopted small-sample correction for BIC.</strong></p>
<p>The reasons for this are rooted in their different theoretical origins:</p>
<ol class="arabic simple">
<li><p><strong>Different Foundations:</strong> AIC is derived from principles of information theory and aims to find the model that best approximates the true data-generating process with minimal information loss. Its derivation allows for a more straightforward second-order correction.</p></li>
<li><p><strong>Asymptotic Nature of BIC:</strong> BIC is derived from a Bayesian probability framework. It is an asymptotic approximation to the full Bayesian model evidence, which is used to calculate the Bayes factor. The <span class="math notranslate nohighlight">\(k \ln(n)\)</span> penalty term arises naturally from this approximation.</p></li>
<li><p><strong>Stronger Inherent Penalty:</strong> The penalty term in BIC, <span class="math notranslate nohighlight">\(k \ln(n)\)</span>, is already much stronger than AIC’s <span class="math notranslate nohighlight">\(2k\)</span> penalty (for any <span class="math notranslate nohighlight">\(n &gt; 7\)</span>). This strong penalty, which scales with sample size, already makes BIC less prone to the kind of overfitting in small samples that necessitated the creation of AICc.</p></li>
</ol>
</section>
<section id="additional-materials">
<h2>Additional Materials<a class="headerlink" href="#additional-materials" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">https://en.wikipedia.org/wiki/Maximum_likelihood_estimation</a></p></li>
<li><p><a class="reference external" href="https://www.quantstart.com/articles/Maximum-Likelihood-Estimation-for-Linear-Regression/">https://www.quantstart.com/articles/Maximum-Likelihood-Estimation-for-Linear-Regression/</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Akaike_information_criterion">https://en.wikipedia.org/wiki/Akaike_information_criterion</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./math\01_basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="goodness-of-fit-and-chi-squared.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Goodness of Fit and Chi-Squared Statistic</p>
      </div>
    </a>
    <a class="right-next"
       href="weighted-least-squares-code-3.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">WLS - Code Examples Part 3</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#akaike-information-criterion-aic">Akaike Information Criterion (AIC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-information-criterion-bic">Bayesian Information Criterion (BIC)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-and-when-to-use-which">Key Differences and When to Use Which</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-the-log-likelihood-for-least-squares">Derivation of the Log-Likelihood for Least Squares</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-this-for-aic-and-bic">How to Use This for AIC and BIC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-case-of-unknown-variance-ordinary-least-squares-ols">The Case of Unknown Variance: Ordinary Least Squares (OLS)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-unknown-variance-sigma-2">Estimating the Unknown Variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-maximized-log-likelihood">The Maximized Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-aic-and-bic-with-unknown-variance">Calculating AIC and BIC with Unknown Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modification-for-small-sample-size-the-corrected-aic-aicc">Modification for Small Sample Size: The Corrected AIC (AICc)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional Materials</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Vladyslav Yakovliev
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>