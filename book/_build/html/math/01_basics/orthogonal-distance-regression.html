
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Orthogonal Distance Regression &#8212; Quantopia&#39;:&#39; Physics, Python and Pi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'math/01_basics/orthogonal-distance-regression';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="The Continuity Equation" href="../../physics/continuity-equation-01.html" />
    <link rel="prev" title="WLS - Code Examples Part 3" href="weighted-least-squares-code-3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Quantopia':' Physics, Python and Pi - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Quantopia':' Physics, Python and Pi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Quantopia: Physics, Python, and Pi (Alpha Version)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">MATH</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gradient-operator.html">Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradient-directional-derivative.html">Directional Derivative</a></li>
<li class="toctree-l1"><a class="reference internal" href="divergence.html">Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier-transform-01.html">Fourier Transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="least-squares-regression.html">Least Squares Regression, RSS, RMSE, R-squared</a></li>
<li class="toctree-l1"><a class="reference internal" href="least-squares-regression-code.html">Least Squares Regression - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares.html">Ordinary Least Squares (OLS) Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares-code.html">Ordinary Least Squares (OLS) Regression - Code Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="variance-covariance.html">Variance and Covariance</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares.html">Weighted Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-1.html">WLS - Code Examples Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-2.html">WLS - Code Examples Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="goodness-of-fit-and-chi-squared.html">Goodness of Fit and Chi-Squared Statistic</a></li>
<li class="toctree-l1"><a class="reference internal" href="aic-and-bic.html">Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-3.html">WLS - Code Examples Part 3</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Orthogonal Distance Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PHYSICS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../physics/continuity-equation-01.html">The Continuity Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/continuity-equation-02.html">The Continuity Equation: One-Dimensional Advection of a Density Profile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/ensemble.html">Statistical Ensembles and Liouville’s Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/microcanonical-ensemble.html">Microcanonical Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/fokker-planck-equation-example.html">Fokker-Planck Equation - Example Analysis (preview)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DATA SCIENCE AND MACHINE LEARNING</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../data-science/knn-algorithm.html">K-Nearest Neighbors (KNN) Algorithm (preview)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data-science/naive-bayes.html">Naive Bayes Method (preview)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data-science/logistic-regression.html">Logistic Regression (preview)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/Yakovliev/quantopia/blob/main/book/math/01_basics/orthogonal-distance-regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia/issues/new?title=Issue%20on%20page%20%2Fmath/01_basics/orthogonal-distance-regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/math/01_basics/orthogonal-distance-regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Orthogonal Distance Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-principles-of-odr">General Principles of ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-two-roles-of-error-in-odr">The Two Roles of Error in ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#odr-with-non-linear-functions">ODR with Non-linear Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-choose-odr-over-ols">When Should You Choose ODR over OLS?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-assess-the-goodness-of-fit-for-orthogonal-distance-regression-odr">How to Assess the Goodness of Fit for Orthogonal Distance Regression (ODR)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chi-squared-chi-2-and-reduced-chi-squared-chi-2-nu-statistics">The Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) and Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2_\nu\)</span>) Statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-r-2-and-adjusted-r-squared">R-Squared (<span class="math notranslate nohighlight">\(R^2\)</span>) and Adjusted R-Squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-criteria-aic-and-bic">Information Criteria (AIC and BIC)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-essential-evaluation-methods">Other Essential Evaluation Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduced-chi-squared-chi-red-2">Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi_{red}^2\)</span>)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-reduced-chi-squared">Interpretation of Reduced Chi-Squared</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-and-odr">R-squared and ODR</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression-and-other-parameters">Non-linear Regression and Other Parameters</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="orthogonal-distance-regression">
<h1>Orthogonal Distance Regression<a class="headerlink" href="#orthogonal-distance-regression" title="Link to this heading">#</a></h1>
<p>Orthogonal Distance Regression (ODR) is a regression method that finds the best-fit line or curve by minimizing the sum of the squared <strong>orthogonal distances</strong> from the data points to the model. Unlike ordinary least squares (OLS) regression, which minimizes the vertical distances (errors in the y-direction), ODR accounts for errors in <strong>both the x and y variables</strong>.</p>
<p>This makes ODR particularly useful when both independent and dependent variables have measurement errors. The “orthogonal” part refers to the fact that the distances are measured perpendicular to the fitted curve, not vertically or horizontally.</p>
<section id="general-principles-of-odr">
<h2>General Principles of ODR<a class="headerlink" href="#general-principles-of-odr" title="Link to this heading">#</a></h2>
<p>In a typical OLS regression, you assume that the independent variable <span class="math notranslate nohighlight">\(x\)</span> is known without error, and all the error is in the dependent variable <span class="math notranslate nohighlight">\(y\)</span>. The goal is to minimize the sum of squared vertical distances, which is the squared difference between the observed value <span class="math notranslate nohighlight">\(y_i\)</span> and the model’s prediction <span class="math notranslate nohighlight">\(f(x_i)\)</span>.</p>
<p>ODR, however, treats both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> as having errors. The method involves finding a point <span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i)\)</span> on the curve <span class="math notranslate nohighlight">\(y = f(x)\)</span> that is closest to the observed data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>. The “distance” that ODR minimizes is the perpendicular distance between the observed point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> and the point on the curve <span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i)\)</span>.</p>
<p>The objective function that ODR minimizes is:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \left( \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2} \right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((x_i, y_i)\)</span> are the observed data points.</p></li>
<li><p><span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i)\)</span> are the corresponding adjusted points that lie on the fitted curve.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_{x,i}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{y,i}\)</span> are the known <strong>standard deviations</strong> of the errors for the <span class="math notranslate nohighlight">\(i\)</span>-th data point in the x and y directions, respectively.</p></li>
<li><p>The terms <span class="math notranslate nohighlight">\(\frac{1}{\sigma_{x,i}^2}\)</span> and <span class="math notranslate nohighlight">\(\frac{1}{\sigma_{y,i}^2}\)</span> are the statistical weights. This is the standard practice of weighting by the inverse variance, which gives more influence to data.</p></li>
<li><p><span class="math notranslate nohighlight">\(S\)</span> is the final value to be minimized, representing the sum of the squared weighted distances between the observed points and the curve.</p></li>
</ul>
<p>The core idea is that the regression should be more influenced by data points with smaller uncertainties and less by those with larger uncertainties.</p>
</section>
<section id="the-two-roles-of-error-in-odr">
<h2>The Two Roles of Error in ODR<a class="headerlink" href="#the-two-roles-of-error-in-odr" title="Link to this heading">#</a></h2>
<p>A careful look at the ODR objective function reveals that the error terms play two distinct and crucial roles in determining the final fit. To see this, we can rewrite the objective function by defining the <strong>error variance ratio</strong>, <span class="math notranslate nohighlight">\(\lambda_i\)</span>, for each data point:</p>
<div class="math notranslate nohighlight">
\[\lambda_i = \frac{\sigma_{y,i}^2}{\sigma_{x,i}^2}\]</div>
<p>The objective function can then be expressed as:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{1}{\sigma_{x,i}^2} \left( (x_i - \hat{x}_i)^2 + \frac{\sigma_{x,i}^2}{\sigma_{y,i}^2}(y_i - \hat{y}_i)^2 \right) = \sum_{i=1}^{n} \frac{1}{\sigma_{x,i}^2} \left( (x_i - \hat{x}_i)^2 + \frac{1}{\lambda_i}(y_i - \hat{y}_i)^2 \right)\]</div>
<p>This form clearly separates the two roles:</p>
<p><strong>1. Intra-Point Geometry (The Error Ratio <span class="math notranslate nohighlight">\(\lambda_i\)</span>)</strong></p>
<p>The term inside the parenthesis, <span class="math notranslate nohighlight">\(\left( (x_i - \hat{x}_i)^2 + \frac{1}{\lambda_i}(y_i - \hat{y}_i)^2 \right)\)</span>, is controlled by the ratio <span class="math notranslate nohighlight">\(\lambda_i\)</span>. This ratio determines the <strong>geometry of the error</strong> for a single point <code class="docutils literal notranslate"><span class="pre">i</span></code>. It sets the relative “cost” of a deviation in the x-direction versus a deviation in the y-direction.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\lambda_i\)</span> is large (y-error &gt;&gt; x-error), the term <span class="math notranslate nohighlight">\(\frac{1}{\lambda_i}\)</span> is small, making it “cheaper” for the fit to accommodate deviations in y.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\lambda_i = 1\)</span> (x-error = y-error), the penalties are equal, and the error geometry is circular, resulting in a truly perpendicular distance.</p></li>
</ul>
<p>In essence, <span class="math notranslate nohighlight">\(\lambda_i\)</span> tells the algorithm the optimal <strong>direction</strong> from the data point to the curve.</p>
<p><strong>2. Inter-Point Weighting (The Absolute Error Variance)</strong></p>
<p>The term outside the parenthesis, <span class="math notranslate nohighlight">\(\frac{1}{\sigma_{x,i}^2}\)</span>, acts as the <strong>overall weight for point <code class="docutils literal notranslate"><span class="pre">i</span></code></strong> within the sum. This term dictates the <strong>influence of point <code class="docutils literal notranslate"><span class="pre">i</span></code> relative to all other points</strong>.</p>
<ul class="simple">
<li><p>A point with small absolute errors (e.g., a small <span class="math notranslate nohighlight">\(\sigma_{x,i}\)</span>) will have a large weight (<span class="math notranslate nohighlight">\(1/\sigma_{x,i}^2\)</span>), making it a “strong magnet.” The algorithm will work much harder to minimize the distance for this point, as it contributes significantly to the total sum <span class="math notranslate nohighlight">\(S\)</span>.</p></li>
<li><p>A point with large absolute errors will have a small weight, acting as a “weak magnet” with less influence on the final position of the curve.</p></li>
</ul>
<p>Therefore, the absolute magnitude of the errors is critical for determining which points the regression should prioritize.</p>
<p><strong>When Does Only the Ratio Matter?</strong></p>
<p>The powerful insight that you might only need the error ratio is <strong>only true under a specific, common assumption: that the error magnitudes are constant for all data points</strong> (homoscedastic errors).</p>
<p>Let’s assume that <span class="math notranslate nohighlight">\(\sigma_{x,i} = \sigma_x\)</span> and <span class="math notranslate nohighlight">\(\sigma_{y,i} = \sigma_y\)</span> for all points <code class="docutils literal notranslate"><span class="pre">i</span></code>. In this case:</p>
<ol class="arabic simple">
<li><p>The ratio <span class="math notranslate nohighlight">\(\lambda_i\)</span> becomes a single constant for the entire dataset: <span class="math notranslate nohighlight">\(\lambda = \sigma_y^2 / \sigma_x^2\)</span>.</p></li>
<li><p>The weighting term <span class="math notranslate nohighlight">\(\frac{1}{\sigma_{x,i}^2}\)</span> also becomes a single constant: <span class="math notranslate nohighlight">\(\frac{1}{\sigma_x^2}\)</span>.</p></li>
</ol>
<p>Now, the objective function simplifies:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{1}{\sigma_x^2} \left( (x_i - \hat{x}_i)^2 + \frac{1}{\lambda}(y_i - \hat{y}_i)^2 \right)\]</div>
<p>Since <span class="math notranslate nohighlight">\(\frac{1}{\sigma_x^2}\)</span> is now a constant multiplier for the <em>entire sum</em>, we can factor it out:</p>
<div class="math notranslate nohighlight">
\[S = \frac{1}{\sigma_x^2} \sum_{i=1}^{n} \left( (x_i - \hat{x}_i)^2 + \frac{1}{\lambda}(y_i - \hat{y}_i)^2 \right)\]</div>
<p>Multiplying the entire function by a constant does not change the location of its minimum. Therefore, if you can assume that your measurement errors are consistent across all your data points, then you <strong>do not need to know their absolute values to find the best-fit curve; you only need to know their ratio, <span class="math notranslate nohighlight">\(\lambda\)</span>.</strong> This is a very common scenario in practice, for instance, when all measurements are made using the same instrument under the same conditions.</p>
<p>Now, let’s consider the implications of the error ratio <span class="math notranslate nohighlight">\(\lambda\)</span>. Let’s look at a single term in the sum for one data point:</p>
<div class="math notranslate nohighlight">
\[\text{Term}_i = \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2}\]</div>
<p>The algorithm wants to make this term small.</p>
<ul>
<li><p><strong>Case 1: <span class="math notranslate nohighlight">\(\lambda_i \to \infty\)</span> (Error in x is zero)</strong></p>
<p>This happens when <span class="math notranslate nohighlight">\(\sigma_{x,i}^2 \to 0\)</span>. Let’s look at the first part of the term: <span class="math notranslate nohighlight">\(\frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2}\)</span>.
As the denominator <span class="math notranslate nohighlight">\(\sigma_{x,i}^2\)</span> gets closer and closer to zero, this fraction will <strong>explode to infinity</strong> <em>unless</em> the numerator is also exactly zero. To prevent the total sum <span class="math notranslate nohighlight">\(S\)</span> from becoming infinite, the algorithm has no choice but to force the numerator to be zero.</p>
<p>It <strong>must</strong> set <span class="math notranslate nohighlight">\(\hat{x}_i = x_i\)</span>.</p>
<p>With this constraint, the first part of the term becomes zero. What’s left to minimize?</p>
<div class="math notranslate nohighlight">
\[\text{Term}_i = 0 + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2}\]</div>
<p>Since the fitted point must lie on the curve, we know <span class="math notranslate nohighlight">\(\hat{y}_i = f(\hat{x}_i)\)</span>. And because we were forced to set <span class="math notranslate nohighlight">\(\hat{x}_i = x_i\)</span>, this means <span class="math notranslate nohighlight">\(\hat{y}_i = f(x_i)\)</span>. Substituting this in, the total objective function becomes:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{(y_i - f(x_i))^2}{\sigma_{y,i}^2}\]</div>
<p>This is the <strong>exact formula for Weighted Least Squares (WLS)</strong>. ODR has transformed into WLS because the infinite penalty on any horizontal deviation left it with no freedom to adjust the x-positions.</p>
</li>
<li><p><strong>Case 2: <span class="math notranslate nohighlight">\(\lambda_i \to 0\)</span> (Error in y is zero)</strong></p>
<p>This is the symmetrical opposite. This happens when <span class="math notranslate nohighlight">\(\sigma_{y,i}^2 \to 0\)</span>. The second part of the term, <span class="math notranslate nohighlight">\(\frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2}\)</span>, will explode unless the algorithm forces <span class="math notranslate nohighlight">\(\hat{y}_i = y_i\)</span>. The objective function then reduces to minimizing only the horizontal weighted distances:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2}\]</div>
<p>This is equivalent to performing WLS, but with the roles of x and y swapped.</p>
</li>
<li><p><strong>Case 3: <span class="math notranslate nohighlight">\(\lambda_i = 1\)</span> (Errors are equal)</strong></p>
<p>This means <span class="math notranslate nohighlight">\(\sigma_{x,i}^2 = \sigma_{y,i}^2\)</span>. Let’s call this common variance <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>. The objective function becomes:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \left( \frac{(x_i - \hat{x}_i)^2}{\sigma_i^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_i^2} \right)\]</div>
<p>We can factor out the denominator:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{1}{\sigma_i^2} \left( (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right)\]</div>
<p>By the Pythagorean theorem, the term <span class="math notranslate nohighlight">\((x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2\)</span> is simply the <strong>squared geometric (Euclidean) distance</strong> between the observed point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> and the fitted point <span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i)\)</span>. The overall objective function is therefore minimizing the weighted sum of these squared perpendicular distances. The weight for each point is given by its inverse variance <span class="math notranslate nohighlight">\(1/\sigma_i^2\)</span> giving more influence to the more certain data points.</p>
<p>This becomes a simple (unweighted) sum of squared perpendicular distances only in the even more specific case where all errors are identical (<span class="math notranslate nohighlight">\(\sigma_i = \sigma\)</span> for all points).</p>
</li>
</ul>
</section>
<section id="odr-with-non-linear-functions">
<h2>ODR with Non-linear Functions<a class="headerlink" href="#odr-with-non-linear-functions" title="Link to this heading">#</a></h2>
<p>ODR is especially powerful for non-linear regression because it can handle cases where both variables have errors and the relationship isn’t a straight line. The process involves an iterative numerical optimization algorithm to find the parameters of the non-linear function that minimize the objective function described above.</p>
<p>For your specific case with a non-linear function and errors for both the x-axis and y-axis, you will need to:</p>
<ol class="arabic simple">
<li><p><strong>Define your non-linear model function</strong>, e.g., <span class="math notranslate nohighlight">\(y = f(x; \beta_1, \beta_2, ...)\)</span> where <span class="math notranslate nohighlight">\(\beta_i\)</span> are the parameters you want to estimate.</p></li>
<li><p><strong>Provide your data</strong> (<span class="math notranslate nohighlight">\(x_i, y_i\)</span>).</p></li>
<li><p><strong>Specify the errors</strong> or weights for both variables.</p></li>
<li><p><strong>Use an ODR-specific software library or package</strong> (e.g., <code class="docutils literal notranslate"><span class="pre">scipy.odr</span></code> in Python) to perform the regression. The solver will then iteratively adjust the parameters of your function until the sum of the squared orthogonal distances is minimized. ODR typically uses trust-region Levenberg-Marquardt algorithms.</p></li>
</ol>
<p>The output will be the best-fit parameters for your non-linear function, along with their standard errors, which can be used to assess the uncertainty of the estimates.</p>
</section>
<section id="when-should-you-choose-odr-over-ols">
<h2>When Should You Choose ODR over OLS?<a class="headerlink" href="#when-should-you-choose-odr-over-ols" title="Link to this heading">#</a></h2>
<p>The choice between Ordinary Least Squares (OLS) and Orthogonal Distance Regression (ODR) depends entirely on the nature of the errors in your data. Since ODR is more complex and computationally intensive, it’s important to know when its use is truly justified.</p>
<p><strong>Use Ordinary Least Squares (OLS) when:</strong></p>
<ul class="simple">
<li><p><strong>Error in the independent variable (x) is negligible or zero.</strong> This is the classic assumption of OLS. If you are regressing against a variable like time, or a concentration that you set with high precision, the error in x is likely insignificant compared to the measurement error in y.</p></li>
<li><p><strong>The error in x is significantly smaller than the error in y.</strong> As a rule of thumb, if the standard deviation of the y-error is more than 5 to 10 times larger than the standard deviation of the x-error, the results from OLS will be very close to those from ODR. In this case, the simplicity and speed of OLS make it the preferred choice.</p></li>
<li><p><strong>The primary goal is prediction.</strong> If your main goal is to predict new y-values from new x-values (and you expect new data to have similar error properties), OLS provides an unbiased predictor for y given x, even if x has some error.</p></li>
</ul>
<p><strong>Use Orthogonal Distance Regression (ODR) when:</strong></p>
<ul class="simple">
<li><p><strong>Errors in both x and y are significant and of a similar magnitude.</strong> This is the primary use case for ODR. Ignoring significant errors in the independent variable leads to a bias known as regression dilution, where the estimated slope of the relationship is systematically underestimated (biased towards zero).</p></li>
<li><p><strong>The choice of independent vs. dependent variable is arbitrary.</strong> For example, if you are comparing two different instruments by measuring the same quantity with both, there is no physical reason to call one instrument’s measurement “x” and the other “y”. ODR treats both variables symmetrically, giving a result that is independent of this arbitrary choice. OLS would give a different best-fit line if you swapped the x and y axes.</p></li>
<li><p><strong>The model is highly non-linear.</strong> On very steep or very flat sections of a curve, the vertical distance minimized by OLS can be a poor representation of the “true” distance from a data point to the model. The perpendicular distance minimized by ODR is often more geometrically stable and robust in these cases.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Scenario</p></th>
<th class="head text-left"><p>Recommended Method</p></th>
<th class="head text-left"><p>Rationale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Error in x is negligible</p></td>
<td class="text-left"><p><strong>OLS</strong></p></td>
<td class="text-left"><p>The core assumption of OLS is met.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Error in x is much smaller than error in y</p></td>
<td class="text-left"><p><strong>OLS</strong></p></td>
<td class="text-left"><p>Results will be nearly identical to ODR, but OLS is simpler.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Errors in x and y are comparable</p></td>
<td class="text-left"><p><strong>ODR</strong></p></td>
<td class="text-left"><p>OLS will produce biased parameter estimates.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Variables are symmetric (e.g., comparing two methods)</p></td>
<td class="text-left"><p><strong>ODR</strong></p></td>
<td class="text-left"><p>The result should not depend on which variable is on which axis.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Highly non-linear model with errors in x</p></td>
<td class="text-left"><p><strong>ODR</strong></p></td>
<td class="text-left"><p>Orthogonal distance can be a more robust error metric.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="how-to-assess-the-goodness-of-fit-for-orthogonal-distance-regression-odr">
<h2>How to Assess the Goodness of Fit for Orthogonal Distance Regression (ODR)<a class="headerlink" href="#how-to-assess-the-goodness-of-fit-for-orthogonal-distance-regression-odr" title="Link to this heading">#</a></h2>
<p>Once you have performed an ODR fit, it is crucial to evaluate how well the resulting model actually represents your data. Because ODR does not seek to minimize the simple vertical distances like OLS, some of the most common goodness-of-fit metrics, like R-squared, are not appropriate. Instead, we must turn to metrics that are consistent with the ODR objective function.</p>
<p>The primary methods for assessing an ODR fit rely on the final, minimized value of the objective function itself, residual analysis, and information criteria for model comparison.</p>
<section id="the-chi-squared-chi-2-and-reduced-chi-squared-chi-2-nu-statistics">
<h3>The Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) and Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2_\nu\)</span>) Statistics<a class="headerlink" href="#the-chi-squared-chi-2-and-reduced-chi-squared-chi-2-nu-statistics" title="Link to this heading">#</a></h3>
<p>The most natural and powerful metrics for ODR goodness of fit are derived directly from the value that the algorithm worked to minimize.</p>
<ul>
<li><p><strong>The Chi-Squared Statistic (<span class="math notranslate nohighlight">\(\chi^2\)</span>)</strong></p>
<p>The final, minimized value of the ODR objective function, <span class="math notranslate nohighlight">\(S\)</span>, is itself a chi-squared statistic.</p>
<div class="math notranslate nohighlight">
\[ \chi^2 = S_{min} = \sum_{i=1}^{n} \left( \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2} \right) \]</div>
<p>This value represents the total weighted squared error of the fit. However, its absolute magnitude is difficult to interpret on its own, as it scales with the number of data points.</p>
</li>
<li><p><strong>The Reduced Chi-Squared Statistic (<span class="math notranslate nohighlight">\(\chi^2_\nu\)</span>) - The Best Indicator</strong></p>
<p>To create a standardized and highly interpretable metric, we calculate the <strong>reduced chi-squared statistic</strong>, which is the final <span class="math notranslate nohighlight">\(\chi^2\)</span> value divided by the <strong>degrees of freedom</strong> (<span class="math notranslate nohighlight">\(\nu\)</span>).</p>
<p>The degrees of freedom are the number of data points (<span class="math notranslate nohighlight">\(n\)</span>) minus the number of parameters (<span class="math notranslate nohighlight">\(p\)</span>) estimated by the model: <span class="math notranslate nohighlight">\(\nu = n - p\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \chi^2_\nu = \frac{\chi^2}{\nu} = \frac{S_{min}}{n - p} \]</div>
<p>The reduced chi-squared value provides a powerful assessment of the fit quality under the assumption that your error estimates (<span class="math notranslate nohighlight">\(\sigma_{x,i}, \sigma_{y,i}\)</span>) are accurate:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\chi^2_\nu \approx 1.0\)</span></strong>: This is the ideal outcome. It indicates that the model is a good fit to the data and that the measurement errors were likely estimated correctly. The observed scatter of the data points around the fitted curve is consistent with their reported error bars.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(\chi^2_\nu &gt; 1.0\)</span></strong>: This suggests a poor fit. There are two possible reasons:</p>
<ol class="arabic simple">
<li><p><strong>The model is wrong:</strong> The chosen function does not adequately describe the true physical relationship.</p></li>
<li><p><strong>The errors were underestimated:</strong> Your reported error values (<span class="math notranslate nohighlight">\(\sigma_{x,i}, \sigma_{y,i}\)</span>) are too small, and the data is actually “noisier” than you thought.</p></li>
</ol>
</li>
<li><p><strong><span class="math notranslate nohighlight">\(\chi^2_\nu &lt; 1.0\)</span></strong>: This indicates that the fit is “too good.” The data points are, on average, closer to the line than their error estimates would predict. This can mean:</p>
<ol class="arabic simple">
<li><p><strong>The errors were overestimated:</strong> Your reported error values are too large.</p></li>
<li><p><strong>The model is overfitting the data:</strong> This is a risk if the model is too complex relative to the number of data points.</p></li>
</ol>
</li>
</ul>
<p>Many regression packages, including <code class="docutils literal notranslate"><span class="pre">scipy.odr</span></code>, report this value as <code class="docutils literal notranslate"><span class="pre">res_var</span></code> (residual variance), which is equivalent to the reduced chi-squared.</p>
</li>
</ul>
<p>See the notes about non-linear case for the Reduced Chi-Squared <a class="reference internal" href="goodness-of-fit-and-chi-squared.html"><span class="std std-doc">here</span></a></p>
</section>
<section id="r-squared-r-2-and-adjusted-r-squared">
<h3>R-Squared (<span class="math notranslate nohighlight">\(R^2\)</span>) and Adjusted R-Squared<a class="headerlink" href="#r-squared-r-2-and-adjusted-r-squared" title="Link to this heading">#</a></h3>
<p><strong>Generally Not Recommended for ODR.</strong></p>
<p>R-squared is defined as <span class="math notranslate nohighlight">\(R^2 = 1 - \frac{SS_{res}}{SS_{tot}}\)</span>, where <span class="math notranslate nohighlight">\(SS_{res}\)</span> is the sum of squared <em>vertical</em> residuals and <span class="math notranslate nohighlight">\(SS_{tot}\)</span> is the total sum of squares related to the <em>vertical</em> variance of the data.</p>
<p>This metric is fundamentally incompatible with the principle of ODR for two reasons:</p>
<ol class="arabic simple">
<li><p>ODR does not minimize the vertical residuals, so <span class="math notranslate nohighlight">\(SS_{res}\)</span> does not represent what the algorithm was trying to do.</p></li>
<li><p><span class="math notranslate nohighlight">\(SS_{tot}\)</span> only accounts for the variance in the y-variable, completely ignoring the error structure of the x-variable, which is central to ODR.</p></li>
</ol>
<p>Using a standard R-squared calculation on an ODR fit gives a misleading and often uninterpretable number. While some “pseudo R-squared” metrics exist, they lack the intuitive “percentage of variance explained” meaning and are best avoided in favor of the chi-squared statistics.</p>
</section>
<section id="information-criteria-aic-and-bic">
<h3>Information Criteria (AIC and BIC)<a class="headerlink" href="#information-criteria-aic-and-bic" title="Link to this heading">#</a></h3>
<p><strong>Recommended for Model Comparison.</strong></p>
<p>The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are excellent tools for <strong>comparing different models</strong> fit to the same dataset. They do not tell you if a single model is a “good fit” in an absolute sense, but they can tell you which model is better relative to others.</p>
<p>Both criteria work by balancing model fit with model complexity (the number of parameters, <span class="math notranslate nohighlight">\(p\)</span>). For models based on a chi-squared statistic (like ODR), they can be calculated as:</p>
<ul class="simple">
<li><p><strong>AIC</strong> = <span class="math notranslate nohighlight">\(\chi^2 + 2p\)</span></p></li>
<li><p><strong>BIC</strong> = <span class="math notranslate nohighlight">\(\chi^2 + p \ln(n)\)</span></p></li>
</ul>
<p>When comparing two or more models, the one with the <strong>lower AIC or BIC value is considered better</strong>. It provides the most explanatory power for the least complexity.</p>
</section>
<section id="other-essential-evaluation-methods">
<h3>Other Essential Evaluation Methods<a class="headerlink" href="#other-essential-evaluation-methods" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Graphical Residual Analysis:</strong> This is one of the most important steps. Instead of plotting vertical residuals, you should analyze the ODR residuals. You can plot a histogram of the standardized residuals or, more effectively, plot the magnitude of the orthogonal distance from each point to the curve against the fitted values. There should be no discernible pattern or trend in this plot; the residuals should appear randomly scattered.</p></li>
<li><p><strong>Parameter Standard Errors:</strong> The ODR output will provide the best-fit parameters along with their standard errors (e.g., <code class="docutils literal notranslate"><span class="pre">beta_std</span></code> in <code class="docutils literal notranslate"><span class="pre">scipy.odr</span></code>). A large standard error relative to the parameter value indicates that the parameter is not well-constrained by the data, which can be a sign of a poor model or insufficient data.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Metric</p></th>
<th class="head text-left"><p>Applicability to ODR</p></th>
<th class="head text-left"><p>Purpose &amp; Interpretation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2_\nu\)</span>)</strong></p></td>
<td class="text-left"><p><strong>Highly Recommended</strong></p></td>
<td class="text-left"><p><strong>Primary indicator of fit quality.</strong> A value near 1.0 indicates a good fit where the model and error estimates are consistent.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>)</strong></p></td>
<td class="text-left"><p><strong>Recommended</strong></p></td>
<td class="text-left"><p>The raw, minimized sum of weighted squared errors. Used to calculate <span class="math notranslate nohighlight">\(\chi^2_\nu\)</span>, AIC, and BIC.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>AIC / BIC</strong></p></td>
<td class="text-left"><p><strong>Recommended</strong></p></td>
<td class="text-left"><p><strong>Model comparison.</strong> Helps select the best model from a set of candidates. Lower values are better.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Residual Plots</strong></p></td>
<td class="text-left"><p><strong>Essential</strong></p></td>
<td class="text-left"><p><strong>Visual check for model validity.</strong> Residuals should be random, without patterns.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Parameter Standard Errors</strong></p></td>
<td class="text-left"><p><strong>Essential</strong></p></td>
<td class="text-left"><p><strong>Assesses the certainty of the fit parameters.</strong> Large errors indicate poorly determined parameters.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>R-Squared (<span class="math notranslate nohighlight">\(R^2\)</span>)</strong></p></td>
<td class="text-left"><p><strong>Not Recommended</strong></p></td>
<td class="text-left"><p>Fundamentally inconsistent with the ODR objective function. Provides a misleading measure of fit.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Assessing the goodness of fit for Orthogonal Distance Regression (ODR), especially with non-linear models, is more complex than with Ordinary Least Squares (OLS). Since ODR accounts for errors in both x and y variables, traditional metrics like R-squared can be misleading or inappropriate.</p>
<p>The most suitable and standard goodness-of-fit parameter for ODR is the <strong>reduced chi-squared (<span class="math notranslate nohighlight">\(\chi_{red}^2\)</span>)</strong> statistic.</p>
</section>
<section id="reduced-chi-squared-chi-red-2">
<h3>Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi_{red}^2\)</span>)<a class="headerlink" href="#reduced-chi-squared-chi-red-2" title="Link to this heading">#</a></h3>
<p>The reduced chi-squared is a powerful and statistically rigorous measure for ODR because it directly incorporates the uncertainties (weights) of both the x and y data points. It is defined as the sum of squared weighted residuals divided by the number of degrees of freedom.</p>
<div class="math notranslate nohighlight">
\[\chi_{red}^2 = \frac{\sum_{i=1}^{n} \left( \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2} \right)}{n - p}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> is the number of parameters in the model.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_{x,i}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{y,i}\)</span> are the known standard deviations of the errors for each data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>.</p></li>
</ul>
<section id="interpretation-of-reduced-chi-squared">
<h4>Interpretation of Reduced Chi-Squared<a class="headerlink" href="#interpretation-of-reduced-chi-squared" title="Link to this heading">#</a></h4>
<p>The value of <span class="math notranslate nohighlight">\(\chi_{red}^2\)</span> tells you how well the model fits the data relative to the expected measurement errors.</p>
<ul class="simple">
<li><p>A value of <strong><span class="math notranslate nohighlight">\(\chi_{red}^2 \approx 1\)</span></strong> indicates an excellent fit. The model’s residuals are consistent with the measurement uncertainties you provided. The observed scatter is what you’d expect.</p></li>
<li><p>A value of <strong><span class="math notranslate nohighlight">\(\chi_{red}^2 \gg 1\)</span></strong> (significantly greater than 1) suggests a poor fit. This means the model does not adequately describe the data, or you have underestimated your measurement uncertainties. The observed scatter is much larger than expected.</p></li>
<li><p>A value of <strong><span class="math notranslate nohighlight">\(\chi_{red}^2 &lt; 1\)</span></strong> may indicate that the model is “overfitting” the data, or that you have overestimated your measurement uncertainties.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="r-squared-and-odr">
<h3>R-squared and ODR<a class="headerlink" href="#r-squared-and-odr" title="Link to this heading">#</a></h3>
<p>While R-squared is a common metric for OLS, its application to ODR is problematic. R-squared is based on the idea of explaining the variance in the y-variable, assuming the x-variable is known without error. This assumption is explicitly violated in ODR. Calculating a pseudo R-squared value for ODR can be done, but it often lacks a clear statistical interpretation and can be insensitive to the order of the variables (swapping x and y). For these reasons, <strong>R-squared and adjusted R-squared are generally not recommended</strong> for evaluating ODR results.</p>
</section>
<section id="non-linear-regression-and-other-parameters">
<h3>Non-linear Regression and Other Parameters<a class="headerlink" href="#non-linear-regression-and-other-parameters" title="Link to this heading">#</a></h3>
<p>For both linear and non-linear regression, other metrics and methods are valuable for assessing the fit:</p>
<ul class="simple">
<li><p><strong>Residual Plots:</strong> These are crucial. Plotting the residuals (the distances from the points to the fitted curve) against the fitted values can reveal systematic patterns (e.g., a “U” shape) that indicate a poor model choice, even if other statistical parameters look good.</p></li>
<li><p><strong>Standard Errors of Parameters:</strong> The standard errors of the fitted parameters provide a measure of their uncertainty. If these are very large, it suggests that the data are not sufficient to accurately determine the model’s parameters.</p></li>
<li><p><strong>Confidence and Prediction Bands:</strong> Visualizing the confidence and prediction bands around the fitted curve is an excellent way to see the range of uncertainty in your model and its predictions.</p></li>
</ul>
<p>The res_var attribute of the Output is the so-called reduced Chi-square value for the fit, a popular choice of goodness-of-fit statistic. It is somewhat problematic for non-linear fitting, though. You can look at the residuals directly (out.delta for the X residuals and out.eps for the Y residuals). Implementing a cross-validation or bootstrap method for determining goodness-of-fit, as suggested in the linked paper, is left as an exercise for the reader.</p>
<p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.odr.Output.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.odr.Output.html</a></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./math\01_basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="weighted-least-squares-code-3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">WLS - Code Examples Part 3</p>
      </div>
    </a>
    <a class="right-next"
       href="../../physics/continuity-equation-01.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Continuity Equation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-principles-of-odr">General Principles of ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-two-roles-of-error-in-odr">The Two Roles of Error in ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#odr-with-non-linear-functions">ODR with Non-linear Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-choose-odr-over-ols">When Should You Choose ODR over OLS?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-assess-the-goodness-of-fit-for-orthogonal-distance-regression-odr">How to Assess the Goodness of Fit for Orthogonal Distance Regression (ODR)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chi-squared-chi-2-and-reduced-chi-squared-chi-2-nu-statistics">The Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) and Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2_\nu\)</span>) Statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-r-2-and-adjusted-r-squared">R-Squared (<span class="math notranslate nohighlight">\(R^2\)</span>) and Adjusted R-Squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-criteria-aic-and-bic">Information Criteria (AIC and BIC)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-essential-evaluation-methods">Other Essential Evaluation Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduced-chi-squared-chi-red-2">Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi_{red}^2\)</span>)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-reduced-chi-squared">Interpretation of Reduced Chi-Squared</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-and-odr">R-squared and ODR</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression-and-other-parameters">Non-linear Regression and Other Parameters</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Vladyslav Yakovliev
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>