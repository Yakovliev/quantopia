
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Orthogonal Distance Regression &#8212; Quantopia&#39;:&#39; Physics, Python and Pi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'math/01_basics/orthogonal-distance-regression';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="ODR - Code Examples" href="odr-code.html" />
    <link rel="prev" title="WLS - Code Examples Part 3" href="weighted-least-squares-code-3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Quantopia':' Physics, Python and Pi - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Quantopia':' Physics, Python and Pi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Quantopia: Physics, Python, and Pi (Alpha Version)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">MATH</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gradient-operator.html">Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradient-directional-derivative.html">Directional Derivative</a></li>
<li class="toctree-l1"><a class="reference internal" href="divergence.html">Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier-transform-01.html">Fourier Transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="least-squares-regression.html">Least Squares Regression, RSS, RMSE, R-squared</a></li>
<li class="toctree-l1"><a class="reference internal" href="least-squares-regression-code.html">Least Squares Regression - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares.html">Ordinary Least Squares (OLS) Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares-code.html">Ordinary Least Squares (OLS) Regression - Code Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="variance-covariance.html">Variance and Covariance</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares.html">Weighted Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-1.html">WLS - Code Examples Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-2.html">WLS - Code Examples Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="goodness-of-fit-and-chi-squared.html">Goodness of Fit and Chi-Squared Statistic</a></li>
<li class="toctree-l1"><a class="reference internal" href="aic-and-bic.html">Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-3.html">WLS - Code Examples Part 3</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Orthogonal Distance Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="odr-code.html">ODR - Code Examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PHYSICS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../physics/continuity-equation-01.html">The Continuity Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/continuity-equation-02.html">The Continuity Equation: One-Dimensional Advection of a Density Profile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/ensemble.html">Statistical Ensembles and Liouville’s Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/microcanonical-ensemble.html">Microcanonical Ensemble</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/Yakovliev/quantopia/blob/main/book/math/01_basics/orthogonal-distance-regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia/issues/new?title=Issue%20on%20page%20%2Fmath/01_basics/orthogonal-distance-regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/math/01_basics/orthogonal-distance-regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Orthogonal Distance Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-principles-of-odr">General Principles of ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-two-roles-of-error-in-odr">The Two Roles of Error in ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#odr-with-non-linear-functions">ODR with Non-linear Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-choose-odr-over-ols">When Should You Choose ODR over OLS?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-assess-the-goodness-of-fit-for-orthogonal-distance-regression-odr">How to Assess the Goodness of Fit for Orthogonal Distance Regression (ODR)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chi-squared-chi-2-and-reduced-chi-squared-chi-2-red-statistics">The Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) and Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2_{red}\)</span>) Statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-r-2-and-adjusted-r-squared">R-Squared (<span class="math notranslate nohighlight">\(R^2\)</span>) and Adjusted R-Squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-criteria-aic-and-bic">Information Criteria (AIC and BIC)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-standard-errors">Parameter Standard Errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood-for-odr">Log-Likelihood for ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional Materials</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="orthogonal-distance-regression">
<h1>Orthogonal Distance Regression<a class="headerlink" href="#orthogonal-distance-regression" title="Link to this heading">#</a></h1>
<p>Orthogonal Distance Regression (ODR) is a regression method that finds the best-fit line or curve by minimizing the sum of the squared <strong>orthogonal distances</strong> from the data points to the model. Unlike ordinary least squares (OLS) regression, which minimizes the vertical distances (errors in the y-direction), ODR accounts for errors in <strong>both the x and y variables</strong>.</p>
<p>This makes ODR particularly useful when both independent and dependent variables have measurement errors. The “orthogonal” part refers to the fact that the distances are measured perpendicular to the fitted curve, not vertically or horizontally.</p>
<section id="general-principles-of-odr">
<h2>General Principles of ODR<a class="headerlink" href="#general-principles-of-odr" title="Link to this heading">#</a></h2>
<p>In a typical OLS regression, you assume that the independent variable <span class="math notranslate nohighlight">\(x\)</span> is known without error, and all the error is in the dependent variable <span class="math notranslate nohighlight">\(y\)</span>. The goal is to minimize the sum of squared vertical distances, which is the squared difference between the observed value <span class="math notranslate nohighlight">\(y_i\)</span> and the model’s prediction <span class="math notranslate nohighlight">\(f(x_i)\)</span>.</p>
<p>ODR, however, treats both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> as having errors. The method involves finding a point <span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i)\)</span> on the curve <span class="math notranslate nohighlight">\(y = f(x)\)</span> that is closest to the observed data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>. The “distance” that ODR minimizes is the perpendicular distance between the observed point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> and the point on the curve <span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i)\)</span>.</p>
<p>The objective function that ODR minimizes is:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \left( \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2} \right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((x_i, y_i)\)</span> are the observed data points.</p></li>
<li><p><span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i)\)</span> are the corresponding adjusted points that lie on the fitted curve.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_{x,i}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{y,i}\)</span> are the known <strong>standard deviations</strong> of the errors for the <span class="math notranslate nohighlight">\(i\)</span>-th data point in the x and y directions, respectively.</p></li>
<li><p>The terms <span class="math notranslate nohighlight">\(\frac{1}{\sigma_{x,i}^2}\)</span> and <span class="math notranslate nohighlight">\(\frac{1}{\sigma_{y,i}^2}\)</span> are the statistical weights. This is the standard practice of weighting by the inverse variance, which gives more influence to data.</p></li>
<li><p><span class="math notranslate nohighlight">\(S\)</span> is the final value to be minimized, representing the sum of the squared weighted distances between the observed points and the curve.</p></li>
</ul>
<p>The core idea is that the regression should be more influenced by data points with smaller uncertainties and less by those with larger uncertainties.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is important to note that this form of the objective function assumes that the errors in the x and y variables are <strong>uncorrelated</strong>. If the errors were correlated, the objective function would require a more complex formulation involving a covariance matrix for each data point.</p>
</div>
</section>
<section id="the-two-roles-of-error-in-odr">
<h2>The Two Roles of Error in ODR<a class="headerlink" href="#the-two-roles-of-error-in-odr" title="Link to this heading">#</a></h2>
<p>A careful look at the ODR objective function reveals that the error terms play two distinct and crucial roles in determining the final fit. To see this, we can rewrite the objective function by defining the <strong>error variance ratio</strong>, <span class="math notranslate nohighlight">\(\lambda_i\)</span>, for each data point:</p>
<div class="math notranslate nohighlight">
\[\lambda_i = \frac{\sigma_{y,i}^2}{\sigma_{x,i}^2}\]</div>
<p>The objective function can then be expressed as:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{1}{\sigma_{x,i}^2} \left( (x_i - \hat{x}_i)^2 + \frac{\sigma_{x,i}^2}{\sigma_{y,i}^2}(y_i - \hat{y}_i)^2 \right) = \sum_{i=1}^{n} \frac{1}{\sigma_{x,i}^2} \left( (x_i - \hat{x}_i)^2 + \frac{1}{\lambda_i}(y_i - \hat{y}_i)^2 \right)\]</div>
<p>This form clearly separates the two roles:</p>
<p><strong>1. Intra-Point Geometry (The Error Ratio <span class="math notranslate nohighlight">\(\lambda_i\)</span>)</strong></p>
<p>The term inside the parenthesis, <span class="math notranslate nohighlight">\(\left( (x_i - \hat{x}_i)^2 + \frac{1}{\lambda_i}(y_i - \hat{y}_i)^2 \right)\)</span>, is controlled by the ratio <span class="math notranslate nohighlight">\(\lambda_i\)</span>. This ratio determines the <strong>geometry of the error</strong> for a single point <code class="docutils literal notranslate"><span class="pre">i</span></code>. It sets the relative “cost” of a deviation in the x-direction versus a deviation in the y-direction.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\lambda_i\)</span> is large (y-error &gt;&gt; x-error), the term <span class="math notranslate nohighlight">\(\frac{1}{\lambda_i}\)</span> is small, making it “cheaper” for the fit to accommodate deviations in y.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\lambda_i = 1\)</span> (x-error = y-error), the penalties are equal, and the error geometry is circular, resulting in a truly perpendicular distance.</p></li>
</ul>
<p>In essence, <span class="math notranslate nohighlight">\(\lambda_i\)</span> tells the algorithm the optimal <strong>direction</strong> from the data point to the curve.</p>
<p><strong>2. Inter-Point Weighting (The Absolute Error Variance)</strong></p>
<p>The term outside the parenthesis, <span class="math notranslate nohighlight">\(\frac{1}{\sigma_{x,i}^2}\)</span>, acts as the <strong>overall weight for point <code class="docutils literal notranslate"><span class="pre">i</span></code></strong> within the sum. This term dictates the <strong>influence of point <code class="docutils literal notranslate"><span class="pre">i</span></code> relative to all other points</strong>.</p>
<ul class="simple">
<li><p>A point with small absolute errors (e.g., a small <span class="math notranslate nohighlight">\(\sigma_{x,i}\)</span>) will have a large weight (<span class="math notranslate nohighlight">\(1/\sigma_{x,i}^2\)</span>), making it a “strong magnet.” The algorithm will work much harder to minimize the distance for this point, as it contributes significantly to the total sum <span class="math notranslate nohighlight">\(S\)</span>.</p></li>
<li><p>A point with large absolute errors will have a small weight, acting as a “weak magnet” with less influence on the final position of the curve.</p></li>
</ul>
<p>Therefore, the absolute magnitude of the errors is critical for determining which points the regression should prioritize.</p>
<p><strong>When Does Only the Ratio Matter for Homoscedastic Errors?</strong></p>
<p>The powerful insight that you might only need the error ratio is <strong>only true under a specific, common assumption: that the error magnitudes are constant for all data points</strong> (homoscedastic errors).</p>
<p>Let’s assume that <span class="math notranslate nohighlight">\(\sigma_{x,i} = \sigma_x\)</span> and <span class="math notranslate nohighlight">\(\sigma_{y,i} = \sigma_y\)</span> for all points <code class="docutils literal notranslate"><span class="pre">i</span></code>. In this case:</p>
<ol class="arabic simple">
<li><p>The ratio <span class="math notranslate nohighlight">\(\lambda_i\)</span> becomes a single constant for the entire dataset: <span class="math notranslate nohighlight">\(\lambda = \sigma_y^2 / \sigma_x^2\)</span>.</p></li>
<li><p>The weighting term <span class="math notranslate nohighlight">\(\frac{1}{\sigma_{x,i}^2}\)</span> also becomes a single constant: <span class="math notranslate nohighlight">\(\frac{1}{\sigma_x^2}\)</span>.</p></li>
</ol>
<p>Now, the objective function simplifies:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{1}{\sigma_x^2} \left( (x_i - \hat{x}_i)^2 + \frac{1}{\lambda}(y_i - \hat{y}_i)^2 \right)\]</div>
<p>Since <span class="math notranslate nohighlight">\(\frac{1}{\sigma_x^2}\)</span> is now a constant multiplier for the <em>entire sum</em>, we can factor it out:</p>
<div class="math notranslate nohighlight">
\[S = \frac{1}{\sigma_x^2} \sum_{i=1}^{n} \left( (x_i - \hat{x}_i)^2 + \frac{1}{\lambda}(y_i - \hat{y}_i)^2 \right)\]</div>
<p>Multiplying the entire function by a constant does not change the location of its minimum. Therefore, if you can assume that your measurement errors are consistent across all your data points, then you <strong>do not need to know their absolute values to find the best-fit curve; you only need to know their ratio, <span class="math notranslate nohighlight">\(\lambda\)</span>.</strong> This is a very common scenario in practice, for instance, when all measurements are made using the same instrument under the same conditions.</p>
<p>Now, let’s consider the implications of the error ratio <span class="math notranslate nohighlight">\(\lambda\)</span>. Let’s look at a single term in the sum for one data point:</p>
<div class="math notranslate nohighlight">
\[\text{Term}_i = \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2}\]</div>
<p>The algorithm wants to make this term small.</p>
<ul>
<li><p><strong>Case 1: <span class="math notranslate nohighlight">\(\lambda_i \to \infty\)</span> (Error in x is zero)</strong></p>
<p>This happens when <span class="math notranslate nohighlight">\(\sigma_{x,i}^2 \to 0\)</span>. Let’s look at the first part of the term: <span class="math notranslate nohighlight">\(\frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2}\)</span>.
As the denominator <span class="math notranslate nohighlight">\(\sigma_{x,i}^2\)</span> gets closer and closer to zero, this fraction will <strong>explode to infinity</strong> <em>unless</em> the numerator is also exactly zero. To prevent the total sum <span class="math notranslate nohighlight">\(S\)</span> from becoming infinite, the algorithm has no choice but to force the numerator to be zero.</p>
<p>It <strong>must</strong> set <span class="math notranslate nohighlight">\(\hat{x}_i = x_i\)</span>.</p>
<p>With this constraint, the first part of the term becomes zero. What’s left to minimize?</p>
<div class="math notranslate nohighlight">
\[\text{Term}_i = 0 + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2}\]</div>
<p>Since the fitted point must lie on the curve, we know <span class="math notranslate nohighlight">\(\hat{y}_i = f(\hat{x}_i)\)</span>. And because we were forced to set <span class="math notranslate nohighlight">\(\hat{x}_i = x_i\)</span>, this means <span class="math notranslate nohighlight">\(\hat{y}_i = f(x_i)\)</span>. Substituting this in, the total objective function becomes:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{(y_i - f(x_i))^2}{\sigma_{y,i}^2}\]</div>
<p>This is the <strong>exact formula for Weighted Least Squares (WLS)</strong>. ODR has transformed into WLS because the infinite penalty on any horizontal deviation left it with no freedom to adjust the x-positions.</p>
</li>
<li><p><strong>Case 2: <span class="math notranslate nohighlight">\(\lambda_i \to 0\)</span> (Error in y is zero)</strong></p>
<p>This is the symmetrical opposite. This happens when <span class="math notranslate nohighlight">\(\sigma_{y,i}^2 \to 0\)</span>. The second part of the term, <span class="math notranslate nohighlight">\(\frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2}\)</span>, will explode unless the algorithm forces <span class="math notranslate nohighlight">\(\hat{y}_i = y_i\)</span>. The objective function then reduces to minimizing only the horizontal weighted distances:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2}\]</div>
<p>This is equivalent to performing WLS, but with the roles of x and y swapped.</p>
</li>
<li><p><strong>Case 3: <span class="math notranslate nohighlight">\(\lambda_i = 1\)</span> (Errors are equal)</strong></p>
<p>This means <span class="math notranslate nohighlight">\(\sigma_{x,i}^2 = \sigma_{y,i}^2\)</span>. Let’s call this common variance <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>. The objective function becomes:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \left( \frac{(x_i - \hat{x}_i)^2}{\sigma_i^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_i^2} \right)\]</div>
<p>We can factor out the denominator:</p>
<div class="math notranslate nohighlight">
\[S = \sum_{i=1}^{n} \frac{1}{\sigma_i^2} \left( (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right)\]</div>
<p>By the Pythagorean theorem, the term <span class="math notranslate nohighlight">\((x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2\)</span> is simply the <strong>squared geometric (Euclidean) distance</strong> between the observed point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> and the fitted point <span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i)\)</span>. The overall objective function is therefore minimizing the weighted sum of these squared perpendicular distances. The weight for each point is given by its inverse variance <span class="math notranslate nohighlight">\(1/\sigma_i^2\)</span> giving more influence to the more certain data points.</p>
<p>This becomes a simple (unweighted) sum of squared perpendicular distances only in the even more specific case where all errors are identical (<span class="math notranslate nohighlight">\(\sigma_i = \sigma\)</span> for all points).</p>
</li>
</ul>
</section>
<section id="odr-with-non-linear-functions">
<h2>ODR with Non-linear Functions<a class="headerlink" href="#odr-with-non-linear-functions" title="Link to this heading">#</a></h2>
<p>ODR is especially powerful for non-linear regression because it can handle cases where both variables have errors and the relationship isn’t a straight line. The process involves an iterative numerical optimization algorithm to find the parameters of the non-linear function that minimize the objective function described above.</p>
<p>For your specific case with a non-linear function and errors for both the x-axis and y-axis, you will need to:</p>
<ol class="arabic simple">
<li><p><strong>Define your non-linear model function</strong>, e.g., <span class="math notranslate nohighlight">\(y = f(x; \beta_1, \beta_2, ...)\)</span> where <span class="math notranslate nohighlight">\(\beta_i\)</span> are the parameters you want to estimate.</p></li>
<li><p><strong>Provide your data</strong> (<span class="math notranslate nohighlight">\(x_i, y_i\)</span>).</p></li>
<li><p><strong>Specify the errors</strong> or weights for both variables.</p></li>
<li><p><strong>Use an ODR-specific software library or package</strong> (e.g., <code class="docutils literal notranslate"><span class="pre">scipy.odr</span></code> in Python) to perform the regression. The solver will then iteratively adjust the parameters of your function until the sum of the squared orthogonal distances is minimized. ODR typically uses trust-region Levenberg-Marquardt algorithms.</p></li>
</ol>
<p>The output will be the best-fit parameters for your non-linear function, along with their standard errors, which can be used to assess the uncertainty of the estimates.</p>
</section>
<section id="when-should-you-choose-odr-over-ols">
<h2>When Should You Choose ODR over OLS?<a class="headerlink" href="#when-should-you-choose-odr-over-ols" title="Link to this heading">#</a></h2>
<p>The choice between Ordinary Least Squares (OLS) and Orthogonal Distance Regression (ODR) depends entirely on the nature of the errors in your data. Since ODR is more complex and computationally intensive, it’s important to know when its use is truly justified.</p>
<p><strong>Use Ordinary Least Squares (OLS) when:</strong></p>
<ul class="simple">
<li><p><strong>Error in the independent variable (x) is negligible or zero.</strong> This is the classic assumption of OLS. If you are regressing against a variable like time, or a concentration that you set with high precision, the error in x is likely insignificant compared to the measurement error in y.</p></li>
<li><p><strong>The error in x is significantly smaller than the error in y.</strong> As a rule of thumb, if the standard deviation of the y-error is more than 5 to 10 times larger than the standard deviation of the x-error, the results from OLS will be very close to those from ODR. In this case, the simplicity and speed of OLS make it the preferred choice.</p></li>
<li><p><strong>The primary goal is prediction.</strong> If your main goal is to predict new y-values from new x-values (and you expect new data to have similar error properties), OLS provides an unbiased predictor for y given x, even if x has some error.</p></li>
</ul>
<p><strong>Use Orthogonal Distance Regression (ODR) when:</strong></p>
<ul class="simple">
<li><p><strong>Errors in both x and y are significant and of a similar magnitude.</strong> This is the primary use case for ODR. Ignoring significant errors in the independent variable leads to a bias known as regression dilution, where the estimated slope of the relationship is systematically underestimated (biased towards zero).</p></li>
<li><p><strong>The choice of independent vs. dependent variable is arbitrary.</strong> For example, if you are comparing two different instruments by measuring the same quantity with both, there is no physical reason to call one instrument’s measurement “x” and the other “y”. ODR treats both variables symmetrically, giving a result that is independent of this arbitrary choice. OLS would give a different best-fit line if you swapped the x and y axes.</p></li>
<li><p><strong>The model is highly non-linear.</strong> On very steep or very flat sections of a curve, the vertical distance minimized by OLS can be a poor representation of the “true” distance from a data point to the model. The perpendicular distance minimized by ODR is often more geometrically stable and robust in these cases.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Scenario</p></th>
<th class="head text-left"><p>Recommended Method</p></th>
<th class="head text-left"><p>Rationale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Error in x is negligible</p></td>
<td class="text-left"><p><strong>OLS</strong></p></td>
<td class="text-left"><p>The core assumption of OLS is met.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Error in x is much smaller than error in y</p></td>
<td class="text-left"><p><strong>OLS</strong></p></td>
<td class="text-left"><p>Results will be nearly identical to ODR, but OLS is simpler.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Errors in x and y are comparable</p></td>
<td class="text-left"><p><strong>ODR</strong></p></td>
<td class="text-left"><p>OLS will produce biased parameter estimates.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Variables are symmetric (e.g., comparing two methods)</p></td>
<td class="text-left"><p><strong>ODR</strong></p></td>
<td class="text-left"><p>The result should not depend on which variable is on which axis.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Highly non-linear model with errors in x</p></td>
<td class="text-left"><p><strong>ODR</strong></p></td>
<td class="text-left"><p>The perpendicular distance minimized by ODR is often more geometrically stable. OLS can suffer from <strong>projection bias</strong> on steep or flat sections of a curve, where the vertical distance is a poor approximation of the true geometric error.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="how-to-assess-the-goodness-of-fit-for-orthogonal-distance-regression-odr">
<h2>How to Assess the Goodness of Fit for Orthogonal Distance Regression (ODR)<a class="headerlink" href="#how-to-assess-the-goodness-of-fit-for-orthogonal-distance-regression-odr" title="Link to this heading">#</a></h2>
<p>Once you have performed an ODR fit, it is crucial to evaluate how well the resulting model actually represents your data. Because ODR does not seek to minimize the simple vertical distances like OLS, some of the most common goodness-of-fit metrics, like R-squared, are not appropriate. Instead, we must turn to metrics that are consistent with the ODR objective function.</p>
<p>The primary methods for assessing an ODR fit rely on the final, minimized value of the objective function itself, residual analysis, and information criteria for model comparison.</p>
<section id="the-chi-squared-chi-2-and-reduced-chi-squared-chi-2-red-statistics">
<h3>The Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) and Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2_{red}\)</span>) Statistics<a class="headerlink" href="#the-chi-squared-chi-2-and-reduced-chi-squared-chi-2-red-statistics" title="Link to this heading">#</a></h3>
<p>The most natural and powerful metrics for ODR goodness of fit are derived directly from the value that the algorithm worked to minimize.</p>
<ul>
<li><p><strong>The Chi-Squared Statistic (<span class="math notranslate nohighlight">\(\chi^2\)</span>)</strong></p>
<p>The final, minimized value of the ODR objective function, <span class="math notranslate nohighlight">\(S\)</span>, is itself a chi-squared statistic.</p>
<div class="math notranslate nohighlight">
\[ \chi^2 = S_{min} = \sum_{i=1}^{n} \left( \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2} \right) \]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{x}\)</span> and <span class="math notranslate nohighlight">\(\hat{y}\)</span> are fitted data points on the curve. This value represents the total weighted squared error of the fit. However, its absolute magnitude is difficult to interpret on its own, as it scales with the number of data points.</p>
</li>
<li><p><strong>The Reduced Chi-Squared Statistic (<span class="math notranslate nohighlight">\(\chi^2_{red}\)</span>) - The Best Indicator</strong></p>
<p>To create a standardized and highly interpretable metric, we calculate the <strong>reduced chi-squared statistic</strong>, which is the final <span class="math notranslate nohighlight">\(\chi^2\)</span> value divided by the <strong>degrees of freedom</strong> (<span class="math notranslate nohighlight">\(\nu\)</span>).</p>
<p>The degrees of freedom are the number of data points (<span class="math notranslate nohighlight">\(n\)</span>) minus the number of parameters (<span class="math notranslate nohighlight">\(p\)</span>) estimated by the model: <span class="math notranslate nohighlight">\(\nu = n - p\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \chi^2_{red} = \frac{\chi^2}{\nu} = \frac{S_{min}}{n - p} \]</div>
<p>The reduced chi-squared value provides a powerful assessment of the fit quality under the assumption that your error estimates (<span class="math notranslate nohighlight">\(\sigma_{x,i}, \sigma_{y,i}\)</span>) are accurate:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\chi^2_{red} \approx 1.0\)</span></strong>: This is the ideal outcome. It indicates that the model is a good fit to the data and that the measurement errors were likely estimated correctly. The observed scatter of the data points around the fitted curve is consistent with their reported error bars.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(\chi^2_{red} &gt; 1.0\)</span></strong>: This suggests a poor fit. There are two possible reasons:</p>
<ol class="arabic simple">
<li><p><strong>The model is wrong:</strong> The chosen function does not adequately describe the true physical relationship.</p></li>
<li><p><strong>The errors were underestimated:</strong> Your reported error values (<span class="math notranslate nohighlight">\(\sigma_{x,i}, \sigma_{y,i}\)</span>) are too small, and the data is actually “noisier” than you thought.</p></li>
<li><p><strong>Presence of Outliers</strong>: The dataset may contain one or more significant outliers. Because the chi-squared statistic is based on the sum of squared deviations, a few points that are very far from the fitted curve can disproportionately inflate the total value, leading to a high reduced chi-squared even if the model is appropriate for the rest of the data.</p></li>
</ol>
</li>
<li><p><strong><span class="math notranslate nohighlight">\(\chi^2_{red} &lt; 1.0\)</span></strong>: This indicates that the fit is “too good.” The data points are, on average, closer to the line than their error estimates would predict. This can mean:</p>
<ol class="arabic simple">
<li><p><strong>The errors were overestimated:</strong> Your reported error values are too large.</p></li>
<li><p><strong>The model is overfitting the data:</strong> This is a risk if the model is too complex relative to the number of data points.</p></li>
</ol>
</li>
</ul>
<p>Many regression packages, including <code class="docutils literal notranslate"><span class="pre">scipy.odr</span></code>, report this value as <code class="docutils literal notranslate"><span class="pre">res_var</span></code> (residual variance), which is equivalent to the reduced chi-squared.</p>
</li>
</ul>
<p>See the notes about non-linear case for the Reduced Chi-Squared <a class="reference internal" href="goodness-of-fit-and-chi-squared.html"><span class="std std-doc">here</span></a>. The interpretation of the reduced chi-squared statistic is most rigorous for models that are linear in their parameters. For non-linear models, the statistical assumptions that guarantee the chi-squared distribution are not always met, and the exact number of degrees of freedom can be ambiguous. In such cases, the reduced chi-squared value remains a powerful and widely used heuristic for assessing the goodness of fit, but it should be interpreted with a degree of caution.</p>
</section>
<section id="r-squared-r-2-and-adjusted-r-squared">
<h3>R-Squared (<span class="math notranslate nohighlight">\(R^2\)</span>) and Adjusted R-Squared<a class="headerlink" href="#r-squared-r-2-and-adjusted-r-squared" title="Link to this heading">#</a></h3>
<p>While R-squared is a common metric for OLS, its application to ODR is problematic. R-squared is based on the idea of explaining the variance in the y-variable, assuming the x-variable is known without error. This assumption is explicitly violated in ODR. Calculating a pseudo R-squared value for ODR can be done, but it often lacks a clear statistical interpretation and can be insensitive to the order of the variables (swapping x and y). For these reasons, <strong>R-squared and adjusted R-squared are generally not recommended</strong> for evaluating ODR results.</p>
</section>
<section id="information-criteria-aic-and-bic">
<h3>Information Criteria (AIC and BIC)<a class="headerlink" href="#information-criteria-aic-and-bic" title="Link to this heading">#</a></h3>
<p><strong>Recommended for Model Comparison.</strong></p>
<p>The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are excellent tools for <strong>comparing different models</strong> fit to the same dataset. They do not tell you if a single model is a “good fit” in an absolute sense, but they can tell you which model is better relative to others.</p>
<p>Both criteria work by balancing model fit with model complexity (the number of parameters, <span class="math notranslate nohighlight">\(p\)</span>).</p>
<p>When comparing two or more models, the one with the <strong>lower AIC or BIC value is considered better</strong>. It provides the most explanatory power for the least complexity.</p>
<p>Formulas for AIC and BIC, see <a class="reference internal" href="aic-and-bic.html"><span class="std std-doc">here</span></a>. Maximum log-likelihood derivation for the case of ODR that is necessary for calculation of AIC and BIC, you can find below.</p>
</section>
<section id="parameter-standard-errors">
<h3>Parameter Standard Errors<a class="headerlink" href="#parameter-standard-errors" title="Link to this heading">#</a></h3>
<p><strong>Parameter Standard Errors:</strong> The ODR output will provide the best-fit parameters along with their standard errors (e.g., <code class="docutils literal notranslate"><span class="pre">beta_std</span></code> in <code class="docutils literal notranslate"><span class="pre">scipy.odr</span></code>). A large standard error relative to the parameter value indicates that the parameter is not well-constrained by the data, which can be a sign of a poor model or insufficient data.</p>
</section>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Metric</p></th>
<th class="head text-left"><p>Applicability to ODR</p></th>
<th class="head text-left"><p>Purpose &amp; Interpretation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2_{red}\)</span>)</strong></p></td>
<td class="text-left"><p><strong>Highly Recommended</strong></p></td>
<td class="text-left"><p><strong>Primary indicator of fit quality.</strong> A value near 1.0 indicates a good fit where the model and error estimates are consistent.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>)</strong></p></td>
<td class="text-left"><p><strong>Recommended</strong></p></td>
<td class="text-left"><p>The raw, minimized sum of weighted squared errors. Used to calculate <span class="math notranslate nohighlight">\(\chi^2_{red}\)</span> and log-likelihood for AIC and BIC.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>AIC / BIC</strong></p></td>
<td class="text-left"><p><strong>Recommended</strong></p></td>
<td class="text-left"><p><strong>Model comparison.</strong> Helps select the best model from a set of candidates. Lower values are better.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Parameter Standard Errors</strong></p></td>
<td class="text-left"><p><strong>Essential</strong></p></td>
<td class="text-left"><p><strong>Assesses the certainty of the fit parameters.</strong> Large errors indicate poorly determined parameters.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>R-Squared (<span class="math notranslate nohighlight">\(R^2\)</span>)</strong></p></td>
<td class="text-left"><p><strong>Not Recommended</strong></p></td>
<td class="text-left"><p>Fundamentally inconsistent with the ODR objective function. Provides a misleading measure of fit.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="log-likelihood-for-odr">
<h2>Log-Likelihood for ODR<a class="headerlink" href="#log-likelihood-for-odr" title="Link to this heading">#</a></h2>
<p>For Orthogonal Distance Regression (ODR), the derivation of the log-likelihood function differs from Ordinary Least Squares (OLS) because ODR accounts for measurement errors in both the independent (x) and dependent (y) variables. More details about basis of log-likelihood see <a class="reference internal" href="aic-and-bic.html"><span class="std std-doc">here</span></a>.</p>
<p>The maximum likelihood approach for ODR is based on the assumption that the errors in both x and y are independent and normally distributed. For each data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, we assume there exists a “true” point <span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i)\)</span> on the model curve, <span class="math notranslate nohighlight">\(y = f(x; \beta)\)</span>, where <span class="math notranslate nohighlight">\(\beta\)</span> is the vector of model parameters. The observed data are subject to measurement errors.</p>
<p>The model is defined by:</p>
<div class="math notranslate nohighlight">
\[x_i = \hat{x}_i + \delta_i\]</div>
<div class="math notranslate nohighlight">
\[y_i = \hat{y}_i + \epsilon_i\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta_i\)</span> and <span class="math notranslate nohighlight">\(\epsilon_i\)</span> are the measurement errors in the x and y coordinates, respectively.</p>
<p>We assume these errors are independent and follow normal distributions with zero means and known variances:</p>
<div class="math notranslate nohighlight">
\[\delta_i \sim N(0, \sigma_{x,i}^2)\]</div>
<div class="math notranslate nohighlight">
\[\epsilon_i \sim N(0, \sigma_{y,i}^2)\]</div>
<p>The likelihood function for a single data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> is the joint probability density of the observed values, given the true values and model parameters. Assuming the errors are normally distributed, this is:</p>
<div class="math notranslate nohighlight">
\[L_i = f(x_i, y_i) = \frac{1}{2\pi \sigma_{x,i} \sigma_{y,i}} \exp \left( -\frac{(x_i - \hat{x}_i)^2}{2\sigma_{x,i}^2} - \frac{(y_i - \hat{y}_i)^2}{2\sigma_{y,i}^2} \right)\]</div>
<p>Since all observations are independent, the total likelihood function for a dataset of <span class="math notranslate nohighlight">\(n\)</span> points is the product of the individual likelihoods:</p>
<div class="math notranslate nohighlight">
\[L = \prod_{i=1}^n L_i = \prod_{i=1}^n \left[ \frac{1}{2\pi \sigma_{x,i} \sigma_{y,i}} \exp \left( -\frac{(x_i - \hat{x}_i)^2}{2\sigma_{x,i}^2} - \frac{(y_i - \hat{y}_i)^2}{2\sigma_{y,i}^2} \right) \right]\]</div>
<div class="math notranslate nohighlight">
\[L = \left[ \prod_{i=1}^n \frac{1}{2\pi \sigma_{x,i} \sigma_{y,i}} \right] \exp \left( -\sum_{i=1}^n \left( \frac{(x_i - \hat{x}_i)^2}{2\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{2\sigma_{y,i}^2} \right) \right)\]</div>
<p>The log-likelihood, which is easier to work with, is then:</p>
<div class="math notranslate nohighlight">
\[\ln L = \ln \left[ \left[ \prod_{i=1}^n \frac{1}{2\pi \sigma_{x,i} \sigma_{y,i}} \right] \cdot \exp \left( -\sum_{i=1}^n \left( \frac{(x_i - \hat{x}_i)^2}{2\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{2\sigma_{y,i}^2} \right) \right) \right]\]</div>
<div class="math notranslate nohighlight">
\[\ln L = \ln \left[ \prod_{i=1}^n \frac{1}{2\pi \sigma_{x,i} \sigma_{y,i}} \right] + \ln \exp \left( -\sum_{i=1}^n \left( \frac{(x_i - \hat{x}_i)^2}{2\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{2\sigma_{y,i}^2} \right) \right)\]</div>
<div class="math notranslate nohighlight">
\[\ln L = \sum_{i=1}^n \ln \left( \frac{1}{2\pi \sigma_{x,i} \sigma_{y,i}} \right) - \sum_{i=1}^n \left( \frac{(x_i - \hat{x}_i)^2}{2\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{2\sigma_{y,i}^2} \right)\]</div>
<div class="math notranslate nohighlight">
\[\ln L = - \sum_{i=1}^n \ln \left( 2\pi \sigma_{x,i} \sigma_{y,i} \right) - \frac{1}{2} \sum_{i=1}^n \left( \frac{(x_i - \hat{x}_i)^2}{\sigma_{x,i}^2} + \frac{(y_i - \hat{y}_i)^2}{\sigma_{y,i}^2} \right)\]</div>
<div class="math notranslate nohighlight">
\[\ln L = - \sum_{i=1}^n \ln \left( 2\pi \sigma_{x,i} \sigma_{y,i} \right) - \frac{1}{2} \chi^2\]</div>
<div class="math notranslate nohighlight">
\[\ln L = - \sum_{i=1}^n \ln \left( 2\pi \right) - \sum_{i=1}^n \ln \left(\sigma_{x,i} \right) - \sum_{i=1}^n \ln \left(\sigma_{y,i} \right) - \frac{1}{2} \chi^2\]</div>
<div class="math notranslate nohighlight">
\[\ln L = - n \ln \left( 2\pi \right) - \sum_{i=1}^n \ln \left(\sigma_{x,i} \right) - \sum_{i=1}^n \ln \left(\sigma_{y,i} \right) - \frac{1}{2} \chi^2\]</div>
<p>Maximizing of <span class="math notranslate nohighlight">\(L\)</span> is equivalent to minimizing of <span class="math notranslate nohighlight">\(\chi^2\)</span> (that is the purpose of the ODR) because all other terms in the aforementioned equation are already known.</p>
<p>If <span class="math notranslate nohighlight">\(\sigma_{x,i} = \sigma_x\)</span> and <span class="math notranslate nohighlight">\(\sigma_{y,i} = \sigma_y\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> values, then the aforementioned equation is simplified even more:</p>
<div class="math notranslate nohighlight">
\[\ln L = - n \ln \left( 2\pi \right) - n \ln \left(\sigma_x \right) - n \ln \left(\sigma_y \right) - \frac{1}{2} \chi^2\]</div>
</section>
<section id="additional-materials">
<h2>Additional Materials<a class="headerlink" href="#additional-materials" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/odr.html#module-scipy.odr">https://docs.scipy.org/doc/scipy/reference/odr.html#module-scipy.odr</a></p></li>
<li><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.odr.Output.html#scipy.odr.Output">https://docs.scipy.org/doc/scipy/reference/generated/scipy.odr.Output.html#scipy.odr.Output</a></p></li>
<li><p><a class="reference external" href="https://stackoverflow.com/questions/21395328/how-to-estimate-goodness-of-fit-using-scipy-odr">https://stackoverflow.com/questions/21395328/how-to-estimate-goodness-of-fit-using-scipy-odr</a></p></li>
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/325581/odr-residual-variance-and-reduced-chi2-do-the-beta-uncertainties-represent-co">https://stats.stackexchange.com/questions/325581/odr-residual-variance-and-reduced-chi2-do-the-beta-uncertainties-represent-co</a></p></li>
<li><p><a class="reference external" href="https://stackoverflow.com/questions/41028846/how-to-compute-standard-error-from-odr-results">https://stackoverflow.com/questions/41028846/how-to-compute-standard-error-from-odr-results</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./math\01_basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="weighted-least-squares-code-3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">WLS - Code Examples Part 3</p>
      </div>
    </a>
    <a class="right-next"
       href="odr-code.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">ODR - Code Examples</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-principles-of-odr">General Principles of ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-two-roles-of-error-in-odr">The Two Roles of Error in ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#odr-with-non-linear-functions">ODR with Non-linear Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-choose-odr-over-ols">When Should You Choose ODR over OLS?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-assess-the-goodness-of-fit-for-orthogonal-distance-regression-odr">How to Assess the Goodness of Fit for Orthogonal Distance Regression (ODR)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chi-squared-chi-2-and-reduced-chi-squared-chi-2-red-statistics">The Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) and Reduced Chi-Squared (<span class="math notranslate nohighlight">\(\chi^2_{red}\)</span>) Statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-r-2-and-adjusted-r-squared">R-Squared (<span class="math notranslate nohighlight">\(R^2\)</span>) and Adjusted R-Squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-criteria-aic-and-bic">Information Criteria (AIC and BIC)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-standard-errors">Parameter Standard Errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood-for-odr">Log-Likelihood for ODR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional Materials</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Vladyslav Yakovliev (Ukraine)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>