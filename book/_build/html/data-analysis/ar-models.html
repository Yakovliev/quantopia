
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Autoregressive (AR) Models &#8212; Quantopia&#39;:&#39; Physics, Python and Pi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'data-analysis/ar-models';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Quantopia':' Physics, Python and Pi - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Quantopia':' Physics, Python and Pi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Quantopia: Physics, Python, and Pi (Alpha Version)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">MATH</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../math/gradient-operator.html">Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/gradient-directional-derivative.html">Directional Derivative</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/divergence.html">Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/fourier-transform-01.html">Fourier Transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/uncertainties-introduction.html">Experimental Errors and Significant Figures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/least-squares-regression.html">Least Squares Regression, RSS, RMSE, R-squared</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/least-squares-regression-code.html">Least Squares Regression - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/ordinary-least-squares.html">Ordinary Least Squares (OLS) Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/ordinary-least-squares-code.html">Ordinary Least Squares (OLS) Regression - Code Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/variance-covariance.html">Variance and Covariance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/variance-covariance-code.html">Variance and Covariance - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/correlation-coefficients.html">Correlation Coefficients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/correlation-coefficients-code.html">Correlation Coefficients - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/weighted-least-squares.html">Weighted Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/weighted-least-squares-code-1.html">WLS - Code Examples Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/weighted-least-squares-code-2.html">WLS - Code Examples Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/goodness-of-fit-and-chi-squared.html">Goodness of Fit and Chi-Squared Statistic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/aic-and-bic.html">Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/weighted-least-squares-code-3.html">WLS - Code Examples Part 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/orthogonal-distance-regression.html">Orthogonal Distance Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/odr-code.html">ODR - Code Examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PHYSICS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../physics/continuity-equation-01.html">The Continuity Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/continuity-equation-02.html">The Continuity Equation: One-Dimensional Advection of a Density Profile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/ensemble.html">Statistical Ensembles and Liouville’s Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/microcanonical-ensemble.html">Microcanonical Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/inertial_vs_gravitational_mass.html">Mass: Inertial vs. Gravitational</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DATA ANALYSIS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="rate-of-return-metrics.html">Advanced Rate of Return Metrics (IRR, XIRR, MIRR, XMIRR, PV, FV, NPV, XNPV)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia/issues/new?title=Issue%20on%20page%20%2Fdata-analysis/ar-models.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/data-analysis/ar-models.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Autoregressive (AR) Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Autoregressive (AR) Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ar-1-model-our-starting-point">The AR(1) Model: Our Starting Point</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-ar-p-model">The General AR(p) Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underlying-mathematical-assumptions-the-foundation-of-ar-models">Underlying Mathematical Assumptions: The Foundation of AR Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stationarity-of-the-time-series-y-t">Stationarity of the Time Series (<span class="math notranslate nohighlight">\(Y_t\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#white-noise-error-term-epsilon-t">White Noise Error Term (<span class="math notranslate nohighlight">\(\epsilon_t\)</span>)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-stationarity-condition-for-ar-1">Derivation of Stationarity Condition for AR(1)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#condition-for-stationarity-in-ar-p-the-characteristic-polynomial">Condition for Stationarity in AR(p): The Characteristic Polynomial</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-notes-about-characteristic-polynomial">Additional Notes about Characteristic Polynomial</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-from-the-ar-1-model">Intuition from the AR(1) Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-to-the-ar-p-model">Generalizing to the AR(p) Model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#identifying-key-parameters-unveiling-the-model-s-secrets">Identifying Key Parameters: Unveiling the Model’s Secrets</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determining-the-order-p-autocorrelation-and-partial-autocorrelation-functions">4.1 Determining the Order (p): Autocorrelation and Partial Autocorrelation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autocorrelation-function-acf">Autocorrelation Function (ACF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-autocorrelation-function-pacf">Partial Autocorrelation Function (PACF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-criteria-aic-bic">Information Criteria (AIC, BIC)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-coefficients-phi-i-and-constant-c">4.2 Estimating Coefficients (<span class="math notranslate nohighlight">\(\phi_i\)</span>) and Constant (c)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-ols-for-ar-1-model">Derivation of OLS for AR(1) Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-of-ols-to-ar-p">Generalization of OLS to AR(p)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#yule-walker-equations">Yule-Walker Equations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-theory-into-practice-python-with-statsmodels">5. Putting Theory into Practice: Python with <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-synthetic-ar-1-data">5.1 Generating Synthetic AR(1) Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-for-stationarity-with-augmented-dickey-fuller-adf-test">5.2 Testing for Stationarity with Augmented Dickey-Fuller (ADF) Test</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-acf-and-pacf">5.3 Visualizing ACF and PACF</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-ar-model">5.4 Fitting the AR Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forecasting">5.5 Forecasting</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-the-power-of-self-referential-modeling">6. Conclusion: The Power of Self-Referential Modeling</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional materials</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-difference-equations">Linear difference equations</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="autoregressive-ar-models">
<h1>Autoregressive (AR) Models<a class="headerlink" href="#autoregressive-ar-models" title="Link to this heading">#</a></h1>
<p>Imagine we are tracking the daily temperature in a specific city. We intuitively know that today’s temperature is probably influenced by yesterday’s temperature, and perhaps even the day before that. It’s unlikely that the temperature will suddenly jump from freezing to boiling hot without any prior indication. This idea - that a value in a sequence is dependent on its preceding values - is at the heart of time series analysis.</p>
<p>A <strong>time series</strong> is simply a sequence of data points indexed in time order. Examples include stock prices, monthly sales figures, daily rainfall, or even your heart rate over time. When we model a time series based on its <em>own past values</em>, we are engaging in a form of <strong>autoregression</strong>. The prefix “auto-” means “self,” so we are essentially regressing the series on itself. Also, we assume that data points are equally spaced points in time.</p>
<p>This approach is incredibly useful because it allows us to capture the inherent temporal dependencies within the data. Instead of looking for external factors (like economic indicators affecting stock prices), we first try to understand the internal dynamics of the series itself.</p>
<p>The Autoregressive (AR) model is one of the simplest yet most effective tools for modeling time series data that exhibits autocorrelation, meaning that past values have a linear influence on current values. The Autoregressive (AR) model is a foundational approach that assumes the current value of the series, <span class="math notranslate nohighlight">\(Y_t\)</span>, can be explained as a linear function of its own past values, or “lags”.</p>
<p>Think of an echo in a canyon. The sound we hear now is, in part, an echo of the sound produced moments ago. The strength of that echo diminishes over time. Similarly, in an AR model, the current value of a time series is a “noisy echo” of its past values.</p>
<p>Consider a simple scenario: predicting your mood tomorrow. Your mood today might be a significant factor. If you’re happy today, you’re more likely to be happy tomorrow. However, your mood from two days ago might have less direct influence, and your mood from a month ago probably has very little direct influence. The AR model formalizes this idea of decaying influence from past observations.</p>
<section id="the-ar-1-model-our-starting-point">
<h2>The AR(1) Model: Our Starting Point<a class="headerlink" href="#the-ar-1-model-our-starting-point" title="Link to this heading">#</a></h2>
<p>Let’s begin with the simplest form of the Autoregressive model, known as AR(1). The ‘1’ indicates that the current value of the time series depends only on its immediately preceding value.</p>
<p>We define the AR(1) model as:</p>
<div class="math notranslate nohighlight">
\[Y_t = c + \phi_1 Y_{t-1} + \epsilon_t\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y_t\)</span> is the value of the time series at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(Y_{t-1}\)</span> is the value of the time series at the previous time step, <span class="math notranslate nohighlight">\(t-1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> is a constant term, representing the intercept or the baseline level of the series if all past influences were zero.</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi_1\)</span> (phi-one) is the autoregressive coefficient. It quantifies the linear influence of the previous value <span class="math notranslate nohighlight">\(Y_{t-1}\)</span> on the current value <span class="math notranslate nohighlight">\(Y_t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_t\)</span> (epsilon-t) is the error term, also known as the white noise term. It represents the random shock or unpredictable component at time <span class="math notranslate nohighlight">\(t\)</span> that cannot be explained by past values.</p></li>
</ul>
</section>
<section id="the-general-ar-p-model">
<h2>The General AR(p) Model<a class="headerlink" href="#the-general-ar-p-model" title="Link to this heading">#</a></h2>
<p>The AR(1) model can be extended to an AR(p) model, where <code class="docutils literal notranslate"><span class="pre">p</span></code> denotes the order of the model. An AR(p) model means that the current value <span class="math notranslate nohighlight">\(Y_t\)</span> depends on its past <span class="math notranslate nohighlight">\(p\)</span> values (<span class="math notranslate nohighlight">\(Y_{t-1}, Y_{t-2}, ..., Y_{t-p}\)</span>).</p>
<p>We define the general AR(p) model as:</p>
<div class="math notranslate nohighlight">
\[Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t\]</div>
<p>This can be written more compactly using summation notation:</p>
<div class="math notranslate nohighlight">
\[Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p\)</span> is the order of the AR model, representing how many past values influence the current value. It’s a dimensionless integer.</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi_i\)</span> are the autoregressive coefficients for lag <span class="math notranslate nohighlight">\(i\)</span>. These are dimensionless quantities.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="underlying-mathematical-assumptions-the-foundation-of-ar-models">
<h2>Underlying Mathematical Assumptions: The Foundation of AR Models<a class="headerlink" href="#underlying-mathematical-assumptions-the-foundation-of-ar-models" title="Link to this heading">#</a></h2>
<p>For the AR model to be valid and for its parameters to be reliably estimated, we rely on several key mathematical assumptions.</p>
<section id="stationarity-of-the-time-series-y-t">
<h3>Stationarity of the Time Series (<span class="math notranslate nohighlight">\(Y_t\)</span>)<a class="headerlink" href="#stationarity-of-the-time-series-y-t" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What it means</strong>: A time series is considered <strong>stationary</strong> if its statistical properties (like mean, variance, and autocorrelation) do not change over time. Imagine taking a “snapshot” of the series at different points in time; the statistical characteristics should look roughly the same.</p></li>
<li><p><strong>Why it’s important</strong>: Stationarity is crucial because it allows us to make inferences about future values based on past observations. If the underlying statistical process is changing, then past relationships might not hold true in the future.</p></li>
<li><p><strong>Types of Stationarity</strong>:</p>
<ul>
<li><p><strong>Strict Stationarity</strong>: The joint probability distribution of any set of observations <span class="math notranslate nohighlight">\((Y_{t_1}, ..., Y_{t_k})\)</span> is the same as <span class="math notranslate nohighlight">\((Y_{t_1+h}, ..., Y_{t_k+h})\)</span> for any time shift <span class="math notranslate nohighlight">\(h\)</span>. This is a very strong condition.</p></li>
<li><p><strong>Weak (or Covariance) Stationarity</strong>: This is a more commonly used and less restrictive assumption. A time series is weakly stationary if:</p>
<ol class="arabic simple">
<li><p>The mean is constant: <span class="math notranslate nohighlight">\(E[Y_t] = \mu\)</span> for all <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>The variance is constant: <span class="math notranslate nohighlight">\(Var(Y_t) = E[(Y_t - \mu)^2] = \sigma_Y^2\)</span> for all <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>The autocovariance depends only on the lag, not on the time <span class="math notranslate nohighlight">\(t\)</span>: <span class="math notranslate nohighlight">\(Cov(Y_t, Y_{t-k}) = E[(Y_t - \mu)(Y_{t-k} - \mu)] = \gamma_k\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> and any lag <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
</ol>
</li>
</ul>
</li>
</ul>
</section>
<section id="white-noise-error-term-epsilon-t">
<h3>White Noise Error Term (<span class="math notranslate nohighlight">\(\epsilon_t\)</span>)<a class="headerlink" href="#white-noise-error-term-epsilon-t" title="Link to this heading">#</a></h3>
<p>The error term <span class="math notranslate nohighlight">\(\epsilon_t\)</span> is assumed to be a <strong>white noise process</strong>. This means it is a sequence of uncorrelated random variables with constant mean and variance.
Specifically, for <span class="math notranslate nohighlight">\(\epsilon_t\)</span>:</p>
<ol class="arabic">
<li><p><strong>Zero Mean</strong>: The expected value of the error at any time point is zero.</p>
<div class="math notranslate nohighlight">
\[E[\epsilon_t] = 0\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\)</span>.</p>
</li>
<li><p><strong>Constant Variance</strong>: The variance of the error is constant over time. This property is called <strong>homoscedasticity</strong>.</p>
<div class="math notranslate nohighlight">
\[Var(\epsilon_t) = \sigma_\epsilon^2\]</div>
</li>
<li><p><strong>No Autocorrelation</strong>: The errors are uncorrelated with each other across time. The error at one point in time gives no information about the error at another point. This is crucial because it means that the errors themselves are unpredictable and do not contain any systematic information that could be used to improve the model. If errors were correlated, it would imply that our model hasn’t captured all the temporal dependencies.</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(\epsilon_t, \epsilon_s) = 0 \quad \text{for} \quad t \neq s\]</div>
</li>
<li><p><strong>Independence (often implied or assumed for inference)</strong>: While not strictly necessary for estimation methods like OLS, for statistical inference (e.g., hypothesis testing, confidence intervals), it is often assumed that <span class="math notranslate nohighlight">\(\epsilon_t\)</span> is independently and identically distributed (i.i.d.).</p></li>
<li><p><strong>Often (but not always) Normal Distribution</strong>: For statistical inference, it is often assumed that <span class="math notranslate nohighlight">\(\epsilon_t \sim N(0, \sigma_\epsilon^2)\)</span>.</p></li>
</ol>
<p>These assumptions ensure that our model is well-behaved, that we can accurately estimate its parameters, and that our forecasts are statistically sound.</p>
</section>
</section>
<section id="derivation-of-stationarity-condition-for-ar-1">
<h2>Derivation of Stationarity Condition for AR(1)<a class="headerlink" href="#derivation-of-stationarity-condition-for-ar-1" title="Link to this heading">#</a></h2>
<p>Let’s derive the condition for stationarity for an AR(1) model.</p>
<p>Our AR(1) model is <span class="math notranslate nohighlight">\(Y_t = c + \phi_1 Y_{t-1} + \epsilon_t\)</span>.</p>
<p><strong>1. Constant Mean (<span class="math notranslate nohighlight">\(E[Y_t] = \mu\)</span>)</strong>:</p>
<p>If the series is stationary, then <span class="math notranslate nohighlight">\(E[Y_t] = E[Y_{t-1}] = \mu\)</span>. We also assume the error term <span class="math notranslate nohighlight">\(\epsilon_t\)</span> has a mean of zero, i.e., <span class="math notranslate nohighlight">\(E[\epsilon_t] = 0\)</span>.</p>
<p>Taking the expectation of the AR(1) equation:</p>
<div class="math notranslate nohighlight">
\[E[Y_t] = E[c + \phi_1 Y_{t-1} + \epsilon_t]\]</div>
<p>Using the linearity of expectation:</p>
<div class="math notranslate nohighlight">
\[E[Y_t] = E[c] + E[\phi_1 Y_{t-1}] + E[\epsilon_t]\]</div>
<div class="math notranslate nohighlight">
\[\mu = c + \phi_1 E[Y_{t-1}] + 0\]</div>
<p>Since <span class="math notranslate nohighlight">\(E[Y_{t-1}] = \mu\)</span> for a stationary series:</p>
<div class="math notranslate nohighlight">
\[\mu = c + \phi_1 \mu\]</div>
<p>Now, we can solve for <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mu - \phi_1 \mu = c\]</div>
<div class="math notranslate nohighlight">
\[\mu (1 - \phi_1) = c\]</div>
<div class="math notranslate nohighlight">
\[\mu = \frac{c}{1 - \phi_1}\]</div>
<p>For <span class="math notranslate nohighlight">\(\mu\)</span> to be well-defined and finite, we must have <span class="math notranslate nohighlight">\(1 - \phi_1 \neq 0\)</span>, which means <span class="math notranslate nohighlight">\(\phi_1 \neq 1\)</span>.</p>
<p><strong>2. Constant Variance (<span class="math notranslate nohighlight">\(Var(Y_t) = \sigma_Y^2\)</span>)</strong>:</p>
<p>For a stationary series, the variance <span class="math notranslate nohighlight">\(Var(Y_t)\)</span> must also be constant over time, let’s denote it as <span class="math notranslate nohighlight">\(\sigma_Y^2\)</span>. We also assume <span class="math notranslate nohighlight">\(\epsilon_t\)</span> is white noise with variance <span class="math notranslate nohighlight">\(Var(\epsilon_t) = \sigma_\epsilon^2\)</span>.</p>
<p>First, let’s express <span class="math notranslate nohighlight">\(Y_t\)</span> in terms of deviations from its mean, <span class="math notranslate nohighlight">\(Y_t - \mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[Y_t - \mu = c + \phi_1 Y_{t-1} + \epsilon_t - \mu\]</div>
<p>Substitute <span class="math notranslate nohighlight">\(c = \mu(1 - \phi_1)\)</span> (from the mean derivation):</p>
<div class="math notranslate nohighlight">
\[Y_t - \mu = \mu(1 - \phi_1) + \phi_1 Y_{t-1} + \epsilon_t - \mu\]</div>
<div class="math notranslate nohighlight">
\[Y_t - \mu = \mu - \phi_1 \mu + \phi_1 Y_{t-1} + \epsilon_t - \mu\]</div>
<div class="math notranslate nohighlight">
\[Y_t - \mu = \phi_1 (Y_{t-1} - \mu) + \epsilon_t\]</div>
<p>Let’s denote <span class="math notranslate nohighlight">\(y'_t = Y_t - \mu\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[y'_t = \phi_1 y'_{t-1} + \epsilon_t\]</div>
<p>Now, we can find the variance <span class="math notranslate nohighlight">\(Var(y'_t) = \sigma_Y^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[Var(y'_t) = Var(\phi_1 y'_{t-1} + \epsilon_t)\]</div>
<p>Since <span class="math notranslate nohighlight">\(\epsilon_t\)</span> is independent of <span class="math notranslate nohighlight">\(y'_{t-1}\)</span> (as <span class="math notranslate nohighlight">\(\epsilon_t\)</span> is a shock at time <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(y'_{t-1}\)</span> depends on past shocks), we can write:</p>
<div class="math notranslate nohighlight">
\[Var(y'_t) = Var(\phi_1 y'_{t-1}) + Var(\epsilon_t)\]</div>
<div class="math notranslate nohighlight">
\[\sigma_Y^2 = \phi_1^2 Var(y'_{t-1}) + \sigma_\epsilon^2\]</div>
<p>For a stationary series, <span class="math notranslate nohighlight">\(Var(y'_t) = Var(y'_{t-1}) = \sigma_Y^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\sigma_Y^2 = \phi_1^2 \sigma_Y^2 + \sigma_\epsilon^2\]</div>
<div class="math notranslate nohighlight">
\[\sigma_Y^2 (1 - \phi_1^2) = \sigma_\epsilon^2\]</div>
<div class="math notranslate nohighlight">
\[\sigma_Y^2 = \frac{\sigma_\epsilon^2}{1 - \phi_1^2}\]</div>
<p>For the variance <span class="math notranslate nohighlight">\(\sigma_Y^2\)</span> to be positive and finite, we must have <span class="math notranslate nohighlight">\(1 - \phi_1^2 &gt; 0\)</span>.
This implies <span class="math notranslate nohighlight">\(\phi_1^2 &lt; 1\)</span>, which means <span class="math notranslate nohighlight">\(-1 &lt; \phi_1 &lt; 1\)</span>.</p>
<p>So, the condition for an AR(1) process to be weakly stationary is that the absolute value of its autoregressive coefficient <span class="math notranslate nohighlight">\(\phi_1\)</span> must be less than 1:</p>
<div class="math notranslate nohighlight">
\[|\phi_1| &lt; 1\]</div>
<p>If <span class="math notranslate nohighlight">\(|\phi_1| \ge 1\)</span>, the series will exhibit explosive behavior (grow indefinitely) or persist indefinitely, and its variance will not be constant.</p>
</section>
<section id="condition-for-stationarity-in-ar-p-the-characteristic-polynomial">
<h2>Condition for Stationarity in AR(p): The Characteristic Polynomial<a class="headerlink" href="#condition-for-stationarity-in-ar-p-the-characteristic-polynomial" title="Link to this heading">#</a></h2>
<p>For an AR(p) process to be weakly stationary, the condition is a generalization of the <span class="math notranslate nohighlight">\(|\phi_1| &lt; 1\)</span> requirement. It involves the roots of its characteristic polynomial.</p>
<p>The original model is:</p>
<div class="math notranslate nohighlight">
\[Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t\]</div>
<p>Or, using summation notation:</p>
<div class="math notranslate nohighlight">
\[Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t\]</div>
<p>Just like in the AR(1) case, we assume the process is <strong>stationary</strong>. A key property of a stationary process is that its mean is constant over time. Let’s call this mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[Y_t] = \mu\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(E[Y_{t-1}] = \mu\)</span></p></li>
<li><p>…</p></li>
<li><p><span class="math notranslate nohighlight">\(E[Y_{t-p}] = \mu\)</span></p></li>
</ul>
<p>We also know that the error term <span class="math notranslate nohighlight">\(\epsilon_t\)</span> is white noise with a mean of zero:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[\epsilon_t] = 0\)</span></p></li>
</ul>
<p>Now, let’s take the expectation (the “mean”) of both sides of the AR(p) equation:</p>
<div class="math notranslate nohighlight">
\[E[Y_t] = E[c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t]\]</div>
<p>Using the rules of expectation (it’s a linear operator):</p>
<div class="math notranslate nohighlight">
\[\mu = E[c] + \sum_{i=1}^{p} \phi_i E[Y_{t-i}] + E[\epsilon_t]\]</div>
<p>Substitute the known mean values:</p>
<div class="math notranslate nohighlight">
\[\mu = c + \sum_{i=1}^{p} \phi_i \mu + 0\]</div>
<p>Now we have an expression relating the constant <span class="math notranslate nohighlight">\(c\)</span> to the mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>Our goal is to substitute <span class="math notranslate nohighlight">\(c\)</span> out of the original equation. Let’s rearrange the equation to solve for <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mu - \sum_{i=1}^{p} \phi_i \mu = c\]</div>
<p>Factor out <span class="math notranslate nohighlight">\(\mu\)</span> from the left side:</p>
<div class="math notranslate nohighlight">
\[c = \mu \left( 1 - \sum_{i=1}^{p} \phi_i \right)\]</div>
<p>This equation tells us that the constant term <span class="math notranslate nohighlight">\(c\)</span> is directly related to the mean of the series (<span class="math notranslate nohighlight">\(\mu\)</span>) and the sum of the autoregressive coefficients (<span class="math notranslate nohighlight">\(\phi_i\)</span>).</p>
<p>Now, take the original AR(p) model and replace <span class="math notranslate nohighlight">\(c\)</span> with the expression we just derived:</p>
<div class="math notranslate nohighlight">
\[Y_t = \underbrace{\mu \left( 1 - \sum_{i=1}^{p} \phi_i \right)}_{c} + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t\]</div>
<p>This is where the algebra comes in. We want to group terms that involve <span class="math notranslate nohighlight">\(\mu\)</span> with terms that involve <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>First, move the <span class="math notranslate nohighlight">\(\mu\)</span> term from the right side to the left side:</p>
<div class="math notranslate nohighlight">
\[Y_t - \mu = -\mu \sum_{i=1}^{p} \phi_i + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t\]</div>
<p>Now, we can combine the two summations because they both run from <span class="math notranslate nohighlight">\(i=1\)</span> to <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="math notranslate nohighlight">
\[Y_t - \mu = \sum_{i=1}^{p} (\phi_i Y_{t-i} - \phi_i \mu) + \epsilon_t\]</div>
<p>Factor out the <span class="math notranslate nohighlight">\(\phi_i\)</span> term inside the summation:</p>
<div class="math notranslate nohighlight">
\[Y_t - \mu = \sum_{i=1}^{p} \phi_i (Y_{t-i} - \mu) + \epsilon_t\]</div>
<p>The term <span class="math notranslate nohighlight">\((Y_t - \mu)\)</span> represents the deviation of the series from its mean at time <span class="math notranslate nohighlight">\(t\)</span>. The term <span class="math notranslate nohighlight">\((Y_{t-i} - \mu)\)</span> is the deviation from the mean at time <span class="math notranslate nohighlight">\((t-i)\)</span>.</p>
<p>Let’s define a new variable, <span class="math notranslate nohighlight">\(y'_t\)</span>, to represent this deviation:</p>
<div class="math notranslate nohighlight">
\[y'_t = Y_t - \mu\]</div>
<p>Now, substitute this new variable into our rearranged equation:</p>
<div class="math notranslate nohighlight">
\[y'_t = \sum_{i=1}^{p} \phi_i y'_{t-i} + \epsilon_t\]</div>
<p>If we expand the summation, we get the following expression:</p>
<div class="math notranslate nohighlight">
\[y'_t = \phi_1 y'_{t-1} + \phi_2 y'_{t-2} + \dots + \phi_p y'_{t-p} + \epsilon_t\]</div>
<p>This is the <strong>mean-centered</strong> or <strong>de-meaned</strong> form of the AR(p) model. We’ve shown that an AR(p) process with a constant <span class="math notranslate nohighlight">\(c\)</span> is equivalent to an AR(p) process with no constant, but applied to the series after its mean has been subtracted out.</p>
<p>This simplified form is much easier to work with when analyzing theoretical properties like stationarity using tools like the characteristic polynomial, as it removes the constant <span class="math notranslate nohighlight">\(c\)</span>.</p>
<p>Let’s introduce the <strong>lag operator</strong> <span class="math notranslate nohighlight">\(L\)</span>, where <span class="math notranslate nohighlight">\(L Y_t = Y_{t-1}\)</span>, <span class="math notranslate nohighlight">\(L^2 Y_t = Y_{t-2}\)</span>, and so on.</p>
<p>So, we have:</p>
<div class="math notranslate nohighlight">
\[y'_t = \phi_1 y'_{t-1} + \phi_2 y'_{t-2} + \dots + \phi_p y'_{t-p} + \epsilon_t\]</div>
<p>Now, apply the lag operator:</p>
<div class="math notranslate nohighlight">
\[y'_t = \phi_1 L y'_t + \phi_2 L^2 y'_t + \dots + \phi_p L^p y'_t + \epsilon_t\]</div>
<p>Rearranging the terms to group <span class="math notranslate nohighlight">\(y'_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[y'_t - \phi_1 L y'_t - \phi_2 L^2 y'_t - \dots - \phi_p L^p y'_t = \epsilon_t\]</div>
<div class="math notranslate nohighlight">
\[(1 - \phi_1 L - \phi_2 L^2 - \dots - \phi_p L^p) y'_t = \epsilon_t\]</div>
<p>The polynomial in <span class="math notranslate nohighlight">\(L\)</span> is called the <strong>autoregressive polynomial</strong>:</p>
<div class="math notranslate nohighlight">
\[\Phi(L) = 1 - \phi_1 L - \phi_2 L^2 - \dots - \phi_p L^p\]</div>
<p>So the AR(<span class="math notranslate nohighlight">\(p\)</span>) model can be written as:</p>
<div class="math notranslate nohighlight">
\[\Phi(L) y'_t = \epsilon_t\]</div>
<p>The <strong>characteristic equation</strong> is obtained by setting the autoregressive polynomial to zero and replacing the lag operator <span class="math notranslate nohighlight">\(L\)</span> with a complex variable (often <span class="math notranslate nohighlight">\(z\)</span> or <span class="math notranslate nohighlight">\(x\)</span>):</p>
<div class="math notranslate nohighlight">
\[1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0\]</div>
<p>The condition for an AR(<span class="math notranslate nohighlight">\(p\)</span>) process to be stationary is that <strong>all the roots of the characteristic equation must lie outside the unit circle</strong> in the complex plane. This means that the absolute value (modulus) of each root must be greater than 1. If any root lies inside or on the unit circle, the process is non-stationary and requires differencing (leading to ARIMA models) or other transformations.</p>
<section id="additional-notes-about-characteristic-polynomial">
<h3>Additional Notes about Characteristic Polynomial<a class="headerlink" href="#additional-notes-about-characteristic-polynomial" title="Link to this heading">#</a></h3>
<p>We rearranged our AR(p) model into a compact form using the lag operator <span class="math notranslate nohighlight">\(L\)</span>:</p>
<div class="math notranslate nohighlight">
\[(1 - \phi_1 L - \phi_2 L^2 - \dots - \phi_p L^p) y'_t = \epsilon_t\]</div>
<p>Or more simply:</p>
<div class="math notranslate nohighlight">
\[\Phi(L) y'_t = \epsilon_t\]</div>
<p>Then, we stated that to check for stationarity, we must analyze the roots of the <strong>characteristic equation</strong>, which is found by setting the autoregressive polynomial to zero and replacing the lag operator <span class="math notranslate nohighlight">\(L\)</span> with a complex variable <span class="math notranslate nohighlight">\(z\)</span>:</p>
<div class="math notranslate nohighlight">
\[\Phi(z) = 1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0\]</div>
<p>This can feel like a “black box” mathematical trick. Why do we do this? The reason is that we are borrowing a powerful technique used for solving <strong>linear difference equations</strong> (note that this is NOT linear differential equations). You can find more information about this type of equations in the “Additional Materials” section of this lecture.</p>
<p>Our AR(p) model for the mean-centered series, <span class="math notranslate nohighlight">\(y'_t = \sum_{i=1}^{p} \phi_i y'_{t-i} + \epsilon_t\)</span>, is precisely such an equation. The value at time <span class="math notranslate nohighlight">\(t\)</span> is a linear combination of previous values, plus an external term (<span class="math notranslate nohighlight">\(\epsilon_t\)</span>).</p>
<p>The stability and long-term behavior of such a system are determined by its <strong>homogeneous part</strong> (the equation without the error term <span class="math notranslate nohighlight">\(\epsilon_t\)</span>):</p>
<div class="math notranslate nohighlight">
\[y'_t - \phi_1 y'_{t-1} - \phi_2 y'_{t-2} - \dots - \phi_p y'_{t-p} = 0\]</div>
<p>To understand the behavior of this system, mathematicians look for solutions of the form <span class="math notranslate nohighlight">\(y'_t = x^t\)</span>. Why this form? Because it has a convenient property where lags are just powers of <span class="math notranslate nohighlight">\(x\)</span>: <span class="math notranslate nohighlight">\(y'_{t-1} = x^{t-1}\)</span>, <span class="math notranslate nohighlight">\(y'_{t-2} = x^{t-2}\)</span>, and so on.</p>
<p>If we substitute <span class="math notranslate nohighlight">\(y'_t = x^t\)</span> into the homogeneous equation, we get:</p>
<div class="math notranslate nohighlight">
\[x^t - \phi_1 x^{t-1} - \phi_2 x^{t-2} - \dots - \phi_p x^{t-p} = 0\]</div>
<p>Now, we can divide the entire equation by <span class="math notranslate nohighlight">\(x^{t-p}\)</span> (assuming <span class="math notranslate nohighlight">\(x \neq 0\)</span>) to eliminate the time index <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[x^p - \phi_1 x^{p-1} - \phi_2 x^{p-2} - \dots - \phi_p = 0\]</div>
<p>This is one form of the characteristic equation. To get the form we mentioned before, we just need to divide this equation by <span class="math notranslate nohighlight">\(x^p\)</span>:</p>
<div class="math notranslate nohighlight">
\[1 - \phi_1 x^{-1} - \phi_2 x^{-2} - \dots - \phi_p x^{-p} = 0\]</div>
<p>and substitute <span class="math notranslate nohighlight">\(z = \frac{1}{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0\]</div>
<p>For mathematical convenience and consistency in time series literature, we use the polynomial in <span class="math notranslate nohighlight">\(L\)</span> and replace <span class="math notranslate nohighlight">\(L\)</span> with <span class="math notranslate nohighlight">\(z\)</span>.</p>
<p>In essence, replacing the lag operator <span class="math notranslate nohighlight">\(L\)</span> with a variable <span class="math notranslate nohighlight">\(z\)</span> transforms our time-domain problem into an algebraic problem. The variable <span class="math notranslate nohighlight">\(z\)</span> allows us to use the powerful tools of polynomial algebra to analyze the fundamental properties (like stability and stationarity) of our time series model, independent of any specific values in the series.</p>
<p>Now, let’s review the even more details. We’ve established the condition for stationarity: <strong>all roots of the characteristic equation <span class="math notranslate nohighlight">\(\Phi(z) = 0\)</span> must lie outside the unit circle</strong>. Let’s explore <em>why</em> this is the case, building our intuition from the AR(1) model up to the general AR(p) case.</p>
<p>The core idea is that for a series to be stationary, the impact of a random shock <span class="math notranslate nohighlight">\(\epsilon_t\)</span> from the distant past must eventually fade away. If a shock from a year ago still has a significant and non-diminishing impact on the series today, then the variance of the series would not be constant, and the process would be non-stationary.</p>
<section id="intuition-from-the-ar-1-model">
<h4>Intuition from the AR(1) Model<a class="headerlink" href="#intuition-from-the-ar-1-model" title="Link to this heading">#</a></h4>
<p>Recall our mean-centered AR(1) model: <span class="math notranslate nohighlight">\(y'_t = \phi_1 y'_{t-1} + \epsilon_t\)</span>. We can also write this using the lag polynomial: <span class="math notranslate nohighlight">\((1 - \phi_1 L) y'_t = \epsilon_t\)</span>.</p>
<p>Let’s formally “solve” for <span class="math notranslate nohighlight">\(y'_t\)</span> by dividing by the polynomial:</p>
<div class="math notranslate nohighlight">
\[y'_t = \frac{1}{1 - \phi_1 L} \epsilon_t = (1 - \phi_1 L)^{-1} \epsilon_t\]</div>
<p>Using the formula for a geometric series, where <span class="math notranslate nohighlight">\((1-x)^{-1} = 1 + x + x^2 + x^3 + \dots\)</span>, we can expand this expression:</p>
<div class="math notranslate nohighlight">
\[y'_t = (1 + \phi_1 L + \phi_1^2 L^2 + \phi_1^3 L^3 + \dots) \epsilon_t\]</div>
<p>Applying the lag operator to <span class="math notranslate nohighlight">\(\epsilon_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[y'_t = \epsilon_t + \phi_1 \epsilon_{t-1} + \phi_1^2 \epsilon_{t-2} + \phi_1^3 \epsilon_{t-3} + \dots\]</div>
<p>This is known as the <strong>moving average (MA) representation</strong> of the AR(1) process. It shows that the current value <span class="math notranslate nohighlight">\(y'_t\)</span> is a weighted sum of all past random shocks.</p>
<p>For this series to be stationary, the weights on these past shocks must diminish to zero as we go further back in time. The weights are <span class="math notranslate nohighlight">\(1, \phi_1, \phi_1^2, \phi_1^3, \dots\)</span>. This sequence converges to zero if and only if <span class="math notranslate nohighlight">\(|\phi_1| &lt; 1\)</span>. This is the same stationarity condition we derived earlier by calculating the variance.</p>
<p>Now, let’s connect this back to the characteristic equation:</p>
<ul class="simple">
<li><p>The equation is <span class="math notranslate nohighlight">\(1 - \phi_1 z = 0\)</span>.</p></li>
<li><p>The root is <span class="math notranslate nohighlight">\(z = 1 / \phi_1\)</span>.</p></li>
<li><p>If our stationarity condition <span class="math notranslate nohighlight">\(|\phi_1| &lt; 1\)</span> holds, then the absolute value of the root is <span class="math notranslate nohighlight">\(|z| = |1 / \phi_1| = 1 / |\phi_1|\)</span>.</p></li>
<li><p>Since <span class="math notranslate nohighlight">\(|\phi_1| &lt; 1\)</span>, it follows that <span class="math notranslate nohighlight">\(1 / |\phi_1| &gt; 1\)</span>.</p></li>
<li><p>Therefore, the condition <span class="math notranslate nohighlight">\(|\phi_1| &lt; 1\)</span> is mathematically equivalent to the condition that the root <span class="math notranslate nohighlight">\(z\)</span> lies outside the unit circle (<span class="math notranslate nohighlight">\(|z| &gt; 1\)</span>).</p></li>
</ul>
</section>
<section id="generalizing-to-the-ar-p-model">
<h4>Generalizing to the AR(p) Model<a class="headerlink" href="#generalizing-to-the-ar-p-model" title="Link to this heading">#</a></h4>
<p>This same logic extends to the AR(p) model. We start with the compact form:</p>
<div class="math notranslate nohighlight">
\[y'_t = \Phi(L)^{-1} \epsilon_t\]</div>
<p>To have a valid stationary process, we must be able to express <span class="math notranslate nohighlight">\(\Phi(L)^{-1}\)</span> as a convergent infinite series of lag operators, just as we did for the AR(1) case:</p>
<div class="math notranslate nohighlight">
\[\Phi(L)^{-1} = \sum_{j=0}^{\infty} \psi_j L^j \quad \text{where} \quad \sum_{j=0}^{\infty} |\psi_j| &lt; \infty\]</div>
<p>This ensures that the weights (<span class="math notranslate nohighlight">\(\psi_j\)</span>) on past shocks diminish over time. From the theory of polynomial algebra and complex analysis, we know that the power series expansion of a function like <span class="math notranslate nohighlight">\(1/\Phi(z)\)</span> converges if and only if the evaluation point (in our case, the unit circle and its interior) does not contain any roots of the denominator polynomial <span class="math notranslate nohighlight">\(\Phi(z)\)</span>.</p>
<p>Therefore, for the moving average MA(∞) representation to be well-behaved and for the process to be stationary, all roots of the characteristic polynomial <span class="math notranslate nohighlight">\(\Phi(z)=0\)</span> must lie <em>outside</em> the unit circle.</p>
<p><strong>What happens if a root is inside or on the unit circle?</strong></p>
<ul class="simple">
<li><p><strong>A root on the unit circle (<span class="math notranslate nohighlight">\(|z|=1\)</span>)</strong>: This is called a <strong>unit root</strong>. It means the effect of a past shock persists indefinitely and is never forgotten. The variance of the process becomes time-dependent and grows to infinity. This is a hallmark of non-stationary series like a random walk.</p></li>
<li><p><strong>A root inside the unit circle (<span class="math notranslate nohighlight">\(|z|&lt;1\)</span>)</strong>: This corresponds to an <strong>explosive</strong> process. The impact of a past shock is amplified over time, causing the series to diverge to infinity.</p></li>
</ul>
<p>In summary, the condition that all roots lie outside the unit circle is the precise mathematical requirement to ensure that the influence of past shocks eventually dies out, which is the fundamental property of a stationary time series.</p>
</section>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="identifying-key-parameters-unveiling-the-model-s-secrets">
<h1>Identifying Key Parameters: Unveiling the Model’s Secrets<a class="headerlink" href="#identifying-key-parameters-unveiling-the-model-s-secrets" title="Link to this heading">#</a></h1>
<p>Once we have a time series, the first challenge is to determine if an AR model is appropriate and, if so, what its order <span class="math notranslate nohighlight">\(p\)</span> is, and what the values of its coefficients (<span class="math notranslate nohighlight">\(\phi_i\)</span>) are.</p>
<section id="determining-the-order-p-autocorrelation-and-partial-autocorrelation-functions">
<h2>4.1 Determining the Order (p): Autocorrelation and Partial Autocorrelation Functions<a class="headerlink" href="#determining-the-order-p-autocorrelation-and-partial-autocorrelation-functions" title="Link to this heading">#</a></h2>
<p>The <strong>Autocorrelation Function (ACF)</strong> and <strong>Partial Autocorrelation Function (PACF)</strong> are indispensable tools for identifying the order of an AR model. They help us understand how much past values directly or indirectly influence current values.</p>
<section id="autocorrelation-function-acf">
<h3>Autocorrelation Function (ACF)<a class="headerlink" href="#autocorrelation-function-acf" title="Link to this heading">#</a></h3>
<p>The ACF measures the linear relationship between an observation at time <span class="math notranslate nohighlight">\(t\)</span> and an observation at a previous time <span class="math notranslate nohighlight">\(t-k\)</span>. It tells us the <em>total</em> correlation between <span class="math notranslate nohighlight">\(Y_t\)</span> and <span class="math notranslate nohighlight">\(Y_{t-k}\)</span>, including indirect effects.</p>
<p>For a stationary time series, the autocorrelation at lag <span class="math notranslate nohighlight">\(k\)</span>, denoted <span class="math notranslate nohighlight">\(\rho_k\)</span>, is defined as:</p>
<div class="math notranslate nohighlight">
\[ \rho_k = \frac{Cov(Y_t, Y_{t-k})}{\sqrt{Var(Y_t) Var(Y_{t-k})}} = \frac{\gamma_k}{\gamma_0} \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Cov(Y_t, Y_{t-k}) = \gamma_k\)</span> is the autocovariance between <span class="math notranslate nohighlight">\(Y_t\)</span> and <span class="math notranslate nohighlight">\(Y_{t-k}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(Var(Y_t) = \gamma_0\)</span> is the variance of the time series <span class="math notranslate nohighlight">\(Y_t\)</span>. Since we assume stationarity, <span class="math notranslate nohighlight">\(Var(Y_t) = Var(Y_{t-k})\)</span>.</p></li>
</ul>
<p><strong>Behavior for AR(p) models</strong>:
For an AR(p) process, the ACF will typically decay exponentially or with a dampened sine wave pattern. It does <em>not</em> cut off sharply; instead, the influence gradually diminishes over increasing lags. This decay pattern is characteristic of AR models.</p>
</section>
<section id="partial-autocorrelation-function-pacf">
<h3>Partial Autocorrelation Function (PACF)<a class="headerlink" href="#partial-autocorrelation-function-pacf" title="Link to this heading">#</a></h3>
<p>The PACF measures the <em>direct</em> linear relationship between <span class="math notranslate nohighlight">\(Y_t\)</span> and <span class="math notranslate nohighlight">\(Y_{t-k}\)</span>, <em>after removing the influence of the intermediate observations</em> <span class="math notranslate nohighlight">\(Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}\)</span>.</p>
<p>Think of it this way: Imagine a chain of dominoes. The ACF between the first and third domino falling would consider the total correlation, including the indirect effect through the second domino. The PACF, however, would try to measure the direct impact of the first domino on the third, <em>if the second domino’s effect was somehow factored out</em>. In time series, the PACF for <span class="math notranslate nohighlight">\(Y_t\)</span> and <span class="math notranslate nohighlight">\(Y_{t-k}\)</span> attempts to isolate the correlation that isn’t already explained by the linear combination of <span class="math notranslate nohighlight">\(Y_{t-1}, \dots, Y_{t-k+1}\)</span>.</p>
<p><strong>Behavior for AR(p) models</strong>:
For an AR(p) process, the PACF is the key diagnostic tool for identifying the order <span class="math notranslate nohighlight">\(p\)</span>. The PACF will <strong>cut off sharply after lag p</strong>. This means that the partial autocorrelations at lags greater than <span class="math notranslate nohighlight">\(p\)</span> will be close to zero and fall within the confidence bounds of statistical insignificance.</p>
<ul class="simple">
<li><p>For an AR(1) model, the PACF will have a significant spike at lag 1 and then cut off.</p></li>
<li><p>For an AR(2) model, the PACF will have significant spikes at lags 1 and 2, and then cut off.</p></li>
</ul>
</section>
<section id="information-criteria-aic-bic">
<h3>Information Criteria (AIC, BIC)<a class="headerlink" href="#information-criteria-aic-bic" title="Link to this heading">#</a></h3>
<p>While ACF and PACF plots are visual tools, information criteria provide a more quantitative approach to model selection.</p>
<ul class="simple">
<li><p><strong>Akaike Information Criterion (AIC)</strong></p></li>
<li><p><strong>Bayesian Information Criterion (BIC)</strong></p></li>
</ul>
<p>These criteria balance model fit with model complexity. They penalize models with more parameters to avoid overfitting. The model with the lowest AIC or BIC is generally preferred. We typically compute these values for AR models of different orders (<span class="math notranslate nohighlight">\(p=1, 2, 3, \dots\)</span>) and choose the <span class="math notranslate nohighlight">\(p\)</span> that minimizes the criterion.</p>
</section>
</section>
<section id="estimating-coefficients-phi-i-and-constant-c">
<h2>4.2 Estimating Coefficients (<span class="math notranslate nohighlight">\(\phi_i\)</span>) and Constant (c)<a class="headerlink" href="#estimating-coefficients-phi-i-and-constant-c" title="Link to this heading">#</a></h2>
<p>Once we have identified the likely order <span class="math notranslate nohighlight">\(p\)</span> of our AR model, the next step is to estimate the coefficients <span class="math notranslate nohighlight">\(\phi_i\)</span> and the constant <span class="math notranslate nohighlight">\(c\)</span>. Two common and effective methods for this are Ordinary Least Squares (OLS) and the Yule-Walker equations.</p>
<section id="derivation-of-ols-for-ar-1-model">
<h3>Derivation of OLS for AR(1) Model<a class="headerlink" href="#derivation-of-ols-for-ar-1-model" title="Link to this heading">#</a></h3>
<p>OLS works by finding the values of the coefficients that minimize the sum of the squared differences between the observed values (<span class="math notranslate nohighlight">\(Y_t\)</span>) and the values predicted by our model (<span class="math notranslate nohighlight">\(\hat{Y}_t\)</span>). These differences are the error terms <span class="math notranslate nohighlight">\(\epsilon_t\)</span>.</p>
<p>Let’s derive the OLS estimators for the simplest case, the AR(1) model.</p>
<p>Our AR(1) model is:</p>
<div class="math notranslate nohighlight">
\[Y_t = c + \phi_1 Y_{t-1} + \epsilon_t\]</div>
<p>We want to find <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(\phi_1\)</span> that minimize the sum of squared errors (SSE):</p>
<div class="math notranslate nohighlight">
\[ SSE = \sum_{t=2}^{N} \epsilon_t^2 = \sum_{t=2}^{N} (Y_t - c - \phi_1 Y_{t-1})^2 \]</div>
<p>Note that the sum starts from <span class="math notranslate nohighlight">\(t=2\)</span> because <span class="math notranslate nohighlight">\(Y_1\)</span> is the first observation for which <span class="math notranslate nohighlight">\(Y_{t-1}\)</span> (i.e., <span class="math notranslate nohighlight">\(Y_0\)</span>) is not available in our observed series. For large <span class="math notranslate nohighlight">\(N\)</span>, this starting point doesn’t significantly impact the estimation.</p>
<p>To find the minimum, we take the partial derivatives of SSE with respect to <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(\phi_1\)</span> and set them to zero.</p>
<p><strong>1. Partial derivative with respect to <span class="math notranslate nohighlight">\(c\)</span></strong>:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial SSE}{\partial c} = \frac{\partial}{\partial c} \sum_{t=2}^{N} (Y_t - c - \phi_1 Y_{t-1})^2 \]</div>
<div class="math notranslate nohighlight">
\[ = \sum_{t=2}^{N} 2(Y_t - c - \phi_1 Y_{t-1})(-1) \]</div>
<p>Setting this to zero:</p>
<div class="math notranslate nohighlight">
\[ \sum_{t=2}^{N} (Y_t - c - \phi_1 Y_{t-1}) = 0 \]</div>
<div class="math notranslate nohighlight">
\[ \sum_{t=2}^{N} Y_t - \sum_{t=2}^{N} c - \sum_{t=2}^{N} \phi_1 Y_{t-1} = 0 \]</div>
<p>Let <span class="math notranslate nohighlight">\(N' = N-1\)</span> be the number of observations used in the regression.</p>
<div class="math notranslate nohighlight">
\[ \sum_{t=2}^{N} Y_t - N'c - \phi_1 \sum_{t=2}^{N} Y_{t-1} = 0 \]</div>
<p>This gives us the first “normal equation”:</p>
<div class="math notranslate nohighlight">
\[ N'c = \sum_{t=2}^{N} Y_t - \phi_1 \sum_{t=2}^{N} Y_{t-1} \quad (1) \]</div>
<p><strong>2. Partial derivative with respect to <span class="math notranslate nohighlight">\(\phi_1\)</span></strong>:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial SSE}{\partial \phi_1} = \frac{\partial}{\partial \phi_1} \sum_{t=2}^{N} (Y_t - c - \phi_1 Y_{t-1})^2 \]</div>
<div class="math notranslate nohighlight">
\[ = \sum_{t=2}^{N} 2(Y_t - c - \phi_1 Y_{t-1})(-Y_{t-1}) \]</div>
<p>Setting this to zero:</p>
<div class="math notranslate nohighlight">
\[ \sum_{t=2}^{N} Y_{t-1}(Y_t - c - \phi_1 Y_{t-1}) = 0 \]</div>
<div class="math notranslate nohighlight">
\[ \sum_{t=2}^{N} Y_{t-1}Y_t - \sum_{t=2}^{N} c Y_{t-1} - \sum_{t=2}^{N} \phi_1 Y_{t-1}^2 = 0 \]</div>
<div class="math notranslate nohighlight">
\[ \sum_{t=2}^{N} Y_{t-1}Y_t - c \sum_{t=2}^{N} Y_{t-1} - \phi_1 \sum_{t=2}^{N} Y_{t-1}^2 = 0 \]</div>
<p>This gives us the second “normal equation”:</p>
<div class="math notranslate nohighlight">
\[ \sum_{t=2}^{N} Y_{t-1}Y_t = c \sum_{t=2}^{N} Y_{t-1} + \phi_1 \sum_{t=2}^{N} Y_{t-1}^2 \quad (2) \]</div>
<p>Now we have a system of two linear equations with two unknowns (<span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(\phi_1\)</span>):</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(N'c + \phi_1 \sum_{t=2}^{N} Y_{t-1} = \sum_{t=2}^{N} Y_t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(c \sum_{t=2}^{N} Y_{t-1} + \phi_1 \sum_{t=2}^{N} Y_{t-1}^2 = \sum_{t=2}^{N} Y_{t-1}Y_t\)</span></p></li>
</ol>
<p>Solving this system for <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(\phi_1\)</span> will give us the OLS estimators.</p>
<p>From equation (1), we can express <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="math notranslate nohighlight">
\[ c = \frac{1}{N'} \left( \sum_{t=2}^{N} Y_t - \phi_1 \sum_{t=2}^{N} Y_{t-1} \right) = \bar{Y}_t - \phi_1 \bar{Y}_{t-1} \]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{Y}_t = \frac{1}{N'} \sum_{t=2}^{N} Y_t\)</span> and <span class="math notranslate nohighlight">\(\bar{Y}_{t-1} = \frac{1}{N'} \sum_{t=2}^{N} Y_{t-1}\)</span> are the sample means of <span class="math notranslate nohighlight">\(Y_t\)</span> and <span class="math notranslate nohighlight">\(Y_{t-1}\)</span> respectively over the regression period.</p>
<p>Substitute this expression for <span class="math notranslate nohighlight">\(c\)</span> into equation (2):</p>
<div class="math notranslate nohighlight">
\[ (\bar{Y}_t - \phi_1 \bar{Y}_{t-1}) \sum_{t=2}^{N} Y_{t-1} + \phi_1 \sum_{t=2}^{N} Y_{t-1}^2 = \sum_{t=2}^{N} Y_{t-1}Y_t \]</div>
<div class="math notranslate nohighlight">
\[ \bar{Y}_t \sum_{t=2}^{N} Y_{t-1} - \phi_1 \bar{Y}_{t-1} \sum_{t=2}^{N} Y_{t-1} + \phi_1 \sum_{t=2}^{N} Y_{t-1}^2 = \sum_{t=2}^{N} Y_{t-1}Y_t \]</div>
<p>Rearrange to solve for <span class="math notranslate nohighlight">\(\phi_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \phi_1 \left( \sum_{t=2}^{N} Y_{t-1}^2 - \bar{Y}_{t-1} \sum_{t=2}^{N} Y_{t-1} \right) = \sum_{t=2}^{N} Y_{t-1}Y_t - \bar{Y}_t \sum_{t=2}^{N} Y_{t-1} \]</div>
<p>We know that <span class="math notranslate nohighlight">\(\sum_{t=2}^{N} Y_{t-1} = N' \bar{Y}_{t-1}\)</span>. So:</p>
<div class="math notranslate nohighlight">
\[ \phi_1 \left( \sum_{t=2}^{N} Y_{t-1}^2 - N' \bar{Y}_{t-1}^2 \right) = \sum_{t=2}^{N} Y_{t-1}Y_t - N' \bar{Y}_t \bar{Y}_{t-1} \]</div>
<p>The terms in the parentheses and on the right side are related to sample variances and covariances. Specifically:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sum_{t=2}^{N} (Y_{t-1} - \bar{Y}_{t-1})^2 = \sum Y_{t-1}^2 - N' \bar{Y}_{t-1}^2\)</span> (sample sum of squares for <span class="math notranslate nohighlight">\(Y_{t-1}\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=2}^{N} (Y_t - \bar{Y}_t)(Y_{t-1} - \bar{Y}_{t-1}) = \sum Y_t Y_{t-1} - N' \bar{Y}_t \bar{Y}_{t-1}\)</span> (sample sum of cross-products)</p></li>
</ul>
<p>Thus, the OLS estimator for <span class="math notranslate nohighlight">\(\phi_1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[ \hat{\phi}_1 = \frac{\sum_{t=2}^{N} (Y_t - \bar{Y}_t)(Y_{t-1} - \bar{Y}_{t-1})}{\sum_{t=2}^{N} (Y_{t-1} - \bar{Y}_{t-1})^2} \]</div>
<p>This is precisely the formula for the sample correlation coefficient multiplied by the ratio of standard deviations, or more simply, the ratio of the sample autocovariance at lag 1 to the sample variance.</p>
<p>Once <span class="math notranslate nohighlight">\(\hat{\phi}_1\)</span> is found, we can substitute it back into the equation for <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \hat{c} = \bar{Y}_t - \hat{\phi}_1 \bar{Y}_{t-1} \]</div>
<p>These are our OLS estimators for the AR(1) model.</p>
</section>
<section id="generalization-of-ols-to-ar-p">
<h3>Generalization of OLS to AR(p)<a class="headerlink" href="#generalization-of-ols-to-ar-p" title="Link to this heading">#</a></h3>
<p>For a general AR(p) model, the OLS method extends naturally. We would minimize the sum of squared errors:</p>
<div class="math notranslate nohighlight">
\[ SSE = \sum_{t=p+1}^{N} (Y_t - c - \sum_{i=1}^{p} \phi_i Y_{t-i})^2 \]</div>
<p>This involves taking <span class="math notranslate nohighlight">\(p+1\)</span> partial derivatives (one for <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(p\)</span> for <span class="math notranslate nohighlight">\(\phi_i\)</span>) and setting them to zero. This leads to a system of <span class="math notranslate nohighlight">\(p+1\)</span> linear equations (the normal equations) that can be solved using matrix algebra. The principle remains the same: find the coefficients that best fit the data in a least squares sense.</p>
</section>
<section id="yule-walker-equations">
<h3>Yule-Walker Equations<a class="headerlink" href="#yule-walker-equations" title="Link to this heading">#</a></h3>
<p>The Yule-Walker equations provide another way to estimate the AR coefficients by relating them to the autocovariances (or autocorrelations) of the time series. These equations are derived by multiplying the AR(<span class="math notranslate nohighlight">\(p\)</span>) equation by <span class="math notranslate nohighlight">\(Y_{t-k}\)</span> for different lags <span class="math notranslate nohighlight">\(k\)</span> and then taking expectations.</p>
<p>Let’s derive them for an AR(1) model, assuming the series is stationary and has mean <span class="math notranslate nohighlight">\(\mu=0\)</span> (we can always center the series by subtracting its mean if <span class="math notranslate nohighlight">\(c \neq 0\)</span>). So, <span class="math notranslate nohighlight">\(Y_t = \phi_1 Y_{t-1} + \epsilon_t\)</span>.</p>
<p>Multiply by <span class="math notranslate nohighlight">\(Y_{t-k}\)</span> for <span class="math notranslate nohighlight">\(k \ge 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[Y_t Y_{t-k} = \phi_1 Y_{t-1} Y_{t-k} + \epsilon_t Y_{t-k}\]</div>
<p>Now, take the expectation of both sides:</p>
<div class="math notranslate nohighlight">
\[E[Y_t Y_{t-k}] = E[\phi_1 Y_{t-1} Y_{t-k}] + E[\epsilon_t Y_{t-k}]\]</div>
<p>Since <span class="math notranslate nohighlight">\(E[Y_t Y_{t-k}] = \gamma_k\)</span> (the autocovariance at lag <span class="math notranslate nohighlight">\(k\)</span>) and <span class="math notranslate nohighlight">\(E[\epsilon_t Y_{t-k}] = 0\)</span> for <span class="math notranslate nohighlight">\(k \ge 1\)</span> (because <span class="math notranslate nohighlight">\(\epsilon_t\)</span> is uncorrelated with past values of <span class="math notranslate nohighlight">\(Y\)</span>), we get:</p>
<div class="math notranslate nohighlight">
\[\gamma_k = \phi_1 \gamma_{k-1}\]</div>
<p>For <span class="math notranslate nohighlight">\(k=1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\gamma_1 = \phi_1 \gamma_0\]</div>
<p>Recall that <span class="math notranslate nohighlight">\(\gamma_0 = Var(Y_t) = \sigma_Y^2\)</span>.
We can divide by <span class="math notranslate nohighlight">\(\gamma_0\)</span> to express this in terms of autocorrelations <span class="math notranslate nohighlight">\(\rho_k = \frac{\gamma_k}{\gamma_0}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\rho_1 = \phi_1 \rho_0\]</div>
<p>Since <span class="math notranslate nohighlight">\(\rho_0 = 1\)</span> (correlation of a series with itself at lag 0 is 1):</p>
<div class="math notranslate nohighlight">
\[\rho_1 = \phi_1\]</div>
<p>This gives us a direct way to estimate <span class="math notranslate nohighlight">\(\phi_1\)</span> for an AR(1) model: it’s simply the autocorrelation at lag 1.</p>
<p>For a general AR(<span class="math notranslate nohighlight">\(p\)</span>) model, the Yule-Walker equations form a system of linear equations:</p>
<div class="math notranslate nohighlight">
\[\rho_k = \phi_1 \rho_{k-1} + \phi_2 \rho_{k-2} + \dots + \phi_p \rho_{k-p} \quad \text{for } k=1, 2, \dots, p\]</div>
<p>In matrix form, for <span class="math notranslate nohighlight">\(p\)</span> equations:</p>
<div class="highlight-math notranslate"><div class="highlight"><pre><span></span>\begin{pmatrix}
\rho_0 &amp; \rho_1 &amp; \dots &amp; \rho_{p-1} \\
\rho_1 &amp; \rho_0 &amp; \dots &amp; \rho_{p-2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\rho_{p-1} &amp; \rho_{p-2} &amp; \dots &amp; \rho_0
\end{pmatrix}
\begin{pmatrix}
\phi_1 \\
\phi_2 \\
\vdots \\
\phi_p
\end{pmatrix}
=
\begin{pmatrix}
\rho_1 \\
\rho_2 \\
\vdots \\
\rho_p
\end{pmatrix}
</pre></div>
</div>
<p>By replacing the theoretical autocorrelations <span class="math notranslate nohighlight">\(\rho_k\)</span> with their sample estimates <span class="math notranslate nohighlight">\(\hat{\rho}_k\)</span> (calculated from the observed data), we can solve this system of equations to get estimates for <span class="math notranslate nohighlight">\(\hat{\phi}_1, \dots, \hat{\phi}_p\)</span>.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="putting-theory-into-practice-python-with-statsmodels">
<h1>5. Putting Theory into Practice: Python with <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code><a class="headerlink" href="#putting-theory-into-practice-python-with-statsmodels" title="Link to this heading">#</a></h1>
<p>Now that we have a solid theoretical foundation, let’s see how we can apply these concepts using Python and the powerful <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> library. We will generate a synthetic AR(1) time series, test its stationarity, analyze its ACF and PACF, and then fit an AR model to it.</p>
<p>First, we need to import the necessary libraries:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.graphics.tsaplots</span><span class="w"> </span><span class="kn">import</span> <span class="n">plot_acf</span><span class="p">,</span> <span class="n">plot_pacf</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.tsa.ar_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoReg</span> <span class="c1"># Recommended for pure AR models</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.tsa.stattools</span><span class="w"> </span><span class="kn">import</span> <span class="n">adfuller</span> <span class="c1"># For stationarity testing</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="c1"># Set a random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<section id="generating-synthetic-ar-1-data">
<h2>5.1 Generating Synthetic AR(1) Data<a class="headerlink" href="#generating-synthetic-ar-1-data" title="Link to this heading">#</a></h2>
<p>Let’s create an AR(1) process with known parameters. We will use the model:
<span class="math notranslate nohighlight">\(Y_t = 0.5 + 0.7 Y_{t-1} + \epsilon_t\)</span>
Here, <span class="math notranslate nohighlight">\(c = 0.5\)</span> and <span class="math notranslate nohighlight">\(\phi_1 = 0.7\)</span>. Since <span class="math notranslate nohighlight">\(|\phi_1| = 0.7 &lt; 1\)</span>, this series will be stationary. The error term <span class="math notranslate nohighlight">\(\epsilon_t\)</span> will be standard normal white noise, meaning <span class="math notranslate nohighlight">\(E[\epsilon_t]=0\)</span> and <span class="math notranslate nohighlight">\(Var(\epsilon_t)=1\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define AR coefficients and constant</span>
<span class="n">ar_coeffs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">]</span> <span class="c1"># AR(1) coefficient</span>
<span class="n">constant_c</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># Define number of observations</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Generate white noise errors with mean 0 and standard deviation 1</span>
<span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="c1"># Initialize the time series</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Start with a random initial value</span>

<span class="c1"># Generate the AR(1) series using the defined equation</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
    <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">constant_c</span> <span class="o">+</span> <span class="n">ar_coeffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">errors</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

<span class="c1"># Convert to a pandas Series for easier handling and plotting</span>
<span class="n">ar1_series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Plot the generated series</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Generated AR(1) Time Series&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="testing-for-stationarity-with-augmented-dickey-fuller-adf-test">
<h2>5.2 Testing for Stationarity with Augmented Dickey-Fuller (ADF) Test<a class="headerlink" href="#testing-for-stationarity-with-augmented-dickey-fuller-adf-test" title="Link to this heading">#</a></h2>
<p>Before proceeding with AR modeling, especially with real-world data, it’s crucial to verify stationarity. The Augmented Dickey-Fuller (ADF) test is a popular statistical test for this purpose.</p>
<p>The null hypothesis (<span class="math notranslate nohighlight">\(H_0\)</span>) of the ADF test is that the time series has a unit root (i.e., it is non-stationary). The alternative hypothesis (<span class="math notranslate nohighlight">\(H_1\)</span>) is that the time series is stationary. We typically look for a low p-value (e.g., less than 0.05) to reject the null hypothesis and conclude that the series is stationary.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform ADF test on the generated series</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- Augmented Dickey-Fuller Test ---&quot;</span><span class="p">)</span>
<span class="n">adf_result</span> <span class="o">=</span> <span class="n">adfuller</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ADF Statistic: </span><span class="si">{</span><span class="n">adf_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P-value: </span><span class="si">{</span><span class="n">adf_result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Critical Values:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">adf_result</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">adf_result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">0.05</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Conclusion: The series is likely stationary (reject H0).&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Conclusion: The series is likely non-stationary (fail to reject H0).&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The output should show a small p-value, indicating that our simulated series is indeed stationary, as expected for <span class="math notranslate nohighlight">\(|\phi_1| &lt; 1\)</span>.</p>
</section>
<section id="visualizing-acf-and-pacf">
<h2>5.3 Visualizing ACF and PACF<a class="headerlink" href="#visualizing-acf-and-pacf" title="Link to this heading">#</a></h2>
<p>Now, let’s plot the ACF and PACF of our generated AR(1) series. We expect the ACF to decay gradually and the PACF to cut off after lag 1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot ACF and PACF</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plot_acf</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Autocorrelation Function (ACF)&#39;</span><span class="p">)</span>
<span class="n">plot_pacf</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Partial Autocorrelation Function (PACF)&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;ywm&#39;</span><span class="p">)</span> <span class="c1"># &#39;ywm&#39; for Yule-Walker method</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Interpretation of the plots</strong>:</p>
<ul class="simple">
<li><p>The <strong>ACF plot</strong> on the left shows a gradual, exponential decay. This is characteristic of an AR process, where the correlation with past values slowly diminishes over time.</p></li>
<li><p>The <strong>PACF plot</strong> on the right shows a significant spike at lag 1, and then the values at subsequent lags quickly drop to near zero, falling within the blue shaded confidence interval. This “cutoff” at lag 1 strongly suggests that our series is an AR(1) process. If it were an AR(p) process, we would see <span class="math notranslate nohighlight">\(p\)</span> significant spikes before the cutoff.</p></li>
</ul>
</section>
<section id="fitting-the-ar-model">
<h2>5.4 Fitting the AR Model<a class="headerlink" href="#fitting-the-ar-model" title="Link to this heading">#</a></h2>
<p>Based on our PACF analysis, we will fit an AR(1) model to our synthetic data using <code class="docutils literal notranslate"><span class="pre">statsmodels.tsa.ar_model.AutoReg</span></code>. The <code class="docutils literal notranslate"><span class="pre">lags</span></code> argument specifies the order <span class="math notranslate nohighlight">\(p\)</span>, and <code class="docutils literal notranslate"><span class="pre">trend='c'</span></code> tells the model to include a constant term (intercept).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit an AR(1) model</span>
<span class="c1"># The lags argument specifies the order p</span>
<span class="c1"># trend=&#39;c&#39; indicates inclusion of a constant (intercept)</span>
<span class="n">ar_model</span> <span class="o">=</span> <span class="n">AutoReg</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">trend</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>
<span class="n">ar_results</span> <span class="o">=</span> <span class="n">ar_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the model summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ar_results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>Interpreting Model Output</strong>:</p>
<p>Let’s look at the key parts of the <code class="docutils literal notranslate"><span class="pre">results.summary()</span></code> output. Note that the <code class="docutils literal notranslate"><span class="pre">const</span></code> term in <code class="docutils literal notranslate"><span class="pre">AutoReg</span></code> when <code class="docutils literal notranslate"><span class="pre">trend='c'</span></code> for a stationary series estimates the <em>mean</em> of the series (<span class="math notranslate nohighlight">\(\mu\)</span>), not the <span class="math notranslate nohighlight">\(c\)</span> directly from our original equation. The relationship is <span class="math notranslate nohighlight">\(\mu = c / (1 - \phi_1)\)</span>. Our true mean for the generated series is <span class="math notranslate nohighlight">\(0.5 / (1 - 0.7) = 0.5 / 0.3 \approx 1.6667\)</span>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>                               <span class="n">AutoReg</span> <span class="n">Results</span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span> <span class="n">Variable</span><span class="p">:</span>                      <span class="n">y</span>   <span class="n">No</span><span class="o">.</span> <span class="n">Observations</span><span class="p">:</span>                  <span class="mi">200</span>
<span class="n">Model</span><span class="p">:</span>                          <span class="n">AutoReg</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="n">Log</span> <span class="n">Likelihood</span>                <span class="o">-</span><span class="mf">281.424</span>
<span class="n">Date</span><span class="p">:</span>                <span class="o">...</span>              <span class="n">AIC</span>                            <span class="mf">568.848</span>
<span class="n">Time</span><span class="p">:</span>                <span class="o">...</span>              <span class="n">BIC</span>                            <span class="mf">578.740</span>
<span class="n">Sample</span><span class="p">:</span>                             <span class="mi">0</span>   <span class="n">HQIC</span>                           <span class="mf">572.822</span>
                                <span class="o">-</span> <span class="mi">200</span>
<span class="n">Covariance</span> <span class="n">Type</span><span class="p">:</span>                  <span class="n">opg</span>
<span class="o">==============================================================================</span>
                 <span class="n">coef</span>    <span class="n">std</span> <span class="n">err</span>          <span class="n">z</span>      <span class="n">P</span><span class="o">&gt;|</span><span class="n">z</span><span class="o">|</span>      <span class="p">[</span><span class="mf">0.025</span>      <span class="mf">0.975</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="n">const</span>          <span class="mf">1.6730</span>    <span class="mf">0.237</span>          <span class="mf">7.069</span>      <span class="mf">0.000</span>       <span class="mf">1.209</span>       <span class="mf">2.137</span>
<span class="n">y</span><span class="o">.</span><span class="n">L1</span>           <span class="mf">0.7025</span>    <span class="mf">0.045</span>         <span class="mf">15.611</span>      <span class="mf">0.000</span>       <span class="mf">0.614</span>       <span class="mf">0.791</span>
<span class="n">sigma2</span>         <span class="mf">0.9634</span>    <span class="mf">0.096</span>         <span class="mf">10.038</span>      <span class="mf">0.000</span>       <span class="mf">0.775</span>       <span class="mf">1.152</span>
<span class="o">===================================================================================</span>
<span class="n">Ljung</span><span class="o">-</span><span class="n">Box</span> <span class="p">(</span><span class="n">L1</span><span class="p">)</span> <span class="p">(</span><span class="n">Q</span><span class="p">):</span>                   <span class="mf">0.00</span>   <span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span> <span class="p">(</span><span class="n">JB</span><span class="p">):</span>                 <span class="mf">0.42</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Q</span><span class="p">):</span>                              <span class="mf">0.99</span>   <span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span>                         <span class="mf">0.81</span>
<span class="n">Heteroskedasticity</span> <span class="p">(</span><span class="n">H</span><span class="p">):</span>               <span class="mf">0.98</span>   <span class="n">Skew</span><span class="p">:</span>                            <span class="o">-</span><span class="mf">0.03</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">H</span><span class="p">)</span> <span class="p">(</span><span class="n">two</span><span class="o">-</span><span class="n">sided</span><span class="p">):</span>                  <span class="mf">0.94</span>   <span class="n">Kurtosis</span><span class="p">:</span>                         <span class="mf">2.86</span>
<span class="o">===================================================================================</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">const</span></code></strong>: This is our estimated mean <span class="math notranslate nohighlight">\(\mu\)</span> of the stationary series. The estimated value is <code class="docutils literal notranslate"><span class="pre">1.6730</span></code>, which is very close to our true mean of approximately <code class="docutils literal notranslate"><span class="pre">1.6667</span></code>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">y.L1</span></code></strong>: This is our estimated <span class="math notranslate nohighlight">\(\phi_1\)</span> coefficient for the first lag. The estimated value is <code class="docutils literal notranslate"><span class="pre">0.7025</span></code>, which is also remarkably close to our true value of <code class="docutils literal notranslate"><span class="pre">0.7</span></code>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">err</span></code></strong>: The standard error of the coefficients. A small standard error indicates a more precise estimate.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">z</span></code></strong>: The z-statistic, which is the coefficient divided by its standard error.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">P&gt;|z|</span></code></strong>: The p-value associated with the z-statistic. A p-value less than 0.05 (or your chosen significance level) indicates that the coefficient is statistically significant, meaning it’s unlikely to be zero. Both our <code class="docutils literal notranslate"><span class="pre">const</span></code> and <code class="docutils literal notranslate"><span class="pre">y.L1</span></code> coefficients have p-values close to <code class="docutils literal notranslate"><span class="pre">0.000</span></code>, confirming their significance.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">sigma2</span></code></strong>: This is the estimated variance of the error term <span class="math notranslate nohighlight">\(\epsilon_t\)</span>. Our true error variance was <span class="math notranslate nohighlight">\(1^2 = 1\)</span>, and the estimated value <code class="docutils literal notranslate"><span class="pre">0.9634</span></code> is close to that.</p></li>
<li><p><strong>Diagnostic Tests</strong>: The <code class="docutils literal notranslate"><span class="pre">Ljung-Box</span> <span class="pre">(Q)</span></code> test checks for remaining autocorrelation in the residuals (errors). A high p-value (like <code class="docutils literal notranslate"><span class="pre">0.99</span></code>) indicates that there is no significant autocorrelation left in the residuals, which is good - it means our model has captured the AR structure. The <code class="docutils literal notranslate"><span class="pre">Jarque-Bera</span> <span class="pre">(JB)</span></code> test checks for normality of residuals. A high p-value (like <code class="docutils literal notranslate"><span class="pre">0.81</span></code>) suggests that the residuals are normally distributed, consistent with our assumption.</p></li>
</ul>
<p>These results demonstrate that the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> library successfully identified and estimated the parameters of our synthetic AR(1) process with high accuracy.</p>
</section>
<section id="forecasting">
<h2>5.5 Forecasting<a class="headerlink" href="#forecasting" title="Link to this heading">#</a></h2>
<p>Once an AR model is fitted, we can use it to forecast future values of the time series. We can make in-sample predictions (within the observed data) to see how well the model fits, and out-of-sample forecasts (into the future).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make in-sample predictions</span>
<span class="c1"># start=ar_model.lags[0] ensures we start prediction from the first point where all lags are available</span>
<span class="n">predictions_in_sample</span> <span class="o">=</span> <span class="n">ar_results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">ar_model</span><span class="o">.</span><span class="n">lags</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Plot actual vs. predicted values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Actual Values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predictions_in_sample</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;In-sample Predicted Values&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;AR(1) Model: Actual vs. In-sample Predicted&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># To make out-of-sample predictions (forecast into the future)</span>
<span class="c1"># Let&#39;s forecast 10 steps ahead</span>
<span class="n">forecast_steps</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">forecast_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">)</span> <span class="o">+</span> <span class="n">forecast_steps</span><span class="p">)</span>
<span class="n">forecast</span> <span class="o">=</span> <span class="n">ar_results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">)</span> <span class="o">+</span> <span class="n">forecast_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Forecasted values for the next 10 steps:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">forecast</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">forecast_index</span><span class="p">))</span> <span class="c1"># Print with appropriate index</span>

<span class="c1"># Plot the original series with the forecast</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ar1_series</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original Series&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">forecast_index</span><span class="p">,</span> <span class="n">forecast</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Forecasted Values&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;AR(1) Model: Original Series and Forecast&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Notice how the forecast converges to the series mean (which is approximately <span class="math notranslate nohighlight">\(1.6667\)</span> in our case) as the forecast horizon increases. This behavior is typical for stationary AR models, as the influence of the last observed value <span class="math notranslate nohighlight">\(Y_t\)</span> diminishes over time due to <span class="math notranslate nohighlight">\(|\phi_1| &lt; 1\)</span>.</p>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="conclusion-the-power-of-self-referential-modeling">
<h1>6. Conclusion: The Power of Self-Referential Modeling<a class="headerlink" href="#conclusion-the-power-of-self-referential-modeling" title="Link to this heading">#</a></h1>
<p>We have thoroughly explored the concept of modeling a time series based on its own past values, focusing specifically on the Autoregressive (AR) model. We started by understanding the intuition behind using past values as predictors and then formally defined the AR(1) and general AR(p) models.</p>
<p>We delved into the crucial mathematical assumptions of stationarity and white noise errors, understanding why they are essential for valid model estimation and inference. We meticulously derived the stationarity condition for the AR(1) model (both mean and variance) and explained the characteristic polynomial approach for the general AR(p) model from scratch. We then learned how to identify the order <span class="math notranslate nohighlight">\(p\)</span> of an AR model using the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF), highlighting the distinct “cutoff” property of PACF for AR models.</p>
<p>Crucially, we derived the Ordinary Least Squares (OLS) estimators for the AR(1) model from scratch, demonstrating the underlying mathematical principles used to find the best-fitting coefficients. We also explored the Yule-Walker equations as an alternative estimation method. Finally, we brought these theoretical concepts to life with practical Python code using the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> library, generating synthetic data, performing a stationarity test, analyzing its ACF/PACF, fitting an AR model with <code class="docutils literal notranslate"><span class="pre">AutoReg</span></code>, and interpreting the results and making predictions.</p>
<p>The AR model, while seemingly simple, provides a powerful framework for understanding and forecasting time series with inherent temporal dependencies. It forms the bedrock for more complex models like ARMA (Autoregressive Moving Average) and ARIMA (Autoregressive Integrated Moving Average), which we might explore in future discussions. By mastering the AR model, we’ve gained a fundamental tool in our time series analysis toolkit.</p>
<section id="additional-materials">
<h2>Additional materials<a class="headerlink" href="#additional-materials" title="Link to this heading">#</a></h2>
<section id="linear-difference-equations">
<h3>Linear difference equations<a class="headerlink" href="#linear-difference-equations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://mpaldridge.github.io/math2750/S04-ldes.html">https://mpaldridge.github.io/math2750/S04-ldes.html</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=_IDS9VVb8DY">https://www.youtube.com/watch?v=_IDS9VVb8DY</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=kFOs-MKvnco">https://www.youtube.com/watch?v=kFOs-MKvnco</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=cj-kMkTEX-I">https://www.youtube.com/watch?v=cj-kMkTEX-I</a></p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./data-analysis"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Autoregressive (AR) Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ar-1-model-our-starting-point">The AR(1) Model: Our Starting Point</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-ar-p-model">The General AR(p) Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underlying-mathematical-assumptions-the-foundation-of-ar-models">Underlying Mathematical Assumptions: The Foundation of AR Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stationarity-of-the-time-series-y-t">Stationarity of the Time Series (<span class="math notranslate nohighlight">\(Y_t\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#white-noise-error-term-epsilon-t">White Noise Error Term (<span class="math notranslate nohighlight">\(\epsilon_t\)</span>)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-stationarity-condition-for-ar-1">Derivation of Stationarity Condition for AR(1)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#condition-for-stationarity-in-ar-p-the-characteristic-polynomial">Condition for Stationarity in AR(p): The Characteristic Polynomial</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-notes-about-characteristic-polynomial">Additional Notes about Characteristic Polynomial</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-from-the-ar-1-model">Intuition from the AR(1) Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-to-the-ar-p-model">Generalizing to the AR(p) Model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#identifying-key-parameters-unveiling-the-model-s-secrets">Identifying Key Parameters: Unveiling the Model’s Secrets</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determining-the-order-p-autocorrelation-and-partial-autocorrelation-functions">4.1 Determining the Order (p): Autocorrelation and Partial Autocorrelation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autocorrelation-function-acf">Autocorrelation Function (ACF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-autocorrelation-function-pacf">Partial Autocorrelation Function (PACF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-criteria-aic-bic">Information Criteria (AIC, BIC)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-coefficients-phi-i-and-constant-c">4.2 Estimating Coefficients (<span class="math notranslate nohighlight">\(\phi_i\)</span>) and Constant (c)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-ols-for-ar-1-model">Derivation of OLS for AR(1) Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-of-ols-to-ar-p">Generalization of OLS to AR(p)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#yule-walker-equations">Yule-Walker Equations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-theory-into-practice-python-with-statsmodels">5. Putting Theory into Practice: Python with <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-synthetic-ar-1-data">5.1 Generating Synthetic AR(1) Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-for-stationarity-with-augmented-dickey-fuller-adf-test">5.2 Testing for Stationarity with Augmented Dickey-Fuller (ADF) Test</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-acf-and-pacf">5.3 Visualizing ACF and PACF</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-ar-model">5.4 Fitting the AR Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forecasting">5.5 Forecasting</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-the-power-of-self-referential-modeling">6. Conclusion: The Power of Self-Referential Modeling</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional materials</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-difference-equations">Linear difference equations</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Vladyslav Yakovliev (Ukraine)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>