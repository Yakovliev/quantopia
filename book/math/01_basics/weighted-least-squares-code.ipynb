{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57aca5d",
   "metadata": {},
   "source": [
    "# Weighted Least Squares - Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb865b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9afdc737",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What are `sigma` and `absolute_sigma` parameters in `scipy.optimize.curve_fit`?\n",
    "\n",
    "1.  **`sigma`**:\n",
    "    * This parameter takes an `M`-length sequence (where `M` is the number of data points) or an `MxM` array.\n",
    "    * If it's an `M`-length sequence (which is what we used), it represents the **standard deviations** of the `ydata` points. So, `sigma[i]` is the estimated standard deviation of `ydata[i]`.\n",
    "    * `curve_fit` then uses these standard deviations to perform a weighted least squares minimization. The weights applied are `w_i = 1 / sigma[i]**2`.\n",
    "    * If `sigma` is not provided, `curve_fit` assumes uniform errors (homoscedasticity), equivalent to all `sigma[i]` being 1, which defaults to Ordinary Least Squares (OLS).\n",
    "\n",
    "2.  **`absolute_sigma`**:\n",
    "    * This is a boolean parameter, defaulting to `False`.\n",
    "    * **`absolute_sigma = False` (Default):** This is the more common scenario for relative weighting. When `sigma` is provided and `absolute_sigma` is `False`, `curve_fit` scales the covariance matrix (`pcov`) by `chisq / (M - N)`, where `chisq` is the reduced chi-squared value (sum of squared weighted residuals), `M` is the number of data points, and `N` is the number of parameters. This scaling effectively assumes that the provided `sigma` values are *relative* estimates of the uncertainties, and the overall scaling factor is determined by the goodness of fit to the data. This means that if your model fits the data very well (small reduced chi-squared), the parameter uncertainties in `pcov` will be smaller, and vice-versa.\n",
    "    * **`absolute_sigma = True`:** When `sigma` is provided and `absolute_sigma` is `True`, `curve_fit` treats the provided `sigma` values as **absolute standard deviations** of the `ydata`. In this case, no scaling of the covariance matrix (`pcov`) is performed based on the goodness of fit. The `pcov` directly reflects the uncertainties implied by the provided `sigma` values. This is appropriate when you have precise knowledge of the absolute errors in your measurements. This is generally what you want when your `sigma` values come from known measurement uncertainties (like your $\\pm 0.5$ error).\n",
    "\n",
    "In your updated code, using `sigma=sigma, absolute_sigma=True` is the correct approach because you have known, absolute uncertainties for your Y values.\n",
    "\n",
    "### What algorithm is running if we use `sigma` parameters?\n",
    "\n",
    "Regardless of whether you provide `sigma` or not, `scipy.optimize.curve_fit` internally uses a **Levenberg-Marquardt algorithm** by default to find the parameters that minimize the sum of squared residuals.\n",
    "\n",
    "The key difference when `sigma` is provided is *what* the Levenberg-Marquardt algorithm minimizes:\n",
    "\n",
    "* **Without `sigma` (OLS):** It minimizes the ordinary sum of squared residuals: $\\sum (y_i - f(x_i, \\beta))^2$.\n",
    "* **With `sigma` (WLS):** It minimizes the **weighted sum of squared residuals**: $\\sum w_i (y_i - f(x_i, \\beta))^2$, where $w_i = 1 / \\sigma_i^2$.\n",
    "\n",
    "The Levenberg-Marquardt algorithm is an iterative numerical optimization algorithm. It cleverly switches between a steepest descent method (when far from the minimum) and the Gauss-Newton method (when close to the minimum) to efficiently converge to the optimal parameter values. When `sigma` is provided, the function being minimized (the objective function) simply changes from the unweighted sum of squares to the weighted sum of squares, but the core optimization algorithm remains Levenberg-Marquardt.\n",
    "\n",
    "You can actually specify other optimization algorithms for `curve_fit` using the `method` parameter (e.g., `'trf'` for Trust Region Reflective, or `'dogbox'` for dogleg algorithm), but Levenberg-Marquardt (`'lm'`) is the default and often a good choice for non-linear least squares problems. The concept of weighting applies similarly regardless of the specific algorithm used for minimization, as long as that algorithm supports minimizing a weighted sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fc429",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de34a0aa",
   "metadata": {},
   "source": [
    "In your specific case, where you defined a $\\pm 0.5$ error for most points and a negligible error for the first point, you are effectively providing `curve_fit` with the standard deviations ($\\sigma_i$) of your measurements. The function then internally squares these to get the variances and uses their inverses as weights. The point with `sigma = 1e-9` gets an extremely large weight, forcing the fitted curve to pass almost exactly through that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80457a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the function to fit\n",
    "def custom_function(x, A, B):\n",
    "    \"\"\"\n",
    "    The custom function to approximate the data.\n",
    "    f(x; A, B) = A * (np.exp(-B * x) - 1) + 100\n",
    "    \"\"\"\n",
    "    return A * (np.exp(-B * x) - 1) + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare the datasets\n",
    "# Dataset 1\n",
    "data1_x = np.array([0, 5, 25, 34, 42, 57, 97])\n",
    "data1_y = np.array([100, 79.7, 51.3, 44.6, 39.8, 29.9, 10.3])\n",
    "\n",
    "# Dataset 2\n",
    "data2_x = np.array([0, 19, 45, 104, 191, 294, 391])\n",
    "data2_y = np.array([100, 80.4, 66.4, 50.1, 41.2, 28.5, 20.1])\n",
    "\n",
    "# Dataset 3\n",
    "data3_x = np.array([0, 23, 51, 98, 196, 292, 401])\n",
    "data3_y = np.array([100, 87.8, 77, 65.7, 50.9, 46.5, 44.4])\n",
    "\n",
    "# Combine datasets into a list for easier iteration\n",
    "datasets = [\n",
    "    {\"name\": \"Dataset 1\", \"x\": data1_x, \"y\": data1_y},\n",
    "    {\"name\": \"Dataset 2\", \"x\": data2_x, \"y\": data2_y},\n",
    "    {\"name\": \"Dataset 3\", \"x\": data3_x, \"y\": data3_y},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. Perform curve fitting and analysis for each dataset\n",
    "results = []\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f\"\\n--- Fitting {dataset['name']} ---\")\n",
    "    x_data = dataset[\"x\"]\n",
    "    y_data = dataset[\"y\"]\n",
    "\n",
    "    # Define sigma for Weighted Least Squares\n",
    "    # First point (Y=100) has no error, assign a very small sigma to give it high weight\n",
    "    # Other points have an error of +/- 0.5\n",
    "    sigma = np.full_like(y_data, 0.5, dtype=float)\n",
    "    sigma[0] = 1e-9  # Very small error for the first point\n",
    "    print(f\"Sigma for {dataset['name']}: {sigma}\")\n",
    "\n",
    "\n",
    "    # Initial guess for parameters A and B\n",
    "    initial_guess = [50, 0.01]\n",
    "\n",
    "    try:\n",
    "        # curve_fit returns:\n",
    "        # popt: Optimal values for the parameters so that the sum of the squared residuals is minimized.\n",
    "        # pcov: The estimated covariance of popt.\n",
    "        # Use sigma for weighted least squares\n",
    "        popt, pcov = curve_fit(custom_function, x_data, y_data, p0=initial_guess, sigma=sigma, absolute_sigma=True)\n",
    "        A_fit, B_fit = popt\n",
    "\n",
    "        # Calculate standard errors from the covariance matrix\n",
    "        perr = np.sqrt(np.diag(pcov))\n",
    "        A_err, B_err = perr\n",
    "\n",
    "        print(f\"Fitted parameters: A = {A_fit:.4f} +/- {A_err:.4f}, B = {B_fit:.4f} +/- {B_err:.4f}\")\n",
    "\n",
    "        print(\"Condition number of the covariance matrix:\", np.linalg.cond(pcov))  # Check condition number of the covariance matrix\n",
    "\n",
    "        # Generate predicted y values using the fitted function\n",
    "        y_predicted = custom_function(x_data, A_fit, B_fit)\n",
    "\n",
    "        # Calculate metrics\n",
    "        # Sum of Squared Residuals (SSR)\n",
    "        ssr = np.sum((y_data - y_predicted)**2)\n",
    "        print(f\"Sum of Squared Residuals (SSR): {ssr:.4f}\")\n",
    "\n",
    "        # Root Mean Squared Error (RMSE)\n",
    "        rmse = np.sqrt(mean_squared_error(y_data, y_predicted))\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "        # Total Sum of Squares (SST)\n",
    "        y_mean = np.mean(y_data)\n",
    "        sst = np.sum((y_data - y_mean)**2)\n",
    "        print(f\"Total Sum of Squares (TSS): {sst:.4f}\")\n",
    "\n",
    "        # R-squared (Coefficient of Determination)\n",
    "        r_squared = r2_score(y_data, y_predicted)\n",
    "        print(f\"R-squared: {r_squared:.4f}\")\n",
    "\n",
    "        # Adjusted R-squared calculation\n",
    "        n = len(x_data) # Number of data points\n",
    "        k = len(popt)  # Number of parameters (A, B)\n",
    "        if (n - k - 1) > 0:\n",
    "            adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)\n",
    "            print(f\"Adjusted R-squared: {adjusted_r_squared:.4f}\")\n",
    "        else:\n",
    "            adjusted_r_squared = np.nan\n",
    "            print(\"Adjusted R-squared: Not applicable (insufficient degrees of freedom)\")\n",
    "\n",
    "        results.append({\n",
    "            \"name\": dataset[\"name\"],\n",
    "            \"x_data\": x_data,\n",
    "            \"y_data\": y_data,\n",
    "            \"y_predicted\": y_predicted,\n",
    "            \"A_fit\": A_fit,\n",
    "            \"B_fit\": B_fit,\n",
    "            \"A_err\": A_err,\n",
    "            \"B_err\": B_err,\n",
    "            \"ssr\": ssr,\n",
    "            \"rmse\": rmse,\n",
    "            \"r_squared\": r_squared,\n",
    "            \"adjusted_r_squared\": adjusted_r_squared\n",
    "        })\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error: Could not fit curve for {dataset['name']}. {e}\")\n",
    "        results.append({\n",
    "            \"name\": dataset[\"name\"],\n",
    "            \"x_data\": x_data,\n",
    "            \"y_data\": y_data,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# 4. Plotting the results\n",
    "plt.figure(figsize=(8, 15))\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    plt.subplot(len(results), 1, i + 1)\n",
    "    plt.scatter(res[\"x_data\"], res[\"y_data\"], label=\"Original Data\", color='blue', s=50)\n",
    "\n",
    "    if \"y_predicted\" in res:\n",
    "        x_plot = np.linspace(min(res[\"x_data\"]), max(res[\"x_data\"]), 500)\n",
    "        y_plot = custom_function(x_plot, res[\"A_fit\"], res[\"B_fit\"])\n",
    "        plt.plot(x_plot, y_plot, color='red', label=f\"Fitted Curve\\nA={res['A_fit']:.2f}±{res['A_err']:.2f}\\nB={res['B_fit']:.4f}±{res['B_err']:.4f}\")\n",
    "\n",
    "        if not np.isnan(res['adjusted_r_squared']):\n",
    "            plt.title(f\"{res['name']} Fit\\nR²={res['r_squared']:.3f}, Adj. R²={res['adjusted_r_squared']:.3f}\\nRMSE={res['rmse']:.2f}\")\n",
    "        else:\n",
    "            plt.title(f\"{res['name']} Fit\\nR²={res['r_squared']:.3f}, RMSE={res['rmse']:.2f}\\nAdj. R²: N/A\")\n",
    "    else:\n",
    "        plt.title(f\"{res['name']} (Fit Error)\")\n",
    "        plt.text(0.5, 0.5, f\"Error: {res['error']}\", horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes, color='red')\n",
    "\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
