# Poisson Distribution

## Introduction

The **Poisson distribution** is a discrete probability distribution that models the probability of a given number of events occurring in a fixed interval of time or space, provided these events occur with a known constant mean rate and independently of the time since the last event. It is particularly adept at modeling **rare events**, or events that happen infrequently, within a given context where we are counting occurrences rather than a fixed number of trials (as in the Binomial distribution).

Consider phenomena such as:
*   The number of phone calls received by a call center in an hour.
*   The number of typos on a page of a book.
*   The number of radioactive decays detected by a Geiger counter in a given minute.
*   The number of defects in a certain length of cable.
*   The number of meteors observed in a specific area of the sky during a defined period.

In all these scenarios, we are counting discrete events over a continuous interval (time, length, area, or volume). The Poisson distribution provides the framework to understand the likelihood of observing $k$ such events.

## The Poisson Process and its Conditions

The Poisson distribution arises from a stochastic process known as the **Poisson Process**. A Poisson process describes a sequence of events that occur randomly and independently over time or space. For a discrete random variable $X$ to follow a **Poisson distribution**, it must represent the number of events in a fixed interval generated by a Poisson process. The underlying conditions (often called axioms) for such a process are crucial:

1.  **Independence of Events**: The occurrence of an event in one interval (or sub-interval) does not affect the probability of another event occurring in any other non-overlapping interval. This means events happen purely by chance, without memory.
2.  **Constant Average Rate (Homogeneity)**: The average rate at which events occur, denoted by $\lambda$ (lambda), is constant over the entire interval of observation. This means the probability of an event occurring in a given small sub-interval is the same for all small sub-intervals of equal length.
3.  **Events are "Rare" in Infinitesimal Intervals (Non-simultaneity)**: In a very small (infinitesimal) sub-interval, the probability of *one* event occurring is approximately proportional to the length of that sub-interval. Crucially, the probability of *two or more* events occurring in such an infinitesimal sub-interval is negligibly small (it approaches zero faster than the length of the sub-interval). This implies events cannot occur precisely simultaneously.
4.  **Number of Events are Counts**: The random variable $X$ represents the number of events, so its possible values are non-negative integers: $k \in \{0, 1, 2, 3, \ldots\}$.

### The Parameter $\lambda$

The single parameter that characterizes a Poisson distribution is $\lambda$.

*   $\lambda$ (lambda) is the **average number of events** occurring in the *fixed interval* under consideration. It is often referred to as the **rate parameter** for the interval.
*   $X$ represents a count of events. The parameter $\lambda$, in the context of the Poisson PMF formula, represents an expected *count* (e.g., "5 calls", "3 defects").
*   However, conceptually, $\lambda$ is directly tied to the *rate* at which events occur. If events occur at an average rate of $R$ events per unit of time (or space), and our fixed interval has a length of $T$ units, then $\lambda = R \cdot T$. In this case, $R$ would have units like $\text{s}^{-1}$ or $\text{events}/\text{meter}$, and $T$ would have units of $\text{s}$ or $\text{meter}$, making $\lambda$ (the average count for the interval) dimensionless. For instance, if a call center receives calls at a rate of $R = 5 \text{ calls/hour}$, and we are interested in a $T = 1 \text{ hour}$ interval, then $\lambda = 5 \text{ calls}$. If we were interested in a $T = 2 \text{ hour}$ interval, $\lambda = 10 \text{ calls}$.

We denote a Poisson random variable as $X \sim \text{Poisson}(\lambda)$.

## Probability Mass Function (PMF)

The Probability Mass Function (PMF) of a Poisson random variable $X$ provides the probability of observing exactly $k$ events in the given interval.

### Derivation of the PMF from the Binomial Distribution

One of the most elegant ways to derive the Poisson PMF is by considering it as a limiting case of the Binomial distribution. Recall that the Binomial distribution models the number of successes in $n$ independent Bernoulli trials, each with a probability of success $p$.

Imagine we are observing events over a continuous interval (e.g., time). We can divide this interval into a very large number of tiny, equal sub-intervals, say $n$ sub-intervals. In each of these small sub-intervals, we can conceptualize a Bernoulli trial: either an event happens (success) or it doesn't (failure).

Let:
*   $n$ be the number of very small sub-intervals (trials). We let $n \to \infty$.
*   $p$ be the probability of an event occurring in any single small sub-interval (probability of success). We assume $p$ is very small, so $p \to 0$.
*   The average number of events in the entire interval is $\lambda$. This means that the product $np$ remains constant and equal to $\lambda$ as $n \to \infty$ and $p \to 0$. From this, we have $p = \lambda/n$.

The PMF for a Binomial random variable $X \sim B(n, p)$ is:

$$P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$$

Now, we substitute $p = \lambda/n$ into this formula and take the limit as $n \to \infty$:

$$P(X=k) = \lim_{n \to \infty} \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k}$$

Let's break down this limit term by term:

**Binomial Coefficient Term**:

We expand the binomial coefficient $\binom{n}{k}$:
$$ \binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{n(n-1)(n-2)\dots(n-k+1)(n-k)!}{k!(n-k)!} = \frac{n(n-1)(n-2)\dots(n-k+1)}{k!} $$
As $n \to \infty$, for a fixed $k$, the numerator $n(n-1)\dots(n-k+1)$ contains $k$ terms, each approximately equal to $n$.
More rigorously, we can write:
$$ \frac{n(n-1)\dots(n-k+1)}{k!} \cdot \frac{1}{n^k} = \frac{1}{k!} \cdot \frac{n(n-1)\dots(n-k+1)}{n \cdot n \cdot \dots \cdot n} \quad (\text{k times}) $$
$$ = \frac{1}{k!} \cdot \left(\frac{n}{n}\right) \cdot \left(\frac{n-1}{n}\right) \cdot \dots \cdot \left(\frac{n-k+1}{n}\right) $$
$$ = \frac{1}{k!} \cdot 1 \cdot \left(1-\frac{1}{n}\right) \cdot \left(1-\frac{2}{n}\right) \cdot \dots \cdot \left(1-\frac{k-1}{n}\right) $$
As $n \to \infty$, each term $\left(1-\frac{j}{n}\right) \to 1$. Therefore, the entire expression $\lim_{n \to \infty} \frac{n(n-1)\dots(n-k+1)}{k! n^k}$ approaches $\frac{1}{k!} \cdot 1 = \frac{1}{k!}$.

**Probability of Success Term**:

This term is simply:
$$ \left(\frac{\lambda}{n}\right)^k = \frac{\lambda^k}{n^k} $$

**Probability of Failure Term**:

This term can be split:
$$ \left(1-\frac{\lambda}{n}\right)^{n-k} = \left(1-\frac{\lambda}{n}\right)^n \left(1-\frac{\lambda}{n}\right)^{-k} $$
Now, let's evaluate the limit of each part:
*   The first part is a fundamental limit definition of $e$:
    $$ \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^n = e^{-\lambda} $$
*   The second part: As $n \to \infty$, $\frac{\lambda}{n} \to 0$, so $\left(1-\frac{\lambda}{n}\right) \to 1$. Therefore, $\lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{-k} = 1^{-k} = 1$.

Now, let's combine these limits in the expression for $P(X=k)$:

$$P(X=k) = \lim_{n \to \infty} \left( \frac{n(n-1)\dots(n-k+1)}{k!} \right) \cdot \left( \frac{\lambda^k}{n^k} \right) \cdot \left(1-\frac{\lambda}{n}\right)^n \cdot \left(1-\frac{\lambda}{n}\right)^{-k}$$

Rearranging terms to group the parts we evaluated:

$$P(X=k) = \frac{\lambda^k}{k!} \cdot \lim_{n \to \infty} \left( \frac{n(n-1)\dots(n-k+1)}{n^k} \right) \cdot \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^n \cdot \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{-k}$$

Substituting the evaluated limits:

$$P(X=k) = \frac{\lambda^k}{k!} \cdot 1 \cdot e^{-\lambda} \cdot 1$$

Thus, the PMF of the Poisson distribution is:

$$p_X(k) = P(X=k) = \frac{e^{-\lambda} \lambda^k}{k!} \quad \text{for } k \in \{0, 1, 2, \ldots\}$$

where:
*   $e$ is Euler's number ($e \approx 2.71828$).
*   $\lambda$ is the average number of events in the interval ($\lambda > 0$).
*   $k$ is the number of events we are interested in ($k$ is a non-negative integer).
*   $k!$ is the factorial of $k$.

### Properties of the PMF (Validation)

1.  **Non-negativity**: Since $\lambda > 0$, $e^{-\lambda}$ is always positive, $\lambda^k \ge 0$, and $k!$ is positive. Therefore, $p_X(k) \ge 0$ for all $k$. This property is satisfied.
2.  **Normalization**: The sum of all probabilities for all possible values of $k$ must equal 1.
    $$\sum_{k=0}^{\infty} p_X(k) = \sum_{k=0}^{\infty} \frac{e^{-\lambda} \lambda^k}{k!}$$
    We can factor out $e^{-\lambda}$ as it does not depend on $k$:
    $$ = e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} $$
    Recall the Maclaurin series expansion for $e^x$: $e^x = \sum_{k=0}^{\infty} \frac{x^k}{k!}$.
    Therefore, $\sum_{k=0}^{\infty} \frac{\lambda^k}{k!} = e^{\lambda}$.
    Substituting this back:
    $$ = e^{-\lambda} \cdot e^{\lambda} = e^0 = 1 $$
    The normalization property is satisfied, confirming this is a valid PMF.

## Expected Value (Mean)

The **expected value** or **mean** of a Poisson random variable $X$, denoted $E[X]$ or $\mu$, is equal to its parameter $\lambda$.

$$E[X] = \lambda$$

Since $X$ is a count, its expected value $\lambda$ is a dimensionless count.

### Derivation of the Mean

We use the definition of the expected value for a discrete random variable:

$$E[X] = \sum_{k=0}^{\infty} k \cdot P(X=k)$$

Substitute the Poisson PMF:

$$E[X] = \sum_{k=0}^{\infty} k \frac{e^{-\lambda} \lambda^k}{k!}$$

The term for $k=0$ is $0 \cdot P(X=0) = 0$, so we can start the sum from $k=1$:

$$E[X] = \sum_{k=1}^{\infty} k \frac{e^{-\lambda} \lambda^k}{k!}$$

Now, we use the property $k! = k \cdot (k-1)!$:

$$E[X] = \sum_{k=1}^{\infty} k \frac{e^{-\lambda} \lambda^k}{k(k-1)!}$$

We can cancel $k$ from the numerator and denominator:

$$E[X] = \sum_{k=1}^{\infty} \frac{e^{-\lambda} \lambda^k}{(k-1)!}$$

Factor out $e^{-\lambda}$ and one $\lambda$ (from $\lambda^k = \lambda \cdot \lambda^{k-1}$):

$$E[X] = \lambda e^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}$$

Let $j = k-1$. As $k$ goes from $1$ to $\infty$, $j$ goes from $0$ to $\infty$:

$$E[X] = \lambda e^{-\lambda} \sum_{j=0}^{\infty} \frac{\lambda^j}{j!}$$

Again, we recognize the Maclaurin series for $e^x$: $\sum_{j=0}^{\infty} \frac{\lambda^j}{j!} = e^{\lambda}$.

$$E[X] = \lambda e^{-\lambda} e^{\lambda}$$

$$E[X] = \lambda$$

This confirms that the mean of a Poisson distribution is simply its rate parameter $\lambda$.

## Variance

The **variance** of a Poisson random variable $X$, denoted $Var(X)$ or $\sigma^2$, is also equal to its parameter $\lambda$.

$$Var(X) = \lambda$$

The variance, like the expected value, is a **dimensionless** quantity, as it represents the dispersion of a count.

### Derivation of the Variance

We use the computational formula for variance: $Var(X) = E[X^2] - (E[X])^2$.
We already know $E[X] = \lambda$. So we need to calculate $E[X^2]$.
A common trick is to first calculate $E[X(X-1)]$:

$$E[X(X-1)] = \sum_{k=0}^{\infty} k(k-1) P(X=k)$$

$$E[X(X-1)] = \sum_{k=0}^{\infty} k(k-1) \frac{e^{-\lambda} \lambda^k}{k!}$$

The terms for $k=0$ (where $0(0-1)=0$) and $k=1$ (where $1(1-1)=0$) are both zero, so we can start the summation from $k=2$:

$$E[X(X-1)] = \sum_{k=2}^{\infty} k(k-1) \frac{e^{-\lambda} \lambda^k}{k!}$$

Use the property $k! = k(k-1)(k-2)!$:

$$E[X(X-1)] = \sum_{k=2}^{\infty} k(k-1) \frac{e^{-\lambda} \lambda^k}{k(k-1)(k-2)!}$$

Cancel $k(k-1)$ from numerator and denominator:

$$E[X(X-1)] = \sum_{k=2}^{\infty} \frac{e^{-\lambda} \lambda^k}{(k-2)!}$$

Factor out $e^{-\lambda}$ and $\lambda^2$ (from $\lambda^k = \lambda^2 \cdot \lambda^{k-2}$):

$$E[X(X-1)] = \lambda^2 e^{-\lambda} \sum_{k=2}^{\infty} \frac{\lambda^{k-2}}{(k-2)!}$$

Let $j = k-2$. As $k$ goes from $2$ to $\infty$, $j$ goes from $0$ to $\infty$:

$$E[X(X-1)] = \lambda^2 e^{-\lambda} \sum_{j=0}^{\infty} \frac{\lambda^j}{j!}$$

Again, we recognize the Maclaurin series for $e^x$: $\sum_{j=0}^{\infty} \frac{\lambda^j}{j!} = e^{\lambda}$.

$$E[X(X-1)] = \lambda^2 e^{-\lambda} e^{\lambda}$$

$$E[X(X-1)] = \lambda^2$$

Now, we use the identity $E[X^2] = E[X(X-1)] + E[X]$:

$$E[X^2] = \lambda^2 + \lambda$$

Finally, substitute this into the variance formula:

$$Var(X) = E[X^2] - (E[X])^2$$

$$Var(X) = (\lambda^2 + \lambda) - (\lambda)^2$$

$$Var(X) = \lambda^2 + \lambda - \lambda^2$$

$$Var(X) = \lambda$$

It's a remarkable and unique property of the Poisson distribution that its mean and variance are equal to its parameter $\lambda$.

## Standard Deviation

The **standard deviation** of a Poisson random variable, denoted $\sigma_X$, is the square root of its variance.

$$\sigma_X = \sqrt{\lambda}$$

This is also a dimensionless quantity, as it's a measure of spread for a count.

## Mode of the Poisson Distribution

The **mode** of a discrete distribution is the value that occurs with the highest probability. For a Poisson distribution with parameter $\lambda$:

*   If $\lambda$ is not an integer, the mode is $\lfloor \lambda \rfloor$ (the floor of $\lambda$, meaning $\lambda$ rounded down to the nearest integer).
*   If $\lambda$ is an integer, then there are two modes: $\lambda - 1$ and $\lambda$.

### Derivation of the Mode

To find the mode, we examine the ratio of consecutive probabilities, $\frac{P(X=k)}{P(X=k-1)}$. The probability $P(X=k)$ will be increasing as long as this ratio is greater than 1, and decreasing when it is less than 1. The mode(s) occur where the probability is maximized.

Let's set up the ratio:

$$ \frac{P(X=k)}{P(X=k-1)} = \frac{\frac{e^{-\lambda} \lambda^k}{k!}}{\frac{e^{-\lambda} \lambda^{k-1}}{(k-1)!}} $$

We can simplify this expression:

$$ = \frac{e^{-\lambda} \lambda^k}{k!} \cdot \frac{(k-1)!}{e^{-\lambda} \lambda^{k-1}} $$

The $e^{-\lambda}$ terms cancel out. We use $k! = k \cdot (k-1)!$ and $\lambda^k = \lambda \cdot \lambda^{k-1}$:

$$ = \frac{\lambda \cdot \lambda^{k-1}}{k \cdot (k-1)!} \cdot \frac{(k-1)!}{\lambda^{k-1}} $$

Further simplification by canceling common terms:

$$ = \frac{\lambda}{k} $$

Now, we want to find $k$ for which $P(X=k) \ge P(X=k-1)$, which means $\frac{P(X=k)}{P(X=k-1)} \ge 1$:

$$ \frac{\lambda}{k} \ge 1 $$

$$ \lambda \ge k $$

This inequality tells us that the probability $P(X=k)$ continues to increase as long as $k$ is less than or equal to $\lambda$. The mode(s) will be the largest integer(s) satisfying this condition.

*   **Case 1: $\lambda$ is not an integer.** If, for example, $\lambda = 3.7$, then $k \le 3.7$. The largest integer $k$ satisfying this is $3$. For $k=3$, $\frac{3.7}{3} > 1$, so $P(X=3) > P(X=2)$. For $k=4$, $\frac{3.7}{4} < 1$, so $P(X=4) < P(X=3)$. Thus, the unique mode is $\lfloor \lambda \rfloor = 3$.
*   **Case 2: $\lambda$ is an integer.** If, for example, $\lambda = 5$, then $k \le 5$. The largest integer $k$ satisfying this is $5$. For $k=5$, $\frac{5}{5} = 1$, which means $P(X=5) = P(X=4)$. For $k=6$, $\frac{5}{6} < 1$, so $P(X=6) < P(X=5)$. In this case, there are two modes: $\lambda-1$ and $\lambda$, i.e., $4$ and $5$.

Thus, the mode of the Poisson distribution is $\lfloor \lambda \rfloor$ if $\lambda$ is not an integer, and $\lambda-1, \lambda$ if $\lambda$ is an integer.

## Cumulative Distribution Function (CDF)

The **Cumulative Distribution Function (CDF)** for a Poisson random variable $X \sim \text{Poisson}(\lambda)$, denoted $F_X(x)$, gives the probability that $X$ will take a value less than or equal to $x$. Since $X$ is a discrete variable, the CDF is a step function.

$$F_X(x) = P(X \le x) = \sum_{k=0}^{\lfloor x \rfloor} \frac{e^{-\lambda} \lambda^k}{k!}$$

where $\lfloor x \rfloor$ is the floor function, representing the largest integer less than or equal to $x$. Similar to the Binomial CDF, there is no simple closed-form expression for the Poisson CDF. It is typically calculated using summation, statistical tables, or computational software.

## Examples and Analogies

The Poisson distribution is incredibly versatile for modeling counts of events in various fields:

*   **Biology**: The number of mutations on a strand of DNA per unit length. The number of bacteria in a given volume of liquid.
*   **Physics**: The number of radioactive decays per second in a sample. The number of photons arriving at a telescope from a distant star per minute.
*   **Business/Operations**: The number of customers arriving at a checkout counter in a 10-minute interval. The number of defects in a roll of fabric of a certain length. The number of emergency calls received by a 911 dispatch center in an hour.
*   **Safety/Insurance**: The number of accidents on a particular stretch of highway per month. The number of insurance claims filed in a day.
*   **Ecology**: The number of trees of a certain species in a defined area.

**Analogy**: Imagine a large, empty field. We are scattering individual seeds (events) onto this field entirely at random, so each seed's landing position is independent of others. The Poisson distribution helps us predict how many seeds (events) will land in any particular small square (fixed interval) on the field, given we know the average density of seeds ($\lambda$) across the whole field. The key is that the seeds are randomly and independently scattered, and we're looking at counts in fixed-size regions.

### Example: Website Traffic

Suppose a popular website receives an average of 3 page views per minute during off-peak hours. We want to find the probability that the website receives exactly 5 page views in the next minute.

Here, $X$ is the number of page views in a minute, and it follows a Poisson distribution with $\lambda = 3$. We want to find $P(X=5)$.

Using the PMF:

$$P(X=5) = \frac{e^{-3} 3^5}{5!}$$

$$P(X=5) = \frac{e^{-3} \cdot 243}{120}$$

$$P(X=5) \approx \frac{0.049787 \cdot 243}{120}$$

$$P(X=5) \approx \frac{12.098}{120}$$

$$P(X=5) \approx 0.1008$$

So, there is approximately a 10.08% chance that the website receives exactly 5 page views in the next minute.

## Relationship to Other Distributions

### Poisson Approximation to the Binomial Distribution

As we saw in its derivation, the Poisson distribution serves as an excellent approximation to the Binomial distribution under specific conditions. This occurs when:
*   The number of trials $n$ is very large ($n \to \infty$).
*   The probability of success $p$ in each trial is very small ($p \to 0$).
*   The product $np$ (which is the expected number of successes in the Binomial distribution) remains constant and is equal to $\lambda$.

This approximation is extremely useful in practice because the Poisson PMF is often simpler to calculate than the Binomial PMF for very large $n$ and small $p$. For instance, if you're looking at the number of defective items in a very large batch where the defect rate is tiny (e.g., $n=10000, p=0.0001$), a Poisson distribution with $\lambda = np = 1$ can provide a good estimate without needing to calculate large binomial coefficients.

### Relationship to the Exponential Distribution

A crucial conceptual link exists between the Poisson distribution and the **Exponential distribution** (which is a continuous probability distribution we will encounter later). If the number of events occurring in a fixed interval follows a Poisson distribution with rate $\lambda$, then the *time between consecutive events* (also known as the inter-arrival time) follows an Exponential distribution with rate parameter $\lambda$.

This means:
*   The Poisson distribution describes **counts of events** in a fixed interval.
*   The Exponential distribution describes **waiting times** until the next event.

This relationship is a direct consequence of the underlying Poisson process. The **memoryless property** of the Exponential distribution (the probability of an event occurring in the next interval of time is independent of how much time has already passed) directly corresponds to the independence assumption of the Poisson process. For example, if the number of phone calls per hour follows a Poisson distribution with $\lambda=5$, then the time (in hours) between any two consecutive phone calls follows an Exponential distribution with rate $\lambda=5$. This connection highlights the versatility of $\lambda$ as a rate parameter, connecting discrete counts to continuous waiting times.

### Normal Approximation

For large values of $\lambda$ (a common rule of thumb is $\lambda \ge 10$ or $\lambda \ge 20$), the Poisson distribution can be approximated by a **Normal (Gaussian) distribution** (which we will cover in a later lecture).

Specifically, if $X \sim \text{Poisson}(\lambda)$, then for large $\lambda$, $X$ can be approximated by $X \sim N(\mu, \sigma^2)$ where $\mu = \lambda$ and $\sigma^2 = \lambda$. This approximation is particularly useful for statistical inference when dealing with count data with high average rates, allowing us to use the well-understood properties of the Normal distribution for hypothesis testing and confidence interval construction. A **continuity correction** is often applied for better accuracy when approximating a discrete distribution with a continuous one.

## Further Insights

### Shape of the Poisson PMF

The shape of the Poisson probability mass function is determined by its single parameter $\lambda$.
*   For small $\lambda$ (e.g., $\lambda=1$), the distribution is highly skewed to the right, with the highest probability at $k=0$ or $k=1$.
*   As $\lambda$ increases, the distribution becomes more symmetrical and bell-shaped, gradually resembling a Normal distribution. The peak of the distribution shifts to the right, centered around $\lambda$.

### Applications in Stochastic Processes

The Poisson distribution is a cornerstone of stochastic processes and finds widespread use in:
*   **Queuing Theory**: Modeling customer arrivals at service points (e.g., call centers, checkout lines).
*   **Reliability Engineering**: Predicting the number of failures of a system over a given period.
*   **Epidemiology**: Modeling the number of new disease cases in a population.
*   **Finance**: Modeling the number of rare events like stock market jumps or insurance claims.

Its fundamental nature, characterized by a single parameter $\lambda$ that governs both its mean and variance, makes it a powerful and widely applicable tool for understanding random phenomena involving counts of events. It is a distribution that beautifully captures the essence of "rare events" occurring randomly over an interval.

















---

$$P(X=k) = \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k}$$

Expand $\binom{n}{k} = \frac{n!}{k!(n-k)!}$:
$$P(X=k) = \frac{n!}{k!(n-k)!} \frac{\lambda^k}{n^k} \left(1-\frac{\lambda}{n}\right)^{n-k}$$

Now, let's rearrange the terms and take the limit as $n \to \infty$:
$$P(X=k) = \frac{\lambda^k}{k!} \cdot \frac{n!}{(n-k)! n^k} \cdot \left(1-\frac{\lambda}{n}\right)^{n-k}$$

Consider each part as $n \to \infty$:

**$\frac{n!}{(n-k)! n^k}$**:
$$ \frac{n(n-1)(n-2)\ldots(n-k+1)}{n^k} = 1 \cdot \left(1-\frac{1}{n}\right) \cdot \left(1-\frac{2}{n}\right) \cdot \ldots \cdot \left(1-\frac{k-1}{n}\right) $$
As $n \to \infty$, each term $\left(1-\frac{j}{n}\right) \to 1$. So, this entire expression approaches $1^k = 1$.

**$\left(1-\frac{\lambda}{n}\right)^{n-k}$**:
We can rewrite this as:
$$\left(1-\frac{\lambda}{n}\right)^{n-k} = \left(1-\frac{\lambda}{n}\right)^n \left(1-\frac{\lambda}{n}\right)^{-k}$$
As $n \to \infty$:
*   $\lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^n = e^{-\lambda}$ (This is a standard limit definition of $e$). ???????????????
*   $\lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{-k} = (1-0)^{-k} = 1^{-k} = 1$.

Combining these limits, as $n \to \infty$:
$$P(X=k) \to \frac{\lambda^k}{k!} \cdot 1 \cdot e^{-\lambda} \cdot 1$$

$$P(X=k) = \frac{e^{-\lambda} \lambda^k}{k!}$$

This completes the derivation of the Poisson PMF. The mode for a Poisson distribution is $\lfloor \lambda \rfloor$ (if $\lambda$ is not an integer) or $\lambda$ and $\lambda-1$ (if $\lambda$ is an integer).

### Verification of PMF Normalization

A fundamental property of any valid Probability Mass Function is that the sum of probabilities over all possible outcomes must equal 1. Let's verify this for the Poisson PMF. We need to show that:

$$\sum_{k=0}^{\infty} P(X=k) = 1$$

Substitute the Poisson PMF formula:

$$ \sum_{k=0}^{\infty} \frac{e^{-\lambda} \lambda^k}{k!} $$

We can factor out $e^{-\lambda}$ from the summation, as it does not depend on the index $k$:

$$ e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} $$

Now, we recognize the summation $\sum_{k=0}^{\infty} \frac{\lambda^k}{k!}$ as the well-known Taylor series expansion for the exponential function $e^x$, evaluated at $x=\lambda$:

$$ \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} = 1 + \lambda + \frac{\lambda^2}{2!} + \frac{\lambda^3}{3!} + \ldots = e^\lambda $$

Substituting this back into our expression for the sum of probabilities:

$$ e^{-\lambda} \cdot (e^\lambda) $$

$$ e^{-\lambda} \cdot e^\lambda = e^{(-\lambda + \lambda)} = e^0 = 1 $$

Thus, the sum of all probabilities for the Poisson distribution equals 1, confirming it as a valid Probability Mass Function.

## Expected Value (Mean) of a Poisson Distribution

The expected value of a Poisson distribution is $\lambda$.

### Derivation of the Mean

The expected value $E[X]$ for a discrete random variable is $\sum_{k=0}^{\infty} k P(X=k)$.
For the Poisson distribution:

$$E[X] = \sum_{k=0}^{\infty} k \frac{e^{-\lambda} \lambda^k}{k!}$$

We can factor out $e^{-\lambda}$ and note that the term for $k=0$ is $0 \cdot P(X=0) = 0$, so we can start the sum from $k=1$:

$$E[X] = e^{-\lambda} \sum_{k=1}^{\infty} k \frac{\lambda^k}{k!}$$

We know that $k! = k \cdot (k-1)!$. So we can simplify $k/k!$:

$$E[X] = e^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^k}{(k-1)!}$$

Let's pull out one $\lambda$ from $\lambda^k = \lambda \cdot \lambda^{k-1}$:

$$E[X] = e^{-\lambda} \lambda \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}$$

Let $j = k-1$. When $k=1$, $j=0$. When $k \to \infty$, $j \to \infty$. So the sum becomes:

$$E[X] = e^{-\lambda} \lambda \sum_{j=0}^{\infty} \frac{\lambda^j}{j!}$$

We recognize the sum $\sum_{j=0}^{\infty} \frac{\lambda^j}{j!}$ as the Taylor series expansion of $e^\lambda$:

$$\sum_{j=0}^{\infty} \frac{\lambda^j}{j!} = e^\lambda$$

Substituting this back into the expression for $E[X]$:

$$E[X] = e^{-\lambda} \lambda e^\lambda$$

$$E[X] = \lambda$$

The mean of a Poisson distribution is $\lambda$, the average number of events. This is consistent with its definition.

## Variance of a Poisson Distribution

The variance of a Poisson distribution is also $\lambda$.

### Derivation of the Variance

The variance can be derived using the formula $Var[X] = E[X^2] - (E[X])^2$. We know $E[X] = \lambda$.
A useful trick for Poisson distribution is to compute $E[X(X-1)]$ first:

$$E[X(X-1)] = \sum_{k=0}^{\infty} k(k-1) \frac{e^{-\lambda} \lambda^k}{k!}$$

The terms for $k=0$ and $k=1$ are zero ($0 \cdot (-1)=0$ and $1 \cdot 0=0$). So we can start the sum from $k=2$:

$$E[X(X-1)] = e^{-\lambda} \sum_{k=2}^{\infty} k(k-1) \frac{\lambda^k}{k(k-1)(k-2)!}$$

$$E[X(X-1)] = e^{-\lambda} \sum_{k=2}^{\infty} \frac{\lambda^k}{(k-2)!}$$

Let's pull out $\lambda^2$ from $\lambda^k = \lambda^2 \cdot \lambda^{k-2}$:

$$E[X(X-1)] = e^{-\lambda} \lambda^2 \sum_{k=2}^{\infty} \frac{\lambda^{k-2}}{(k-2)!}$$

Let $j = k-2$. When $k=2$, $j=0$. When $k \to \infty$, $j \to \infty$.

$$E[X(X-1)] = e^{-\lambda} \lambda^2 \sum_{j=0}^{\infty} \frac{\lambda^j}{j!}$$

Again, we recognize the sum as $e^\lambda$:

$$E[X(X-1)] = e^{-\lambda} \lambda^2 e^\lambda$$

$$E[X(X-1)] = \lambda^2$$

Now we use the property $E[X(X-1)] = E[X^2 - X] = E[X^2] - E[X]$:

$$\lambda^2 = E[X^2] - E[X]$$

We know $E[X] = \lambda$, so:

$$\lambda^2 = E[X^2] - \lambda$$

$$E[X^2] = \lambda^2 + \lambda$$

Finally, we calculate the variance:

$$Var[X] = E[X^2] - (E[X])^2$$

$$Var[X] = (\lambda^2 + \lambda) - (\lambda)^2$$

$$Var[X] = \lambda^2 + \lambda - \lambda^2$$

$$Var[X] = \lambda$$

The variance of a Poisson distribution is $\lambda$. This unique property, where the mean equals the variance ($E[X] = Var[X] = \lambda$), is a characteristic hallmark of the Poisson distribution. This mean-variance equality is often used to test if observed count data is consistent with a Poisson model. If the sample variance is significantly different from the sample mean, it suggests that a Poisson model might not be appropriate (e.g., **overdispersion** if variance > mean, often leading to a Negative Binomial model). The standard deviation is $\sigma_X = \sqrt{\lambda}$. Both are dimensionless counts.

## Cumulative Distribution Function (CDF)

The CDF for a Poisson random variable $X$ is the sum of its PMF values up to a certain point $k$.

### Derivation of the CDF

The CDF $F_X(k) = P(X \le k)$ is defined as the sum of the probabilities of observing $0, 1, \ldots, k$ events:

$$F_X(k) = P(X \le k) = \sum_{i=0}^{k} P(X=i) = \sum_{i=0}^{k} \frac{e^{-\lambda} \lambda^i}{i!}$$

This is a step function. For practical calculations, especially for larger $k$, statistical software or tables are used. For example, if $X \sim \text{Poisson}(1)$: ??????????
*   $P(X=0) = \frac{e^{-1} 1^0}{0!} = e^{-1} \approx 0.3679$
*   $P(X=1) = \frac{e^{-1} 1^1}{1!} = e^{-1} \approx 0.3679$
*   $P(X=2) = \frac{e^{-1} 1^2}{2!} = \frac{e^{-1}}{2} \approx 0.1839$

Then, the CDF values are: ????????????
*   For $x < 0$: $F_X(x) = 0$
*   For $0 \le x < 1$: $F_X(x) = P(X \le x) = P(X=0) \approx 0.3679$
*   For $1 \le x < 2$: $F_X(x) = P(X \le x) = P(X=0) + P(X=1) \approx 0.3679 + 0.3679 = 0.7358$
*   For $2 \le x < 3$: $F_X(x) = P(X \le x) = P(X \le 1) + P(X=2) \approx 0.7358 + 0.1839 = 0.9197$
*   And so on, approaching 1 as $x \to \infty$.

## Example of Poisson Distribution

Suppose a call center receives an average of 5 calls per hour ($\lambda=5$). We assume the calls arrive independently and at a constant rate. What is the probability that the call center receives exactly 3 calls in the next hour?

Using the Poisson PMF with $\lambda=5$ and $k=3$: ??????????

$$P(X=3) = \frac{e^{-5} 5^3}{3!}$$

$$P(X=3) = \frac{e^{-5} \cdot 125}{3 \cdot 2 \cdot 1}$$

$$P(X=3) = \frac{0.0067379 \cdot 125}{6}$$

$$P(X=3) = \frac{0.8422375}{6}$$

$$P(X=3) \approx 0.1404$$

So, there's approximately a 14.04% chance of receiving exactly 3 calls in the next hour.
The expected number of calls is $E[X] = \lambda = 5$.
The variance of the number of calls is also $Var[X] = \lambda = 5$.




















# Poisson Distribution

The Poisson distribution is another crucial discrete probability distribution, often used for modeling the number of events occurring in a fixed interval of time or space. It is particularly useful for rare events.

## What is a Poisson Distribution?

We use the **Poisson distribution** to model the number of times an event occurs in a fixed interval of time or space, under the following crucial assumptions:
1.  **Events Occur Independently:** The occurrence of one event does not affect the probability of another event occurring in the interval.
2.  **Constant Average Rate ($\lambda$):** Events occur at a constant average rate over the given interval. This rate is uniform across time or space.
3.  **Events Cannot Occur Simultaneously:** In any infinitesimally small sub-interval, the probability of more than one event occurring is negligible.
4.  **Count of Events:** The random variable $X$ counts the number of occurrences of these events.

If these assumptions are violated (e.g., events cluster together, the rate changes over time, or multiple events can happen at precisely the same instant), the Poisson distribution may not be an appropriate model. Examples include the number of phone calls received by a call center per hour, the number of defects per square meter of fabric, or the number of accidents on a particular stretch of road per month.

### Parameters of the Poisson Distribution

The Poisson distribution is characterized by a single parameter:
*   $\lambda$ (lambda): The average rate of events occurring in the given fixed interval. $\lambda$ must be a positive real number ($\lambda > 0$).

A random variable $X$ that follows a Poisson distribution is denoted as $X \sim \text{Poisson}(\lambda)$.
The possible values for $X$ are $0, 1, 2, \dots$ (non-negative integers, potentially infinite). This infinite support is a key distinction from the Binomial distribution, which has a finite upper bound ($n$). While the probability of very large $k$ values becomes infinitesimally small very quickly, there is theoretically no upper limit to the number of events that can occur.

### Clarification on Units for $\lambda$

The parameter $\lambda$ represents the *expected number of events* in the specified interval. As such, when directly used in the Poisson PMF, $\lambda$ is a **dimensionless count**. For example, if we say "the average number of customers arriving at a store is 10 per hour," then $\lambda = 10$ for an interval of one hour.

However, it's important to understand the underlying physical rate. If we consider an average *rate* $\mu$ (e.g., events per second) and an observation interval length $T$ (e.g., in seconds), then $\lambda$ is calculated as $\lambda = \mu T$. In this context:
*   $\mu$ would have units of inverse time (e.g., $1/\text{s}$ or $\text{s}^{-1}$ in SI units).
*   $T$ would have units of time (e.g., s in SI units).
*   Multiplying these, $\lambda = \mu T$ becomes dimensionless ($\text{s}^{-1} \cdot \text{s} = 1$).
Therefore, in the context of the Poisson PMF, Mean, and Variance derivations, we will consistently treat $\lambda$ as a dimensionless average count for the interval of interest.

### Parameter Estimation in Data Science

In data analysis, the average rate $\lambda$ for a Poisson distribution is typically unknown and must be estimated from a sample of observed counts. The **Maximum Likelihood Estimator (MLE)** for $\lambda$ is the sample mean of the observed counts. If we have a dataset of $N$ observations $x_1, x_2, \dots, x_N$ (each representing a count of events in its respective interval), the MLE for $\lambda$ is $\hat{\lambda} = \bar{x} = \frac{1}{N} \sum_{i=1}^N x_i$. This estimate also corresponds to the **Method of Moments** estimator for $\lambda$.

### Probability Mass Function (PMF)

The PMF of a Poisson random variable $X$ gives us the probability of observing exactly $k$ events in the given interval.

For $k \in \{0, 1, 2, \dots\}$, the PMF is defined as:

$$p_X(k) = P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$$

where $e$ is Euler's number (the base of the natural logarithm), approximately $2.71828$. The factorial $k!$ in the denominator naturally restricts $k$ to be a non-negative integer.

#### Derivation of the PMF (as a limit of Binomial Distribution)

One of the most intuitive and powerful ways to derive the Poisson PMF is by considering it as a limiting case of the Binomial distribution. Imagine we are observing events over a continuous interval (e.g., an hour). We can divide this hour into $n$ very small, equal sub-intervals.

For each tiny sub-interval, an event either occurs or it doesn't. This can be seen as a Bernoulli trial.
*   Let $n$ be the number of very small sub-intervals.
*   Let $p$ be the probability of an event occurring in any single small sub-interval.

If the average rate of events over the entire interval is $\lambda$, then for each small sub-interval, the probability of an event $p$ can be approximated as $\lambda/n$. This implies that the total number of events in the full interval can be modeled by a Binomial distribution $X \sim \text{Binomial}(n, p)$ where $p = \lambda/n$.

Now, we let the number of sub-intervals $n$ become infinitely large ($n \to \infty$). As $n \to \infty$, $p = \lambda/n \to 0$, meaning the probability of an event in any single infinitesimal sub-interval becomes very small (a "rare event" in that tiny interval). The product $np = \lambda$ remains constant.

The Binomial PMF is $P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$.
Substitute $p = \lambda/n$:

$$P(X=k) = \lim_{n \to \infty} \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k}$$

Let's break down the limit:

1.  **Binomial coefficient term**:
    $$\binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{n(n-1)(n-2)\dots(n-k+1)}{k!}$$
    Multiply this by $\left(\frac{1}{n}\right)^k$:
    $$\frac{n(n-1)(n-2)\dots(n-k+1)}{k! n^k} = \frac{1}{k!} \cdot \frac{n}{n} \cdot \frac{n-1}{n} \cdot \dots \cdot \frac{n-k+1}{n}$$
    As $n \to \infty$, each term $\frac{n-i}{n}$ approaches 1. So,
    $$\lim_{n \to \infty} \frac{n(n-1)\dots(n-k+1)}{k! n^k} = \frac{1}{k!} \cdot 1 \cdot 1 \cdot \dots \cdot 1 = \frac{1}{k!}$$

2.  **$(1-p)^{n-k}$ term**:
    $$\left(1-\frac{\lambda}{n}\right)^{n-k} = \left(1-\frac{\lambda}{n}\right)^n \left(1-\frac{\lambda}{n}\right)^{-k}$$
    As $n \to \infty$:
    *   We know that $\lim_{n \to \infty} \left(1+\frac{x}{n}\right)^n = e^x$. So, $\lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^n = e^{-\lambda}$.
    *   And $\lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{-k} = (1-0)^{-k} = 1^{-k} = 1$.
    So, $\lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{n-k} = e^{-\lambda} \cdot 1 = e^{-\lambda}$.

3.  **$\lambda^k$ term**:
    The $\lambda^k$ part from $\left(\frac{\lambda}{n}\right)^k$ remains as $\lambda^k$.

Combining these parts, as $n \to \infty$:

$$P(X=k) = \frac{1}{k!} \cdot \lambda^k \cdot e^{-\lambda}$$

$$P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$$

This derivation beautifully links the discrete, fixed-trial Binomial distribution to the discrete, continuous-time Poisson distribution, showing how Poisson emerges when events are rare and numerous opportunities for them exist. This is why the Poisson is often referred to as the "law of rare events."

### Expected Value (Mean)

#### Derivation of the Expected Value

The expected value of a Poisson random variable can be derived directly from its PMF:

$$E[X] = \sum_{k=0}^{\infty} k \cdot P(X=k) = \sum_{k=0}^{\infty} k \frac{\lambda^k e^{-\lambda}}{k!}$$

We can factor out $e^{-\lambda}$ and note that the $k=0$ term is 0:

$$E[X] = e^{-\lambda} \sum_{k=1}^{\infty} k \frac{\lambda^k}{k!}$$

For $k \ge 1$, we can simplify $k/k! = 1/(k-1)!$:

$$E[X] = e^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^k}{(k-1)!}$$

Let $j = k-1$. When $k=1$, $j=0$. When $k \to \infty$, $j \to \infty$.

$$E[X] = e^{-\lambda} \sum_{j=0}^{\infty} \frac{\lambda^{j+1}}{j!}$$

We can factor out $\lambda$ from $\lambda^{j+1}$:

$$E[X] = e^{-\lambda} \lambda \sum_{j=0}^{\infty} \frac{\lambda^j}{j!}$$

We recognize the sum as the Taylor series expansion for $e^{\lambda}$: $\sum_{j=0}^{\infty} \frac{x^j}{j!} = e^x$.

$$E[X] = e^{-\lambda} \lambda e^{\lambda}$$

$$E[X] = \lambda$$

The expected value of a Poisson random variable is its parameter $\lambda$, which by definition is the average rate of events in the given interval.

### Variance

#### Derivation of the Variance

To find the variance, $Var(X) = E[X^2] - (E[X])^2$, we first need $E[X^2]$. It's often easier to compute $E[X(X-1)]$ first for distributions involving $k!$ in the denominator:

$$E[X(X-1)] = \sum_{k=0}^{\infty} k(k-1) \cdot P(X=k) = \sum_{k=0}^{\infty} k(k-1) \frac{\lambda^k e^{-\lambda}}{k!}$$

The terms for $k=0$ and $k=1$ are 0, so we can start the sum from $k=2$:

$$E[X(X-1)] = e^{-\lambda} \sum_{k=2}^{\infty} k(k-1) \frac{\lambda^k}{k!}$$

For $k \ge 2$, we can simplify $k(k-1)/k! = 1/(k-2)!$:

$$E[X(X-1)] = e^{-\lambda} \sum_{k=2}^{\infty} \frac{\lambda^k}{(k-2)!}$$

Let $j = k-2$. When $k=2$, $j=0$. When $k \to \infty$, $j \to \infty$.

$$E[X(X-1)] = e^{-\lambda} \sum_{j=0}^{\infty} \frac{\lambda^{j+2}}{j!}$$

Factor out $\lambda^2$:

$$E[X(X-1)] = e^{-\lambda} \lambda^2 \sum_{j=0}^{\infty} \frac{\lambda^j}{j!}$$

Again, recognizing the sum as $e^{\lambda}$:

$$E[X(X-1)] = e^{-\lambda} \lambda^2 e^{\lambda}$$

$$E[X(X-1)] = \lambda^2$$

Now, we use the identity $E[X^2] = E[X(X-1)] + E[X]$:

$$E[X^2] = \lambda^2 + \lambda$$

Finally, we can compute the variance:

$$Var(X) = E[X^2] - (E[X])^2$$

$$Var(X) = (\lambda^2 + \lambda) - (\lambda)^2$$

$$Var(X) = \lambda^2 + \lambda - \lambda^2$$

$$Var(X) = \lambda$$

A remarkable property of the Poisson distribution is that its variance is equal to its mean, both being $\lambda$. This mean-variance equality is a key characteristic that is often tested when determining if a dataset can be modeled by a Poisson distribution. If a dataset shows significant over-dispersion (variance much greater than mean) or under-dispersion (variance much less than mean), a Poisson model might not be appropriate.

### Cumulative Distribution Function (CDF)

The CDF, $F_X(k) = P(X \le k)$, for a Poisson random variable is given by the sum of its PMF values up to $k$:

$$F_X(k) = \sum_{i=0}^{k} \frac{\lambda^i e^{-\lambda}}{i!}$$

Similar to the Bernoulli and Binomial distributions, the Poisson CDF is a step function. Each step occurs at an integer value $k$, and the height of the step corresponds to $P(X=k)$. The CDF accumulates the probabilities from $X=0$ up to $k$, eventually approaching 1 as $k \to \infty$. This function is essential for calculating probabilities over intervals, such as $P(a < X \le b) = F_X(b) - F_X(a)$. There is no simple closed-form expression for the Poisson CDF, so it's usually calculated by summing terms or using computational tools.

### Example and Analogy

**Example**: A call center receives an average of 5 calls per hour. Assuming the number of calls follows a Poisson distribution, what is the probability of receiving exactly 3 calls in the next hour?
Here, $\lambda=5$ (average calls per hour), and we want $k=3$ calls.

*   $X \sim \text{Poisson}(5)$
*   PMF: $P(X=3) = \frac{5^3 e^{-5}}{3!}$
    $P(X=3) = \frac{125 \cdot e^{-5}}{6}$
    $P(X=3) \approx \frac{125 \cdot 0.0067379}{6} \approx 0.14037$

*   Mean: $E[X] = \lambda = 5$. We expect 5 calls per hour.
*   Variance: $Var(X) = \lambda = 5$.

**Analogy**: Think of the Poisson distribution as counting the number of unpredictable occurrences in a given window, like the number of typos on a page of a book, or the number of meteors observed in an hour during a meteor shower.

---

# Relationships and Approximations

Understanding the relationships between these distributions provides deeper insights into their applicability and allows for useful approximations in practical scenarios.

## Binomial to Poisson Approximation

As we discussed during the derivation of the Poisson PMF, the Poisson distribution can be seen as a special limiting case of the Binomial distribution. This relationship is not just a mathematical curiosity; it's a powerful tool for approximation, especially when direct Binomial calculations become cumbersome.

**Conditions for Approximation:**
A Binomial distribution $B(n, p)$ can be accurately approximated by a Poisson distribution $\text{Poisson}(\lambda)$ when:
1.  The number of trials $n$ is very large ($n \to \infty$).
2.  The probability of success $p$ is very small ($p \to 0$).
3.  The product $np$ is moderate and remains relatively constant, where $\lambda = np$.

**Intuition:**
Consider a situation where we have many opportunities for an event to occur (large $n$), but each individual event is very unlikely (small $p$). For example, the number of typographical errors on a page of a large book. There are many words (large $n$), but the probability of any single word having a typo is very small (small $p$). The total number of typos on the page would then be well-approximated by a Poisson distribution with $\lambda$ being the average number of typos per page. Each word is a Bernoulli trial, and the page is a collection of many such trials where success (typo) is rare.

This approximation is extremely useful because calculating Binomial probabilities for large $n$ can be computationally intensive due to the factorials involved (e.g., $n!$ for large $n$). The Poisson PMF, with its single parameter $\lambda$, often provides a much simpler and sufficiently accurate calculation, making it a valuable shortcut in many real-world applications.

**Rule of Thumb for Practical Use:**
The approximation is generally considered good when:
*   $n \ge 20$ and $p \le 0.05$
*   Even better when $n \ge 100$ and $np \le 10$

**Example Demonstrating Approximation:**
Suppose a manufacturer produces light bulbs, and the defect rate is $p=0.01$. If we inspect a batch of $n=200$ bulbs, what is the probability of finding exactly 3 defective bulbs?

1.  **Exact Binomial Calculation:**
    This is a Binomial problem: $X \sim \text{Binomial}(200, 0.01)$.
    $$P(X=3) = \binom{200}{3} (0.01)^3 (0.99)^{197}$$
    $$P(X=3) = \frac{200!}{3!197!} (0.01)^3 (0.99)^{197}$$
    Calculating this exactly requires a scientific calculator or software and yields approximately $0.1808$.

2.  **Poisson Approximation:**
    First, we check the conditions: $n=200$ (large) and $p=0.01$ (small).
    Calculate $\lambda = np = 200 \cdot 0.01 = 2$.
    Now, approximate with $Y \sim \text{Poisson}(2)$:
    $$P(Y=3) = \frac{e^{-2} 2^3}{3!} = \frac{e^{-2} \cdot 8}{6}$$
    $$P(Y=3) \approx \frac{0.135335 \cdot 8}{6} \approx 0.1804$$

As we can see, the Poisson approximation ($0.1804$) provides a very close estimate to the exact Binomial probability ($0.1808$), significantly simplifying the calculation.

---

## When to Use Poisson vs. Binomial

It's helpful to distinguish when to apply the Binomial versus the Poisson distribution:

*   **Binomial Distribution**: Used when we have a *fixed number of trials* ($n$) and are counting the number of "successes" among them. The probability of success $p$ is constant for each trial. The outcomes are binary (success/failure).
    *   *Example*: Number of defective items in a batch of 100.
*   **Poisson Distribution**: Used when we are counting the number of events in a *fixed interval of time or space*, and we know the average rate ($\lambda$) at which these events occur. There isn't a predefined "number of trials" in the same way as the Binomial; rather, there are infinitely many "opportunities" for the event to occur, but each with an infinitesimally small probability. It's often associated with rare events.
    *   *Example*: Number of emails received in a day.

As we saw in the derivation, the Poisson distribution can be viewed as an approximation to the Binomial distribution when $n$ is very large and $p$ is very small, such that $np$ (which is $\lambda$) remains constant. This is why the Poisson is often called the "law of rare events."

---

# Diagnostic Checks and Real-World Considerations

Successfully applying these discrete distributions in data science involves more than just understanding their formulas; it requires careful consideration of their underlying assumptions and how well they fit observed data.

## Goodness-of-Fit and Assumption Verification

Once a distribution (Bernoulli, Binomial, or Poisson) has been hypothesized and its parameters estimated from data, it's crucial to perform **goodness-of-fit tests** to assess whether the model adequately describes the observed phenomenon.
*   **Graphical Methods:** Visual comparisons, such as plotting observed frequencies against the probabilities predicted by the fitted PMF, or comparing empirical CDFs to theoretical CDFs, can reveal discrepancies. For count data, a bar chart of observed counts versus expected counts (from the fitted distribution) is a simple yet powerful diagnostic tool.
*   **Formal Statistical Tests:** The **Chi-squared goodness-of-fit test** is a common method to statistically evaluate if the observed counts in different categories (or counts of successes for Binomial, or event counts for Poisson) deviate significantly from the counts expected under the assumed distribution.
*   **Assumption Checks:** Beyond statistical fit, verifying the core assumptions is paramount. For Binomial, this includes checking if the probability of success $p$ is truly constant across trials and if trials are independent (e.g., using run charts or autocorrelation analysis). For Poisson, verifying that events are independent and occur at a constant average rate $\lambda$ is critical. Violations of these assumptions often necessitate alternative modeling approaches.

## Overdispersion in Poisson Data

A common challenge with real-world count data modeled by the Poisson distribution is **overdispersion**. This occurs when the observed variance of the data is significantly greater than its mean ($Var(X) > E[X]$), which directly contradicts the Poisson property $E[X] = Var[X] = \lambda$. Overdispersion typically arises from unobserved heterogeneity in the event rate across observations, or from positive correlation between events. When overdispersion is present and not accounted for, a Poisson model will underestimate standard errors, leading to artificially narrow confidence intervals and potentially erroneous conclusions about statistical significance. In such cases, the **Negative Binomial distribution** is often a more appropriate alternative, as it includes an additional parameter to model this extra variability, offering a more flexible fit to overdispersed count data. Conversely, *underdispersion* (variance less than mean) is rarer but can occur due to highly regular event patterns.

---

# Summary and Conclusion

We have taken a comprehensive journey through three cornerstone discrete probability distributions: Bernoulli, Binomial, and Poisson.

*   The **Bernoulli distribution** is the simplest, modeling a single trial with two outcomes (success/failure) and parameterized by the probability of success, $p$ (dimensionless). It forms the fundamental building block for more complex models, with $X \sim \text{Bernoulli}(p)$ being equivalent to $X \sim \text{Binomial}(1, p)$. We derived its mean as $p$ and variance as $p(1-p)$.
*   The **Binomial distribution** extends the Bernoulli concept to a fixed number of independent trials, counting the number of successes. Its parameters are the number of trials, $n$ (dimensionless count), and the probability of success in each trial, $p$ (dimensionless probability). We saw how its PMF arises from combinatorial arguments and how linearity of expectation simplifies its mean ($np$) and variance ($np(1-p)$) derivations.
*   The **Poisson distribution** excels at modeling the number of rare events occurring in a fixed interval, given a constant average rate $\lambda$ (dimensionless count). Its elegant PMF and the intriguing property of having equal mean and variance ($\lambda$) make it invaluable for a wide array of applications. Crucially, we derived the Poisson distribution as a limiting case of the Binomial distribution, providing a powerful approximation tool when $n$ is large and $p$ is small.

For all three distributions, we explored their Cumulative Distribution Functions (CDFs), highlighting their role as step functions that accumulate probability and are essential for calculating probabilities over intervals. The factorial terms in their PMFs naturally define their discrete, integer domains.

### Advanced Applications in Data Science

These distributions form the bedrock for many advanced statistical and machine learning techniques, providing the likelihood functions for various models:
*   **A/B Testing:** The Binomial distribution is central to A/B testing, where we compare conversion rates (probabilities of success) between two or more groups to determine which performs better.
*   **Logistic Regression:** For binary classification tasks, the underlying statistical model is often a **Generalized Linear Model (GLM)** that assumes a Bernoulli (or Binomial, if data are grouped) distribution for the response variable, linking the probability of success to predictor variables via a logit function.
*   **Poisson Regression:** For modeling count data (e.g., number of website clicks, customer arrivals, disease cases), **Poisson Regression** is a standard GLM where the response variable is assumed to follow a Poisson distribution, allowing us to quantify the effect of various factors on the expected event count.
*   **Anomaly Detection:** Poisson distributions can be used to model expected counts of rare events; significant deviations from these expected counts might signal an anomaly or unusual activity (e.g., an unexpected surge in network traffic).

### Beyond These Basics: Related Distributions

While these three distributions are fundamental, it's worth noting that they are part of a larger family of discrete probability distributions, each suitable for specific scenarios. For instance, the **Geometric distribution** models the number of Bernoulli trials needed to get the *first* success, while the **Negative Binomial distribution** generalizes this to the number of trials needed for $r$ successes (and, as noted, also serves as an important alternative to the Poisson distribution for overdispersed count data). Understanding these foundational distributions is the gateway to exploring these more specialized models and building sophisticated probabilistic reasoning skills.

These distributions are not just abstract mathematical constructs; they are practical tools for understanding, modeling, and predicting discrete phenomena in various fields, from engineering and finance to biology and social sciences. By understanding their assumptions, PMFs, CDFs, properties, and interrelationships, we are better equipped to analyze count data and make informed decisions based on probabilistic reasoning.








# Poisson Distribution

## What is the Poisson Distribution?

The **Poisson distribution** is a discrete probability distribution that models the number of times an event occurs in a fixed interval of time or space, given that these events occur with a known constant average rate and independently of the time since the last event. It is particularly useful for modeling rare events that happen randomly and infrequently.

Imagine we are counting occurrences of something over a continuous period or region. The Poisson distribution helps us answer questions like:
*   How many emails will we receive in the next hour?
*   How many customers will arrive at a service desk in a 15-minute period?
*   How many defects will we find on a 10-square-meter sheet of metal?
*   How many car accidents will occur on a particular stretch of highway in a week?

In all these scenarios, we are counting discrete events (emails, customers, defects, accidents) within a defined continuous interval (hour, 15-minute period, 10 sq-meter, week). The key is that we are looking for the *number* of events, not the outcome of a fixed number of trials (like in the Binomial distribution).

## Conditions for a Poisson Process

A random variable $X$ is said to follow a Poisson distribution if the events it counts occur according to a **Poisson process**. A Poisson process is a stochastic process characterized by the following conditions:

1.  **Independence of Events**: The occurrence of an event in one interval (or region) does not affect the probability of an event occurring in any other disjoint interval (or region). For example, one customer arriving at a bank does not make it more or less likely for the next customer to arrive.
2.  **Constant Average Rate**: The average rate at which events occur is constant over the entire interval of interest. This underlying rate, often denoted by $r$, does not change with time or location.
3.  **Small Probability of Multiple Events**: In any very small sub-interval of length $\Delta t$, the probability of exactly one event occurring is approximately $r \Delta t$, while the probability of *more than one* event occurring is negligible (i.e., approaches zero faster than $\Delta t$). This implies that events cannot occur simultaneously.
4.  **Events are Discrete**: We are counting whole, distinct events (e.g., 1 car, 2 defects, not 1.5 cars or 0.7 defects).

These conditions allow us to model phenomena ranging from radioactive decay to website traffic, where individual events are somewhat unpredictable but their average rate is stable.

## The Poisson Parameter $\lambda$ (Lambda)

The Poisson distribution is characterized by a single parameter, $\lambda$ (lambda).

*   $\lambda$ represents the **average number of events** expected to occur in the given fixed interval of time or space.
*   **Units of $\lambda$**: This is a crucial point that often causes confusion. The parameter $\lambda$ itself, as used directly in the Poisson PMF formula ($e^{-\lambda} \lambda^k / k!$), is a **dimensionless quantity** representing an expected count. It must be a positive real number ($\lambda > 0$).
    However, $\lambda$ is often calculated from an underlying **rate** $r$ of a Poisson process and the **length of the interval** $t$ (or area $A$, or volume $V$) over which we are counting events.
    For example, if the average rate of customers arriving is $r = 10 \frac{\text{customers}}{\text{hour}}$, and we are interested in a time interval of $t = 0.5 \text{ hours}$, then the Poisson parameter $\lambda$ for this specific interval is:

    $$\lambda = r \cdot t$$

    $$\lambda = \left(10 \frac{\text{customers}}{\text{hour}}\right) \cdot (0.5 \text{ hours}) = 5 \text{ customers}$$

    In this calculation, the units cancel, making $\lambda = 5$ a dimensionless expected count. The underlying rate $r$ is a physical quantity with units like $s^{-1}$, $m^{-1}$, $m^{-2}$, or $m^{-3}$ (in SI units, representing events per unit time, length, area, or volume). The interval length $t$ (or area $A$, or volume $V$) has corresponding SI units of seconds (s), meters (m), square meters ($m^2$), or cubic meters ($m^3$).

We denote a Poisson random variable $X$ with parameter $\lambda$ as $X \sim \text{Poisson}(\lambda)$. The possible values for $X$ are $k \in \{0, 1, 2, \ldots\}$, meaning there can be any non-negative integer number of events. The number of events $X$ itself is a dimensionless count.

## Probability Mass Function (PMF) of the Poisson Distribution

The Probability Mass Function (PMF) of a Poisson random variable $X$ gives the probability of observing exactly $k$ events in a given interval.

The PMF for a Poisson random variable is:

$$p_X(k) = P(X=k) = \frac{e^{-\lambda} \lambda^k}{k!} \quad \text{for } k \in \{0, 1, 2, \ldots\}$$

where:
*   $e$ is Euler's number (the base of the natural logarithm), approximately $2.71828$.
*   $\lambda$ is the average number of events in the interval (dimensionless).
*   $k$ is the actual number of events we are interested in (dimensionless count).
*   $k!$ is the factorial of $k$ ($k! = k \times (k-1) \times \dots \times 2 \times 1$), and $0! = 1$.

Let's check the units for consistency: $e^{-\lambda}$ is dimensionless because the exponent of $e$ must always be dimensionless. $\lambda^k$ is dimensionless since $\lambda$ is dimensionless. $k!$ is dimensionless. Therefore, the entire PMF $P(X=k)$ is dimensionless, as expected for a probability.

### Derivation of the PMF from the Binomial Distribution

One of the most elegant ways to understand the Poisson PMF is to derive it as a limiting case of the Binomial distribution. We previously discussed that the Poisson distribution can approximate the Binomial distribution when the number of trials $n$ is very large and the probability of success $p$ is very small.

Let's consider a Binomial random variable $X \sim B(n, p)$ representing the number of successes in $n$ trials. Its PMF is:

$$P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$$

Now, let's impose the conditions for the Poisson approximation:
1.  The number of trials $n \to \infty$.
2.  The probability of success $p \to 0$.
3.  The product $np$ remains constant and finite. We define this constant product as $\lambda$, so $\lambda = np$. This implies $p = \lambda/n$.

Substitute $p = \lambda/n$ into the Binomial PMF:

$$P(X=k) = \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k}$$

Let's expand the binomial coefficient $\binom{n}{k} = \frac{n!}{k!(n-k)!}$:

$$P(X=k) = \frac{n!}{k!(n-k)!} \frac{\lambda^k}{n^k} \left(1-\frac{\lambda}{n}\right)^{n-k}$$

We can rearrange the terms to group factors that will simplify in the limit:

$$P(X=k) = \frac{\lambda^k}{k!} \cdot \frac{n!}{(n-k)! n^k} \cdot \left(1-\frac{\lambda}{n}\right)^{n-k}$$

Now, we take the limit as $n \to \infty$:

1.  Consider the term $\frac{n!}{(n-k)! n^k}$:
    $$ \lim_{n \to \infty} \frac{n(n-1)(n-2)\dots(n-k+1)}{n^k} $$
    We can rewrite this as:
    $$ = \lim_{n \to \infty} \left(\frac{n}{n}\right) \left(\frac{n-1}{n}\right) \dots \left(\frac{n-k+1}{n}\right) $$
    $$ = \lim_{n \to \infty} \left(1\right) \left(1-\frac{1}{n}\right) \dots \left(1-\frac{k-1}{n}\right) $$
    As $n \to \infty$, each factor $(1 - j/n)$ approaches 1. So, this entire term approaches $1^k = 1$.

2.  Consider the term $\left(1-\frac{\lambda}{n}\right)^{n-k}$:
    We can split this into two parts: $\left(1-\frac{\lambda}{n}\right)^n$ and $\left(1-\frac{\lambda}{n}\right)^{-k}$.
    *   For the first part, we use the fundamental limit definition of $e^x$: $\lim_{n \to \infty} \left(1+\frac{x}{n}\right)^n = e^x$.
        $$ \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^n = e^{-\lambda} $$
    *   For the second part, as $n \to \infty$, $\frac{\lambda}{n} \to 0$:
        $$ \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{-k} = (1-0)^{-k} = 1^{-k} = 1 $$

Combining these limits, as $n \to \infty$, the Binomial PMF becomes:

$$ P(X=k) = \frac{\lambda^k}{k!} \cdot 1 \cdot e^{-\lambda} \cdot 1 $$

Thus, we arrive at the Poisson PMF:

$$P(X=k) = \frac{e^{-\lambda} \lambda^k}{k!}$$

This derivation beautifully illustrates how the Poisson distribution emerges when we observe rare events (small $p$) over a very large number of opportunities (large $n$), such that their average rate ($\lambda = np$) remains constant.

### Properties of a PMF (for Poisson)

1.  **Non-negativity**: Since $\lambda > 0$, $e^{-\lambda} > 0$, $\lambda^k > 0$, and $k! > 0$ for $k \ge 0$, it follows that $P(X=k) \ge 0$ for all $k \ge 0$.
2.  **Normalization**: The sum of all probabilities for all possible values of $k$ must equal 1.

    $$ \sum_{k=0}^{\infty} P(X=k) = \sum_{k=0}^{\infty} \frac{e^{-\lambda} \lambda^k}{k!} $$
    We can factor out $e^{-\lambda}$ as it does not depend on $k$:
    $$ = e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} $$
    Recall the Maclaurin series expansion for $e^x$:
    $$ e^x = \sum_{k=0}^{\infty} \frac{x^k}{k!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots $$
    If we let $x=\lambda$, then $\sum_{k=0}^{\infty} \frac{\lambda^k}{k!} = e^{\lambda}$.
    Substituting this back into our sum:
    $$ = e^{-\lambda} \cdot e^{\lambda} = e^0 = 1 $$
    Both properties hold, confirming the Poisson PMF is valid.

## Expected Value (Mean) of the Poisson Distribution

The expected value or mean of a Poisson random variable $X \sim \text{Poisson}(\lambda)$ is:

$$E[X] = \lambda$$

This is quite intuitive, as $\lambda$ was defined as the average number of events in the given interval. Since $X$ is a dimensionless count, its expected value $E[X]$ is also a dimensionless count.

### Derivation of the Expected Value

We use the definition of expected value for a discrete random variable:

$$E[X] = \sum_{k=0}^{\infty} k \cdot P(X=k)$$

Substitute the Poisson PMF:

$$E[X] = \sum_{k=0}^{\infty} k \frac{e^{-\lambda} \lambda^k}{k!}$$

The term for $k=0$ is $0 \cdot \frac{e^{-\lambda} \lambda^0}{0!} = 0$. So, we can start the summation from $k=1$:

$$E[X] = e^{-\lambda} \sum_{k=1}^{\infty} k \frac{\lambda^k}{k!}$$

For $k \ge 1$, we can simplify $k/k! = k / (k \cdot (k-1)!) = 1/(k-1)!$.

$$E[X] = e^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^k}{(k-1)!}$$

Now, let's factor out one $\lambda$ from $\lambda^k$: $\lambda^k = \lambda \cdot \lambda^{k-1}$.

$$E[X] = e^{-\lambda} \lambda \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}$$

Let $j = k-1$. As $k$ goes from $1$ to $\infty$, $j$ goes from $0$ to $\infty$.

$$E[X] = e^{-\lambda} \lambda \sum_{j=0}^{\infty} \frac{\lambda^j}{j!}$$

We recognize the sum as the Maclaurin series for $e^{\lambda}$: $\sum_{j=0}^{\infty} \frac{\lambda^j}{j!} = e^{\lambda}$.

$$E[X] = e^{-\lambda} \lambda e^{\lambda}$$

$$E[X] = \lambda e^0 = \lambda \cdot 1 = \lambda$$

This confirms that the mean of a Poisson distribution is indeed $\lambda$.

## Variance of the Poisson Distribution

The variance of a Poisson random variable $X \sim \text{Poisson}(\lambda)$ is also:

$$Var(X) = \lambda$$

It's a unique and important property of the Poisson distribution that its mean and variance are equal. The variance is also a dimensionless quantity.

### Derivation of the Variance

We use the computationally convenient formula for variance: $Var(X) = E[X^2] - (E[X])^2$.
We already know $E[X] = \lambda$, so $(E[X])^2 = \lambda^2$. We need to find $E[X^2]$.

A common trick for this is to first calculate $E[X(X-1)]$.

$$E[X(X-1)] = \sum_{k=0}^{\infty} k(k-1) P(X=k)$$

Substitute the Poisson PMF:

$$E[X(X-1)] = \sum_{k=0}^{\infty} k(k-1) \frac{e^{-\lambda} \lambda^k}{k!}$$

The terms for $k=0$ ($0 \cdot (-1) \cdot P(X=0) = 0$) and $k=1$ ($1 \cdot 0 \cdot P(X=1) = 0$) are both zero. So we can start the summation from $k=2$:

$$E[X(X-1)] = e^{-\lambda} \sum_{k=2}^{\infty} k(k-1) \frac{\lambda^k}{k!}$$

For $k \ge 2$, we can simplify $k(k-1)/k! = k(k-1) / (k(k-1)(k-2)!) = 1/(k-2)!$.

$$E[X(X-1)] = e^{-\lambda} \sum_{k=2}^{\infty} \frac{\lambda^k}{(k-2)!}$$

Factor out $\lambda^2$ from $\lambda^k$: $\lambda^k = \lambda^2 \cdot \lambda^{k-2}$.

$$E[X(X-1)] = e^{-\lambda} \lambda^2 \sum_{k=2}^{\infty} \frac{\lambda^{k-2}}{(k-2)!}$$

Let $j = k-2$. As $k$ goes from $2$ to $\infty$, $j$ goes from $0$ to $\infty$.

$$E[X(X-1)] = e^{-\lambda} \lambda^2 \sum_{j=0}^{\infty} \frac{\lambda^j}{j!}$$

Again, the sum is the Maclaurin series for $e^{\lambda}$: $\sum_{j=0}^{\infty} \frac{\lambda^j}{j!} = e^{\lambda}$.

$$E[X(X-1)] = e^{-\lambda} \lambda^2 e^{\lambda}$$

$$E[X(X-1)] = \lambda^2$$

Now, we relate $E[X(X-1)]$ to $E[X^2]$:

$$E[X(X-1)] = E[X^2 - X] = E[X^2] - E[X]$$

So, we have:

$$\lambda^2 = E[X^2] - E[X]$$

We know $E[X] = \lambda$, so:

$$\lambda^2 = E[X^2] - \lambda$$

Solving for $E[X^2]$:

$$E[X^2] = \lambda^2 + \lambda$$

Finally, substitute this into the variance formula:

$$Var(X) = E[X^2] - (E[X])^2$$

$$Var(X) = (\lambda^2 + \lambda) - (\lambda)^2$$

$$Var(X) = \lambda^2 + \lambda - \lambda^2$$

$$Var(X) = \lambda$$

The derivation confirms that the variance of a Poisson distribution is indeed equal to its mean, $\lambda$.

## Standard Deviation of the Poisson Distribution

The standard deviation, $\sigma_X$, is the square root of the variance. It provides a measure of spread in the same units as the random variable itself (dimensionless counts).

$$\sigma_X = \sqrt{Var(X)}$$

For a Poisson random variable:

$$\sigma_X = \sqrt{\lambda}$$

## Mode of the Poisson Distribution

The **mode** of a discrete distribution is the value that occurs with the highest probability. For the Poisson distribution, the mode is closely related to $\lambda$.

*   If $\lambda$ is not an integer, the mode is $\lfloor \lambda \rfloor$.
*   If $\lambda$ is an integer, then there are two modes: $\lambda - 1$ and $\lambda$.

Here, $\lfloor \lambda \rfloor$ denotes the **floor function**, which gives the largest integer that is less than or equal to $\lambda$.

### Derivation of the Mode

To find the mode, we examine the ratio of consecutive probabilities, $\frac{P(X=k)}{P(X=k-1)}$, and determine when this ratio is greater than or equal to 1. The probability $P(X=k)$ increases as long as this ratio is $\ge 1$, and decreases when it's $< 1$.

$$ \frac{P(X=k)}{P(X=k-1)} = \frac{\frac{e^{-\lambda} \lambda^k}{k!}}{\frac{e^{-\lambda} \lambda^{k-1}}{(k-1)!}} $$

We can simplify this expression by canceling common terms:

$$ = \frac{e^{-\lambda} \lambda^k}{k!} \cdot \frac{(k-1)!}{e^{-\lambda} \lambda^{k-1}} $$

$$ = \frac{\lambda^k}{\lambda^{k-1}} \cdot \frac{(k-1)!}{k!} = \lambda \cdot \frac{1}{k} = \frac{\lambda}{k} $$

We are looking for the value of $k$ where the probability is greatest. This occurs when $P(X=k) \ge P(X=k-1)$, which means our ratio must be greater than or equal to 1:

$$ \frac{\lambda}{k} \ge 1 $$

Multiplying both sides by $k$ (which must be positive since $k \ge 0$):

$$ \lambda \ge k \quad \text{or equivalently} \quad k \le \lambda $$

This inequality tells us that the probability $P(X=k)$ continues to increase as long as $k$ is less than or equal to $\lambda$. The mode will be the largest integer value of $k$ that satisfies this condition.

**Handling the special cases:**
1.  **If $\lambda$ is not an integer**: For example, if $\lambda = 3.7$. The largest integer $k$ such that $k \le 3.7$ is $k=3$. For $k=4$, the inequality $4 \le 3.7$ is false, meaning $P(X=4) < P(X=3)$. Therefore, the probabilities increase up to $k=3$ and then start to decrease, resulting in a single, unique mode at $\lfloor \lambda \rfloor$.
2.  **If $\lambda$ is an integer**: For example, if $\lambda = 5$. For $k=5$, the inequality $5 \le 5$ is true, and the ratio $\frac{\lambda}{k} = \frac{5}{5} = 1$. This implies that $P(X=5) = P(X=4)$. For any $k > 5$, the ratio $\frac{\lambda}{k} < 1$, so the probabilities decrease. Therefore, if $\lambda$ is an integer, there are two modes: $\lambda-1$ and $\lambda$. Both values have the same maximum probability.

## Cumulative Distribution Function (CDF) of the Poisson Distribution

The CDF $F_X(x)$ for a Poisson random variable $X \sim \text{Poisson}(\lambda)$ gives the probability that the number of events is less than or equal to $x$. As with other discrete distributions, it is found by summing the probabilities from the PMF for all possible values of events up to $x$.

$$F_X(x) = P(X \le x) = \sum_{k=0}^{\lfloor x \rfloor} P(X=k) = \sum_{k=0}^{\lfloor x \rfloor} \frac{e^{-\lambda} \lambda^k}{k!}$$

where $\lfloor x \rfloor$ is the floor function.

Since this involves a summation, there is generally no simple closed-form expression for the Poisson CDF. We typically rely on tables or computational software to calculate Poisson cumulative probabilities. Like all CDFs for discrete variables, it is a step function.

## Example: Customer Arrivals at a Store

Let's say, based on historical data, we know that customers arrive at a small convenience store at an average rate of $r = 10 \text{ customers per hour}$. We are interested in the number of customers arriving in a 30-minute period.

First, we need to adjust our average rate to match the desired interval. The time interval is $t = 30 \text{ minutes} = 0.5 \text{ hours}$.
The Poisson parameter $\lambda$ for this 30-minute period is:
$\lambda = r \cdot t = (10 \frac{\text{customers}}{\text{hour}}) \times (0.5 \text{ hours}) = 5 \text{ customers}$.
So, for this problem, $X \sim \text{Poisson}(5)$, where $\lambda = 5$ is a dimensionless expected count. The number of customers $X$ is also a dimensionless count.

*   **What is the probability that exactly 3 customers arrive in a 30-minute period?**

    We use the PMF with $k=3$ and $\lambda=5$:

    $$P(X=3) = \frac{e^{-5} 5^3}{3!}$$

    $$P(X=3) = \frac{e^{-5} \cdot 125}{6}$$

    Using $e^{-5} \approx 0.006738$:

    $$P(X=3) \approx \frac{0.006738 \cdot 125}{6} \approx \frac{0.84225}{6} \approx 0.140375$$

    So, there's about a 14.04% chance that exactly 3 customers will arrive in a 30-minute period.

*   **What is the expected number of customers in a 30-minute period?**

    $$E[X] = \lambda = 5 \text{ customers}$$

*   **What is the variance and standard deviation of the number of customers in a 30-minute period?**

    $$Var(X) = \lambda = 5$$

    $$\sigma_X = \sqrt{\lambda} = \sqrt{5} \approx 2.236$$

*   **What is the mode (most probable number) of customers?**

    Since $\lambda = 5$ is an integer, there are two modes: $\lambda - 1 = 5 - 1 = 4$ and $\lambda = 5$.
    This means that both 4 and 5 customers are equally likely to be the most frequent number of arrivals in a 30-minute period.

## Further Insights and Advanced Considerations

### Shape of the Poisson Distribution

The visual shape of the Poisson PMF is entirely determined by its parameter $\lambda$.
*   For small values of $\lambda$ (e.g., $\lambda < 5$), the distribution is typically **right-skewed**, meaning it has a longer tail extending towards higher counts. The mode is often near the smallest possible counts.
*   As $\lambda$ increases, the Poisson distribution becomes progressively more **symmetrical** and bell-shaped. For larger $\lambda$ (e.g., $\lambda \ge 10$ or $20$), its shape closely approximates that of a Normal (Gaussian) distribution, which underlies the usefulness of the Normal approximation for large $\lambda$.

### Additivity of Poisson Random Variables

A very useful property of Poisson distributions is their additivity. If $X_1$ and $X_2$ are independent Poisson random variables with parameters $\lambda_1$ and $\lambda_2$ respectively, then their sum $Y = X_1 + X_2$ is also a Poisson random variable with parameter $\lambda_1 + \lambda_2$.

$$ \text{If } X_1 \sim \text{Poisson}(\lambda_1) \text{ and } X_2 \sim \text{Poisson}(\lambda_2) \text{ (independent)}, $$
$$ \text{then } X_1 + X_2 \sim \text{Poisson}(\lambda_1 + \lambda_2) $$

This property makes intuitive sense: if we have an average of $\lambda_1$ events in interval 1 and $\lambda_2$ events in interval 2, then the average number of events in the combined interval (or combined types of events) is simply $\lambda_1 + \lambda_2$. This applies to both combining disjoint intervals of time/space or combining different types of events happening simultaneously. For example, if calls arrive at a call center at rate $r_1$ and emails at rate $r_2$, the total rate of communications is $r_1+r_2$, leading to a combined Poisson process with parameter $\lambda_1+\lambda_2$ for a given interval.

### Relationship to the Exponential and Gamma Distributions

The Poisson distribution is deeply connected to two important continuous probability distributions that arise from the same underlying Poisson process:

1.  **Exponential Distribution**: While the Poisson distribution models the *number of events* in a fixed interval, the **Exponential distribution** models the *time between consecutive events* in a Poisson process. If events occur according to a Poisson process with rate $r$ (e.g., $r \text{ events/second}$), then the time until the next event (or the time between any two consecutive events) follows an Exponential distribution with rate parameter $r$. This connection highlights the deep relationship between counting discrete events and measuring continuous time intervals in stochastic processes.
2.  **Gamma Distribution**: Extending this, the **Gamma distribution** models the *waiting time until the $k$-th event* in a Poisson process. If the time between consecutive events is Exponentially distributed with rate $r$, then the sum of $k$ such independent exponential waiting times (i.e., the total time until the $k$-th event) follows a Gamma distribution. This completes a powerful triumvirate of distributions used to model various aspects of event occurrences and waiting times in a Poisson process.

### Memorylessness of the Poisson Process

The underlying **Poisson process** exhibits a fascinating property known as **memorylessness**. This means that the number of events occurring in any future interval is independent of the number of events that have occurred in any past disjoint interval. More profoundly, if we consider the waiting time until the next event in a Poisson process (which, as we discussed, follows an Exponential distribution), the fact that no event has occurred for a certain duration does not change the probability distribution of how much longer we have to wait for the next event. In simpler terms, the process "forgets" its history; the probability of an event happening in the next tiny moment is always the same, regardless of when the last event occurred. This mathematical property is fundamental to the construction and analysis of Poisson processes and the Exponential distribution.

### Limitations of the Poisson Distribution

Despite its versatility, the Poisson distribution has limitations that are important to acknowledge:
*   **Constant Rate Assumption**: It assumes a constant average rate $\lambda$ (or $r$). If the rate of events changes over time (e.g., more customers during peak hours or higher defect rates at certain stages of production), a single Poisson distribution may not be appropriate for the entire observation period. We might need to model different intervals with different $\lambda$ values or use more advanced non-homogeneous Poisson processes.
*   **Independence Assumption**: It assumes events occur independently. If events tend to cluster (e.g., after one defect, others are more likely due to a machine fault), or are inhibited by prior events, the Poisson model might not fit well.
*   **Mean Equals Variance (Over/Underdispersion)**: As discussed, a defining characteristic of the Poisson distribution is that its mean and variance are equal. However, real-world count data frequently exhibits **overdispersion**, where the observed variance is greater than the mean. This often arises from unobserved heterogeneity in the population, positive correlation between events, or omitted covariates. When overdispersion is present, applying a standard Poisson model can lead to underestimated standard errors and an inflated rate of Type I errors (false positives) in hypothesis testing. In such cases, alternative models like the **Negative Binomial distribution** are often more appropriate, as they introduce an additional dispersion parameter to model the variance independently of the mean, allowing for greater flexibility in handling overdispersed data. Conversely, **underdispersion** (variance < mean) is less common but also violates the Poisson assumption.
*   **Zero-Inflation**: Many count datasets, particularly in fields like economics or public health, exhibit an excessive number of zero counts compared to what a standard Poisson distribution would predict. For example, the number of doctor visits in a year might have many zeros for healthy individuals, but non-zero counts for others. **Zero-inflated Poisson (ZIP)** or **Hurdle models** are specialized statistical models designed to explicitly account for this excess of zeros.

### Goodness-of-Fit and Diagnostics

In practical data analysis, after hypothesizing that count data follows a Poisson distribution, it is crucial to perform **goodness-of-fit tests** to validate this assumption. Key diagnostic checks for Poisson models include:
*   **Comparing Sample Mean and Variance**: The most fundamental check for Poisson data is to compare the empirical sample mean and sample variance. If the sample variance is significantly larger than the sample mean, it indicates **overdispersion**, suggesting that the Poisson model might not be appropriate. Conversely, if the variance is much smaller than the mean, it indicates **underdispersion**, though this is less common with real-world count data.
*   **Chi-Squared Goodness-of-Fit Test**: This statistical test can formally assess whether the observed frequencies of counts align with the frequencies predicted by a Poisson distribution with the estimated $\lambda$.
*   **Residual Analysis**: For Poisson regression models (an extension for modeling count data with predictors), examining residuals can reveal patterns or deviations from the Poisson assumptions.
*   **Visualization**: Plotting a histogram of the observed counts and superimposing the theoretical Poisson PMF can provide an intuitive visual assessment of fit.

### Practical Data Science Applications

Beyond the theoretical understanding, the Poisson distribution is a workhorse in various data science domains:
*   **A/B Testing**: When analyzing discrete event counts (e.g., website clicks, conversions, sign-ups) on websites or applications, Poisson models are frequently used to compare the performance of different versions (A vs. B) over a fixed period.
*   **Anomaly Detection**: Unusual spikes or drops in event counts (e.g., server requests, failed logins, network intrusions) can be identified by modeling expected counts with a Poisson distribution and flagging observations that fall far outside the expected range as potential anomalies.
*   **Queueing Theory**: Essential for optimizing service systems (e.g., call centers, checkout lines), Poisson processes model customer arrivals, while exponential distributions model service times, enabling predictions of waiting times and resource allocation to improve efficiency.
*   **Epidemiology**: Counting rare disease occurrences or adverse events in populations to understand incidence rates and inform public health interventions.

The Poisson distribution is a cornerstone of probability and statistics, offering a powerful tool for understanding and modeling count data across a vast range of scientific and practical applications.