{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1ac570",
   "metadata": {},
   "source": [
    "# Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "\n",
    "Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are both statistical tools used for **model selection**. They help you choose the best model from a set of candidate models by balancing two competing goals: **goodness of fit** and **model complexity**.\n",
    "\n",
    "Both criteria are particularly useful when comparing models that may have different numbers of parameters, as they penalize models for being more complex to prevent **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbf17b",
   "metadata": {},
   "source": [
    "## Akaike Information Criterion (AIC)\n",
    "\n",
    "AIC is an estimator of prediction error and is rooted in information theory. It estimates the relative amount of information a model loses when representing the process that generated the data. The model with the lowest AIC value is considered the best among the candidate models. \n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$AIC = -2 \\ln(L) + 2k$$\n",
    "\n",
    "* **$-2 \\ln(L)$**: This is the **goodness-of-fit term**. It's derived from the maximum likelihood ($L$) of the model, which measures how well the model fits the data. A higher likelihood (and thus a smaller negative log-likelihood) indicates a better fit.\n",
    "* **$2k$**: This is the **penalty term**. It's a penalty for model complexity, where $k$ is the number of parameters in the model. A more complex model (one with more parameters) gets a higher penalty.\n",
    "\n",
    "The AIC's goal is to find the model that best approximates the unknown data-generating process, even if that process isn't one of the candidate models. It's focused on **predictive accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3b3de",
   "metadata": {},
   "source": [
    "## Bayesian Information Criterion (BIC)\n",
    "\n",
    "BIC, also known as the Schwarz Information Criterion (SIC), is a criterion for model selection derived from a Bayesian perspective. Like AIC, it balances goodness of fit and complexity, but it does so more aggressively. The model with the lowest BIC value is the one preferred. \n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$BIC = -2 \\ln(L) + k \\ln(n)$$\n",
    "\n",
    "* **$-2 \\ln(L)$**: This is the same goodness-of-fit term as in AIC.\n",
    "* **$k \\ln(n)$**: This is the **penalty term**. The penalty for complexity is stronger than AIC's because it includes the natural logarithm of the number of data points ($n$).\n",
    "\n",
    "Because of the $\\ln(n)$ factor, BIC applies a much heavier penalty for additional parameters, especially as the sample size grows. This means BIC tends to favor **simpler models** more strongly than AIC. It assumes that one of the candidate models is the \"true\" model, and its goal is to find that true model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df0282",
   "metadata": {},
   "source": [
    "### Key Differences and When to Use Which\n",
    "\n",
    "| Feature | Akaike Information Criterion (AIC) | Bayesian Information Criterion (BIC) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Penalty for Complexity** | Penalizes with $2k$. | Penalizes with $k \\ln(n)$. |\n",
    "| **Focus** | Finds the best **approximating** model for predictive accuracy. | Finds the \"true\" model. |\n",
    "| **Sample Size** | The penalty is constant with respect to sample size. | The penalty increases with sample size, favoring simpler models. |\n",
    "| **Behavior** | Tends to select more complex models than BIC. | Tends to select more simpler models than AIC. |\n",
    "\n",
    "NOTE: The terms \"approximating model\" and \"true model\" refer to their underlying assumptions about the reality you are trying to model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6982e55e",
   "metadata": {},
   "source": [
    "**AIC assumes that the \"true\" data-generating process is infinitely complex and therefore unknowable.** We can only ever hope to approximate it with our models.\n",
    "\n",
    "* **Goal**: AIC's goal is to find the model that provides the best trade-off between bias (how far your model is from the true process) and variance (how much your model would change with different data). In simpler terms, it's about building the model that will give you the most accurate predictions on a new, unseen dataset.\n",
    "* **Best for**: This makes AIC ideal for tasks where **prediction** is the primary goal, such as forecasting future stock prices, predicting customer behavior, or machine learning applications. You're not trying to discover the fundamental laws of the universe, you're just trying to make the most useful prediction you can.\n",
    "* **Mathematical Justification**: AIC's penalty term ($2k$) is derived from the Kullback-Leibler (KL) divergence, which is a measure of the information lost when approximating reality with a given model. By minimizing AIC, you are minimizing this information loss.\n",
    "\n",
    "**BIC assumes that there is a \"true\" model that generated the data, and this model exists within your set of candidate models.** This is often a good assumption for scientific fields where we believe there are underlying, fixed physical laws governing a system.\n",
    "\n",
    "* **Goal**: BIC's goal is to select the model that is most likely to be this \"true\" model. Because of its heavier penalty for more parameters ($k\\ln(n)$), BIC is more likely to choose simpler, more parsimonious models. It essentially bets that the simplest model that explains the data well is the correct one.\n",
    "* **Best for**: This makes BIC more suitable for tasks of **explanation and discovery**, where you want to find the most fundamental, elegant, and concise model that describes a phenomenon. This is common in fields like physics, biology, and some social sciences.\n",
    "* **Mathematical Justification**: BIC is derived from Bayesian inference and is an approximation of the posterior probability of a model being the true model, given the data. By minimizing BIC, you are maximizing this posterior probability.\n",
    "\n",
    "In essence:\n",
    "\n",
    "* **AIC** asks: \"Which model will give me the most accurate predictions for the future?\"\n",
    "* **BIC** asks: \"Which model is most likely to be the true explanation for the data I have?\"\n",
    "\n",
    "This is why AIC often chooses a slightly more complex model than BIC. AIC is willing to accept a little extra complexity if it improves predictive accuracy, while BIC is more conservative, preferring a simpler model unless the evidence for a more complex one is overwhelming.\n",
    "\n",
    "In practice, if your goal is to find the model with the highest predictive power, AIC is often preferred. If your goal is to select the most efficient or fundamental model that is likely to be the \"true\" one, BIC might be a better choice.\n",
    "\n",
    "When you compare two models, you find AIC Difference $\\Delta \\text{AIC}$ and BIC Difference $\\Delta \\text{BIC}$ for these modesl. Here are some general guidelines for interpreting these differences, based on conventions in the scientific community:\n",
    "\n",
    "* **AIC Difference ($\\Delta \\text{AIC}$)**: A difference of more than 2 between two models is often considered significant. If the difference is between 0 and 2, the models are considered to have a similar level of support from the data. A model with a $\\Delta$AIC of 10 or more is considered to have very little support compared to the best model.\n",
    "* **BIC Difference ($\\Delta \\text{BIC}$)**: Because BIC has a stronger penalty for complexity, its differences are interpreted more stringently. A difference of 0-2 is considered as weak evidence for the model with the lower BIC, a difference of 2-6 is considered positive evidence, a difference of 6-10 is strong evidence, and a difference of more than 10 is considered very strong evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55128a",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a powerful and flexible method for estimating the parameters of a statistical model. It's used to find the parameter values that make the observed data most probable. The **likelihood function** quantifies how probable the observed data is for a given set of parameters. The **log-likelihood function** is simply the natural logarithm of the likelihood function.\n",
    "\n",
    "The core idea of MLE is to find the parameter values that maximize the likelihood function, $L(\\theta|x)$, where $\\theta$ represents the parameters and $x$ is the data. The likelihood function is often a product of probability density functions (or probability mass functions) for each data point. For a set of independent and identically distributed (i.i.d.) observations, the likelihood is:\n",
    "\n",
    "$$L(\\theta|x) = \\prod_{i=1}^{n} f(x_i|\\theta)$$\n",
    "\n",
    "Working with this product can be difficult, especially with many data points, as multiplying many small numbers can lead to computational underflow. This is where the **log-likelihood** comes in. By taking the natural logarithm, the product is transformed into a sum, which is much more computationally stable and easier to differentiate.\n",
    "\n",
    "$$\\ln L(\\theta|x) = \\ln \\left( \\prod_{i=1}^{n} f(x_i|\\theta) \\right) = \\sum_{i=1}^{n} \\ln f(x_i|\\theta)$$\n",
    "\n",
    "Since the logarithm is a monotonic function, maximizing the log-likelihood function yields the **exact same parameter estimates** as maximizing the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d713e40",
   "metadata": {},
   "source": [
    "The connection between Maximum Likelihood and regression modeling becomes clear when we make assumptions about the distribution of the errors. While Ordinary Least Squares (OLS) regression minimizes the sum of squared residuals, MLE provides a more general framework that can lead to the same result under specific conditions.\n",
    "\n",
    "**For OLS:**\n",
    "The OLS method finds the coefficients that minimize the **sum of squared residuals** (or errors). That is, it minimizes the following:\n",
    "\n",
    "$$\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1x_i))^2$$\n",
    "\n",
    "**Relation to MLE:**\n",
    "The OLS solution is the Maximum Likelihood Estimate for a linear regression model **if we assume that the error terms ($\\epsilon_i$) are independently and identically distributed (i.i.d.) according to a normal distribution with a mean of zero and a constant variance ($\\sigma^2$).**\n",
    "\n",
    "When we make this assumption, the probability of observing a particular data point $y_i$ for a given $x_i$ is a normal distribution centered at the predicted value, $\\beta_0 + \\beta_1x_i$. The log-likelihood function for this model can be written as:\n",
    "\n",
    "$$\\ln L(\\beta_0, \\beta_1, \\sigma^2|x,y) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1x_i))^2$$\n",
    "\n",
    "NOTE: We will derive a similar formula below but in a bit more general form.\n",
    "\n",
    "Notice the last term in the equation. To maximize the log-likelihood function, we need to maximize this entire expression. The first term is a constant with respect to the $\\beta$ coefficients, and the second term is negative. Therefore, to make the overall value as large as possible, we must make the negative term as small as possible. This is equivalent to minimizing the **sum of squared residuals**, which is exactly what OLS does.\n",
    "\n",
    "**Why is this important?** This connection shows that OLS is not just an arbitrary method for fitting a line; it has a probabilistic foundation under the assumption of normally distributed errors. For other types of regression, such as logistic regression, there is no OLS equivalent. Instead, the parameters are always estimated using Maximum Likelihood, where the log-likelihood function is based on the appropriate distribution for the outcome variable (e.g., a Bernoulli distribution for binary outcomes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501ba8e",
   "metadata": {},
   "source": [
    "## Derivation of the Log-Likelihood for Least Squares\n",
    "\n",
    "Yes, there is a very useful formula for the log-likelihood ($\\ln(L)$) that we can use for our case. It's derived by making one key assumption: that the errors (the residuals) follow a normal distribution. This is the same underlying assumption that allows us to use least squares methods in the first place.\n",
    "\n",
    "Here is the derivation:\n",
    "\n",
    "1.  **Start with the probability of one data point.** The probability of observing a single data point $(x_i, y_i)$ given the model's prediction $f(x_i, \\beta)$ is described by a normal distribution with a mean of $f(x_i, \\beta)$ and a standard deviation of $\\sigma_i$. The probability density function (PDF) for a single point is:\n",
    "\n",
    "    $$p(y_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{(y_i - f(x_i, \\beta))^2}{2\\sigma_i^2}}$$\n",
    "\n",
    "2.  **Form the likelihood function.** Assuming each data point is an independent measurement, the total likelihood ($L$) of observing all $N$ data points is the product of their individual probabilities:\n",
    "\n",
    "    $$L = \\prod_{i=1}^{N} p(y_i) = \\prod_{i=1}^{N} \\left( \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{(y_i - f(x_i, \\beta))^2}{2\\sigma_i^2}} \\right)$$\n",
    "\n",
    "3.  **Take the natural logarithm.** To simplify the product and make the calculations more manageable, we take the natural logarithm of the likelihood function. A product becomes a sum, and the exponential term simplifies.\n",
    "\n",
    "    $$\\ln(L) = \\ln \\left( \\prod_{i=1}^{N} p(y_i) \\right) = \\sum_{i=1}^{N} \\ln(p(y_i))$$   \n",
    "    \n",
    "    $$\\ln(L) = \\sum_{i=1}^{N} \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{(y_i - f(x_i, \\beta))^2}{2\\sigma_i^2}} \\right)$$\n",
    "\n",
    "4.  **Simplify the expression.** Using logarithm rules ($\\ln(a \\cdot b) = \\ln(a) + \\ln(b)$ and $\\ln(e^x) = x$), we can expand the sum:\n",
    "\n",
    "    $$\\ln(L) = \\sum_{i=1}^{N} \\left( \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\right) - \\frac{(y_i - f(x_i, \\beta))^2}{2\\sigma_i^2} \\right)$$   \n",
    "    \n",
    "    $$\\ln(L) = - \\sum_{i=1}^{N} \\ln(\\sqrt{2\\pi\\sigma_i^2}) - \\frac{1}{2} \\sum_{i=1}^{N} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma_i^2}$$\n",
    "\n",
    "    $$\\ln(L) = - \\sum_{i=1}^{N} \\ln(\\sqrt{2\\pi\\sigma_i^2}) - \\frac{1}{2} \\sum_{i=1}^{N} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78829bd9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Final Formula\n",
    "\n",
    "The second term in this equation is directly related to our Chi-Squared ($\\chi^2$) statistic. Since $\\chi^2 = \\sum_{i=1}^{N} \\frac{(y_i - f(x_i, \\beta))^2}{\\sigma_i^2}$, we can rewrite the equation as:\n",
    "\n",
    "$$\\ln(L) = - \\frac{1}{2}\\chi^2 - \\sum_{i=1}^{N} \\ln(\\sqrt{2\\pi\\sigma_i^2})$$\n",
    "\n",
    "When comparing different models on the same dataset, the second term, which depends on the constant measurement uncertainties ($\\sigma_i$), is a constant and doesn't affect which model is selected. Therefore, for our purposes, we can write a simplified and more practical relationship:\n",
    "\n",
    "$$\\ln(L) \\propto - \\frac{1}{2}\\chi^2$$\n",
    "\n",
    "This means that maximizing the log-likelihood is equivalent to **minimizing the chi-squared statistic**.\n",
    "\n",
    "### How to Use This for AIC and BIC\n",
    "\n",
    "Since the AIC and BIC formulas depend on $-2\\ln(L)$, we can substitute our derived expression. When `curve_fit` finds the optimal parameters, it's minimizing the $\\chi^2$ value. This is the best-fit $\\chi^2$.\n",
    "\n",
    "So, for a model fitted with least squares, we can calculate the information criteria using the final $\\chi^2$ from the fit:\n",
    "\n",
    "* **For AIC:**\n",
    "    $$-2\\ln(L) = -2 \\left( - \\frac{1}{2}\\chi^2 + \\text{constant} \\right) = \\chi^2 + \\text{constant'}$$\n",
    "    Since the constants don't affect the model ranking, we can simply use:\n",
    "    $$AIC = \\chi^2 + 2k$$\n",
    "\n",
    "* **For BIC:**\n",
    "    $$BIC = \\chi^2 + k\\ln(N)$$\n",
    "\n",
    "Where $\\chi^2$ is the final chi-squared value calculated from the WLS fit, $k$ is the number of parameters, and $N$ is the number of data points. By calculating these values for different models fitted to the same data, you can choose the model with the lowest AIC or BIC as the one that provides the best balance of fit and parsimony."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30edae2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a93ed55",
   "metadata": {},
   "source": [
    "Excellent clarification question. You've hit on a crucial and often misunderstood point in statistical modeling, particularly in the context of regression.\n",
    "\n",
    "Let's break down your options. The correct interpretation is a combination of your second and third points, which are essentially two ways of stating the same core assumption.\n",
    "\n",
    "### The Correct Assumption: Normality of Errors\n",
    "\n",
    "You are correct with these two statements:\n",
    "\n",
    "*   **We expect that errors (the residuals) for each point follow a normal distribution.**\n",
    "*   **The probability of observing a single data point $$y_i$$ given the model's prediction $$f(x_i, \\beta)$$ is described by a normal distribution with a mean of $$f(x_i, \\beta)$$.**\n",
    "\n",
    "These two statements describe the same concept from slightly different angles. In a standard linear regression model, we model the relationship as:\n",
    "\n",
    "$$\n",
    "y_i = f(x_i, \\beta) + \\epsilon_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $$y_i$$ is the observed outcome for the i-th data point.\n",
    "- $$f(x_i, \\beta)$$ is the model's prediction for $$y_i$$ based on its input $$x_i$$ and parameters $$\\beta$$. This is the **mean** of the distribution for $$y_i$$.\n",
    "- $$\\epsilon_i$$ is the **error term** or residual for that data point.\n",
    "\n",
    "The fundamental assumption is that this error term, $$\\epsilon_i$$, is a random variable drawn from a normal distribution with a mean of 0 and a constant variance $$\\sigma^2$$. We write this as:\n",
    "\n",
    "$$\n",
    "\\epsilon_i \\sim N(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "This directly implies your third point: the observed value $$y_i$$ for a given $$x_i$$ is expected to follow a normal distribution centered around the model's prediction. We write this as:\n",
    "\n",
    "$$\n",
    "y_i | x_i \\sim N(f(x_i, \\beta), \\sigma^2)\n",
    "$$\n",
    "\n",
    "This means that if we could collect many $$y$$ values for the *exact same* $$x$$ value, those $$y$$ values would form a normal (bell-shaped) distribution around the line of best fit.\n",
    "\n",
    "### Why the First Option is Incorrect\n",
    "\n",
    "You asked: **\"Do we expect that all the data points should be on the normal distribution curve?\"**\n",
    "\n",
    "This is a common misconception. Here's why it's incorrect:\n",
    "1.  **Data points ($$x_i, y_i$$) are pairs:** They represent points in a 2D (or higher dimensional) space. A single normal distribution describes a single variable, not a relationship between variables.\n",
    "2.  **The dependent variable $$y$$ itself doesn't need to be normally distributed:** The assumption applies to the *residuals* ($$y_i - \\hat{y}_i$$), not the raw $$y$$ values. Your $$y$$ values could have any shape (e.g., be skewed or bimodal), but as long as the errors around the regression line are normally distributed, the assumption holds.\n",
    "\n",
    "In summary, when we use the log-likelihood formula for a normal distribution in a regression context, we are not assuming the raw data is normal. Instead, we are making a precise assumption that the **errors of our model's predictions are normally distributed.** This assumption is what allows us to calculate the likelihood of observing our data given the model, which is a necessary step for calculating AIC and BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbb029",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "893c6819",
   "metadata": {},
   "source": [
    "That first formula, $\\sigma^2 = \\frac{SSR}{n-k-1}$, is not quite right. The formula for the estimated error variance is a sample statistic, and it's more accurately represented by $s^2$, which is commonly called the **Mean Squared Error (MSE)**. The second formula is correct and provides the proper definition.\n",
    "\n",
    "---\n",
    "\n",
    "### What is the Mean Squared Error of Residuals ($MSE$)?\n",
    "\n",
    "The **Mean Squared Error of Residuals ($MSE$)** is the average squared difference between the observed values and the values predicted by the regression model. It's the unbiased estimator for the true error variance, $\\sigma^2$, in a linear regression model. \n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$MSE = s^2 = \\frac{SSR}{n - k - 1}$$\n",
    "\n",
    "Here's what each term means:\n",
    "\n",
    "* **$s^2$** or **$MSE$**: The estimated error variance.\n",
    "* **$SSR$ (Sum of Squared Residuals)**: The sum of the squared differences between the observed values ($y_i$) and the predicted values ($\\hat{y}_i$).\n",
    "    $$SSR = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "* **$n$**: The number of observations or data points.\n",
    "* **$k$**: The number of independent variables (predictors) in the model.\n",
    "* **$n - k - 1$**: The **degrees of freedom** for the residuals. This is the denominator because we lose one degree of freedom for the intercept and one for each predictor we estimate.\n",
    "\n",
    "---\n",
    "\n",
    "### Derivation and Intuition\n",
    "\n",
    "The formula isn't derived in the traditional sense of a mathematical proof but is based on the concept of an **unbiased estimator**.\n",
    "\n",
    "The goal is to estimate the true variance of the errors, $\\sigma^2$, which we don't know. The sum of squared residuals, $SSR$, is a good starting point, but simply dividing by $n$ would give us a biased estimate. The reason for this is that we use the sample data to estimate the regression coefficients (the slope and intercept), which forces the residuals to have a mean of zero. This constraint means the residuals are not truly independent.\n",
    "\n",
    "By dividing by the **degrees of freedom**, $n - k - 1$, instead of just $n$, we correct for this bias. The degrees of freedom represent the number of values in the final calculation of a statistic that are free to vary. In linear regression, once you've estimated the intercept and the slopes (a total of $k+1$ parameters), the values of the residuals are no longer entirely free to vary. Therefore, dividing by $n - k - 1$ provides an **unbiased estimator** of the true population variance, $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5509f15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bda4bc9",
   "metadata": {},
   "source": [
    "## Additional Materials\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "* https://www.quantstart.com/articles/Maximum-Likelihood-Estimation-for-Linear-Regression/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
