{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf22ae86",
   "metadata": {},
   "source": [
    "# Orthogonal Distance Regression\n",
    "\n",
    "Orthogonal Distance Regression (ODR) is a regression method that finds the best-fit line or curve by minimizing the sum of the squared **orthogonal distances** from the data points to the model. Unlike ordinary least squares (OLS) regression, which minimizes the vertical distances (errors in the y-direction), ODR accounts for errors in **both the x and y variables**.\n",
    "\n",
    "This makes ODR particularly useful when both independent and dependent variables have measurement errors. The \"orthogonal\" part refers to the fact that the distances are measured perpendicular to the fitted curve, not vertically or horizontally.\n",
    "\n",
    "## General Principles of ODR\n",
    "\n",
    "In a typical OLS regression, you assume that the independent variable ($x$) is known without error, and all the error is in the dependent variable ($y$). The goal is to minimize the sum of squared vertical distances from each data point $(x_i, y_i)$ to the fitted line $y = f(x_i)$.\n",
    "\n",
    "ODR, however, treats both $x$ and $y$ as having errors. The method involves finding a point $(\\hat{x}_i, \\hat{y}_i)$ on the curve $y = f(x)$ that is closest to the observed data point $(x_i, y_i)$. The \"distance\" that ODR minimizes is the perpendicular distance between the observed point $(x_i, y_i)$ and the point on the curve $(\\hat{x}_i, \\hat{y}_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fac6eb",
   "metadata": {},
   "source": [
    "The objective function that ODR minimizes is:\n",
    "\n",
    "$$\\sum_{i=1}^{n} \\left( w_{x,i}^2(x_i - \\hat{x}_i)^2 + w_{y,i}^2(y_i - \\hat{y}_i)^2 \\right)$$\n",
    "\n",
    "A more common and statistically grounded formulation, especially when using standard deviations, is to define the weights as the inverse of the variances:\n",
    "\n",
    "$$S = \\sum_{i=1}^{n} \\left( \\frac{(x_i - \\hat{x}_i)^2}{\\sigma_{x,i}^2} + \\frac{(y_i - \\hat{y}_i)^2}{\\sigma_{y,i}^2} \\right)$$\n",
    "\n",
    "where:\n",
    "* $S$ is the sum of squared weighted orthogonal distances.\n",
    "* $(x_i, y_i)$ are the observed data points.\n",
    "* $(\\hat{x}_i, \\hat{y}_i)$ are the corresponding points on the fitted curve.\n",
    "* The weights $w_{x,i}$ and $w_{y,i}$ are used to properly scale the errors in the objective function. They represent the inverse of the variance (the square of the standard deviation) of the errors for each variable.\n",
    "* $\\sigma_{x,i}$ and $\\sigma_{y,i}$ are the **standard deviations** of the errors for the $i$-th data point in the x and y directions, respectively.\n",
    "* The terms $\\frac{1}{\\sigma_{x,i}^2}$ and $\\frac{1}{\\sigma_{y,i}^2}$ are the actual weights used in the minimization. They represent the **inverse variance**, a standard way to weight observations in statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad1137",
   "metadata": {},
   "source": [
    "The core idea is that the regression should be more influenced by data points with smaller uncertainties and less by those with larger uncertainties.\n",
    "\n",
    "* **$w_{x,i}$ (x-axis weights):** A data point with a small error in the x-direction (a small standard deviation) will have a large $w_x$ value. This means the algorithm will penalize deviations in the x-direction more heavily, forcing the fitted curve to be closer to that point horizontally.\n",
    "* **$w_{y,i}$ (y-axis weights):** Similarly, a data point with a small error in the y-direction will have a large $w_y$ value. This pushes the curve to be closer to that point vertically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ff217",
   "metadata": {},
   "source": [
    "### ODR with Non-linear Functions\n",
    "\n",
    "ODR is especially powerful for non-linear regression because it can handle cases where both variables have errors and the relationship isn't a straight line. The process involves an iterative numerical optimization algorithm to find the parameters of the non-linear function that minimize the objective function described above.\n",
    "\n",
    "For your specific case with a non-linear function and errors for both the x-axis and y-axis, you will need to:\n",
    "\n",
    "1.  **Define your non-linear model function**, e.g., $y = f(x; \\beta_1, \\beta_2, ...)$ where $\\beta_i$ are the parameters you want to estimate.\n",
    "2.  **Provide your data** ($x_i, y_i$).\n",
    "3.  **Specify the errors** or weights for both variables.\n",
    "4.  **Use an ODR-specific software library or package** (e.g., `scipy.odr` in Python) to perform the regression. The solver will then iteratively adjust the parameters of your function until the sum of the squared orthogonal distances is minimized. ODR typically uses trust-region Levenberg-Marquardt algorithms.\n",
    "\n",
    "The output will be the best-fit parameters for your non-linear function, along with their standard errors, which can be used to assess the uncertainty of the estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
