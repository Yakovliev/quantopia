
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Logistic Regression (preview) &#8212; Quantopia&#39;:&#39; Physics, Python and Pi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'data-science/logistic-regression';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Quantopia':' Physics, Python and Pi - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Quantopia':' Physics, Python and Pi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Quantopia: Physics, Python, and Pi (Alpha Version)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">MATH</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/gradient-operator.html">Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/gradient-directional-derivative.html">Directional Derivative</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/divergence.html">Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/fourier-transform-01.html">Fourier Transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/least-squares-regression.html">Least Squares Regression, RSS, RMSE, R-squared</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/least-squares-regression-code.html">Least Squares Regression - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/ordinary-least-squares.html">Ordinary Least Squares (OLS) Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/ordinary-least-squares-code.html">Ordinary Least Squares (OLS) Regression - Code Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/variance-covariance.html">Variance and Covariance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/weighted-least-squares.html">Weighted Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/weighted-least-squares-code-1.html">WLS - Code Examples Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/weighted-least-squares-code-2.html">WLS - Code Examples Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/goodness-of-fit-and-chi-squared.html">Goodness of Fit and Chi-Squared Statistic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/aic-and-bic.html">Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/weighted-least-squares-code-3.html">WLS - Code Examples Part 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/orthogonal-distance-regression.html">Orthogonal Distance Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/01_basics/odr-code.html">ODR - Code Examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PHYSICS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../physics/continuity-equation-01.html">The Continuity Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/continuity-equation-02.html">The Continuity Equation: One-Dimensional Advection of a Density Profile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/ensemble.html">Statistical Ensembles and Liouville’s Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/microcanonical-ensemble.html">Microcanonical Ensemble</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/Yakovliev/quantopia/blob/main/book/data-science/logistic-regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia/issues/new?title=Issue%20on%20page%20%2Fdata-science/logistic-regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/data-science/logistic-regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Logistic Regression (preview)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-core-idea-modeling-probability">The Core Idea: Modeling Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-linear-part-the-log-odds">The Linear Part: The Log-Odds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-regression-model-equation">The Logistic Regression Model Equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-coefficients">Interpreting the Coefficients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary">Decision Boundary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-the-parameters-maximum-likelihood-estimation">Learning the Parameters: Maximum Likelihood Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-for-optimization">Gradient Descent for Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-examples">Simple Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-admitting-to-college">Example 1: Admitting to College</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-detecting-spam-email">Example 2: Detecting Spam Email</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional Materials</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="logistic-regression-preview">
<h1>Logistic Regression (preview)<a class="headerlink" href="#logistic-regression-preview" title="Link to this heading">#</a></h1>
<p>Logistic Regression is a powerful and widely used statistical method for binary classification problems. Despite its name, it’s a classification algorithm, not a regression algorithm. It models the probability of a binary outcome (e.g., spam/not spam, disease/no disease) using a logistic function.</p>
<section id="the-core-idea-modeling-probability">
<h2>The Core Idea: Modeling Probability<a class="headerlink" href="#the-core-idea-modeling-probability" title="Link to this heading">#</a></h2>
<p>Imagine you want to predict whether an email is spam based on certain features like the number of suspicious words, sender’s reputation, etc. Logistic Regression allows you to predict “spam” or “not spam” and to predict the <em>probability</em> that an email is spam, a value between 0 and 1. Based on this probability, you can then classify the email. For example, if the probability is greater than 0.5, you might classify it as spam.</p>
<p><strong>Why Not Linear Regression?</strong></p>
<p>You might wonder, why can’t we just use linear regression for this? If we were to use linear regression, our output could be any real number, but probabilities must be between 0 and 1. A linear model could predict probabilities less than 0 or greater than 1, which is nonsensical for probabilities. This is where the logistic function comes into play.</p>
<p><strong>The Sigmoid (Logistic) Function</strong></p>
<p>The heart of Logistic Regression is the <strong>sigmoid function</strong>, also known as the <strong>logistic function</strong>. This function squashes any real-valued number into a range between 0 and 1, making it perfect for modeling probabilities.</p>
<p>The sigmoid function is defined as:</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma(z)\)</span> (sigma of z) is the output of the sigmoid function, a probability between 0 and 1.</p></li>
<li><p><span class="math notranslate nohighlight">\(e\)</span> is Euler’s number (approximately 2.71828).</p></li>
<li><p><span class="math notranslate nohighlight">\(z\)</span> is the input to the function, which can be any real number.</p></li>
</ul>
<p>Let’s look at the shape of the sigmoid function:</p>
<ul class="simple">
<li><p>As <span class="math notranslate nohighlight">\( z \to \infty \)</span>, <span class="math notranslate nohighlight">\( e^{-z} \to 0 \)</span>, so <span class="math notranslate nohighlight">\(\sigma(z) \to \frac{1}{1+0} = 1\)</span>.</p></li>
<li><p>As <span class="math notranslate nohighlight">\( z \to -\infty \)</span>, <span class="math notranslate nohighlight">\( e^{-z} \to \infty \)</span>, so <span class="math notranslate nohighlight">\(\sigma(z) \to \frac{1}{\infty} = 0\)</span>.</p></li>
<li><p>When <span class="math notranslate nohighlight">\( z = 0 \)</span>, <span class="math notranslate nohighlight">\( e^{-z} = e^0 = 1 \)</span>, so <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1+1} = 0.5\)</span>.</p></li>
</ul>
<p>This S-shaped curve nicely maps any linear combination of features to a probability.</p>
</section>
<section id="the-linear-part-the-log-odds">
<h2>The Linear Part: The Log-Odds<a class="headerlink" href="#the-linear-part-the-log-odds" title="Link to this heading">#</a></h2>
<p>Before feeding into the sigmoid function, we create a linear combination of our input features, similar to linear regression. This linear combination is often referred to as the <strong>log-odds</strong> or <strong>logit</strong>.</p>
<p>For a single feature <span class="math notranslate nohighlight">\( x_1 \)</span>:</p>
<div class="math notranslate nohighlight">
\[z = \beta_0 + \beta_1 x_1\]</div>
<p>For multiple features <span class="math notranslate nohighlight">\( x_1, x_2, \ldots, x_n \)</span>:</p>
<div class="math notranslate nohighlight">
\[z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n\]</div>
<p>In a more compact vector notation, where <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a vector of features (including a bias term <span class="math notranslate nohighlight">\(x_0 = 1\)</span>) and <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is a vector of coefficients:</p>
<div class="math notranslate nohighlight">
\[z = \boldsymbol{\beta}^T \mathbf{x}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept (bias term).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1, \ldots, \beta_n\)</span> are the coefficients (weights) associated with each feature. These are the parameters our model will learn.</p></li>
</ul>
</section>
<section id="the-logistic-regression-model-equation">
<h2>The Logistic Regression Model Equation<a class="headerlink" href="#the-logistic-regression-model-equation" title="Link to this heading">#</a></h2>
<p>Combining the linear part with the sigmoid function, we get the core equation for Logistic Regression:</p>
<div class="math notranslate nohighlight">
\[P(Y=1|\mathbf{x}) = \sigma(\boldsymbol{\beta}^T \mathbf{x}) = \frac{1}{1 + e^{-(\boldsymbol{\beta}^T \mathbf{x})}}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(P(Y=1|\mathbf{x})\)</span> represents the probability that the outcome <span class="math notranslate nohighlight">\( Y \)</span> is 1 (the positive class) given the input features <span class="math notranslate nohighlight">\( \mathbf{x} \)</span>.</p>
</section>
<section id="interpreting-the-coefficients">
<h2>Interpreting the Coefficients<a class="headerlink" href="#interpreting-the-coefficients" title="Link to this heading">#</a></h2>
<p>The coefficients <span class="math notranslate nohighlight">\(\beta_i\)</span> in Logistic Regression are not as straightforward to interpret as in linear regression. A positive <span class="math notranslate nohighlight">\(\beta_i\)</span> means that as <span class="math notranslate nohighlight">\(x_i\)</span> increases, the probability of the positive class (<span class="math notranslate nohighlight">\(Y=1\)</span>) increases. Conversely, a negative <span class="math notranslate nohighlight">\(\beta_i\)</span> means that as <span class="math notranslate nohighlight">\(x_i\)</span> increases, the probability of the positive class decreases.</p>
<p>Specifically, <span class="math notranslate nohighlight">\(e^{\beta_i}\)</span> represents the odds ratio. For a one-unit increase in <span class="math notranslate nohighlight">\( x_i \)</span>, the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> (versus <span class="math notranslate nohighlight">\(Y=0\)</span>) are multiplied by <span class="math notranslate nohighlight">\( e^{\beta_i} \)</span>, assuming all other features remain constant.</p>
</section>
<section id="decision-boundary">
<h2>Decision Boundary<a class="headerlink" href="#decision-boundary" title="Link to this heading">#</a></h2>
<p>Once we have the probabilities, we need a way to classify. We typically set a threshold (e.g., 0.5). If <span class="math notranslate nohighlight">\( P(Y=1|\mathbf{x}) \geq 0.5 \)</span>, we classify as 1; otherwise, we classify as 0.</p>
<p>The decision boundary is where the probability is 0.5. Since <span class="math notranslate nohighlight">\(\sigma(z) = 0.5\)</span> when <span class="math notranslate nohighlight">\( z = 0 \)</span>, the decision boundary is defined by:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\beta}^T \mathbf{x} = 0\]</div>
<p>This means that the decision boundary is a linear hyperplane in the feature space.</p>
</section>
<hr class="docutils" />
<section id="learning-the-parameters-maximum-likelihood-estimation">
<h2>Learning the Parameters: Maximum Likelihood Estimation<a class="headerlink" href="#learning-the-parameters-maximum-likelihood-estimation" title="Link to this heading">#</a></h2>
<p>How do we find the optimal values for <span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span>? We use a method called <strong>Maximum Likelihood Estimation (MLE)</strong>.</p>
<p>Instead of minimizing squared errors (like in linear regression), we want to find the <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> values that maximize the likelihood of observing our training data. In other words, we want to choose <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> such that the predicted probabilities for the observed outcomes are as high as possible.</p>
<p>Let’s assume we have <span class="math notranslate nohighlight">\(m\)</span> training examples <span class="math notranslate nohighlight">\( (\mathbf{x}^{(1)}, y^{(1)}), \ldots, (\mathbf{x}^{(m)}, y^{(m)}) \)</span>, where <span class="math notranslate nohighlight">\(y^{(i)} \in \{0, 1\}\)</span>.</p>
<p>The probability of observing a single training example <span class="math notranslate nohighlight">\((\mathbf{x}^{(i)}, y^{(i)})\)</span> is:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\( y^{(i)} = 1 \)</span>: <span class="math notranslate nohighlight">\(P(Y=1|\mathbf{x}^{(i)}; \boldsymbol{\beta}) = h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\( y^{(i)} = 0 \)</span>: <span class="math notranslate nohighlight">\(P(Y=0|\mathbf{x}^{(i)}; \boldsymbol{\beta}) = 1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})\)</span></p></li>
</ul>
<p>We can combine these into a single expression:</p>
<div class="math notranslate nohighlight">
\[P(y^{(i)}|\mathbf{x}^{(i)}; \boldsymbol{\beta}) = (h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}))^{y^{(i)}} (1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}))^{1 - y^{(i)}}\]</div>
<p>Where <span class="math notranslate nohighlight">\(h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) = \sigma(\boldsymbol{\beta}^T \mathbf{x}^{(i)})\)</span>.</p>
<p>The <strong>likelihood function</strong> <span class="math notranslate nohighlight">\(L(\boldsymbol{\beta})\)</span> is the product of the probabilities of all training examples (assuming they are independent and identically distributed):</p>
<div class="math notranslate nohighlight">
\[L(\boldsymbol{\beta}) = \prod_{i=1}^{m} (h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}))^{y^{(i)}} (1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}))^{1 - y^{(i)}}\]</div>
<p>To simplify calculations, it’s common to work with the <strong>log-likelihood function</strong>, as maximizing the likelihood is equivalent to maximizing the log-likelihood (due to the monotonic nature of the logarithm):</p>
<div class="math notranslate nohighlight">
\[\text{log}L(\boldsymbol{\beta}) = \sum_{i=1}^{m} [y^{(i)} \text{log}(h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \text{log}(1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}))]\]</div>
<p>This log-likelihood function is also known as the <strong>cross-entropy cost function</strong> (or negative log-likelihood) for binary classification. We typically try to <em>minimize</em> the negative log-likelihood:</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \text{log}(h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \text{log}(1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}))]\]</div>
</section>
<section id="gradient-descent-for-optimization">
<h2>Gradient Descent for Optimization<a class="headerlink" href="#gradient-descent-for-optimization" title="Link to this heading">#</a></h2>
<p>To find the values of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> that minimize <span class="math notranslate nohighlight">\( J(\boldsymbol{\beta}) \)</span>, we use an iterative optimization algorithm like <strong>Gradient Descent</strong>.</p>
<p>The update rule for each parameter <span class="math notranslate nohighlight">\(\beta_j\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\beta_j := \beta_j - \alpha \frac{\partial J(\boldsymbol{\beta})}{\partial \beta_j}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate.</p>
<p>Let’s derive the partial derivative <span class="math notranslate nohighlight">\( \frac{\partial J(\boldsymbol{\beta})}{\partial \beta_j} \)</span>:</p>
<p>First, recall <span class="math notranslate nohighlight">\(h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) = \sigma(z^{(i)}) = \frac{1}{1 + e^{-z^{(i)}}}\)</span> where <span class="math notranslate nohighlight">\(z^{(i)} = \boldsymbol{\beta}^T \mathbf{x}^{(i)}\)</span>.</p>
<p>The derivative of the sigmoid function is quite elegant:
$<span class="math notranslate nohighlight">\(\frac{d\sigma(z)}{dz} = \sigma(z)(1 - \sigma(z))\)</span>$</p>
<p>Now, let’s derive the derivative of a single term in the cost function with respect to <span class="math notranslate nohighlight">\( \beta_j \)</span>:</p>
<p>Let <span class="math notranslate nohighlight">\(\text{cost}^{(i)} = -[y^{(i)} \text{log}(h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \text{log}(1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}))]\)</span>.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \text{cost}^{(i)}}{\partial \beta_j} = - \left[ \frac{y^{(i)}}{h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})} \frac{\partial h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})}{\partial \beta_j} + \frac{1 - y^{(i)}}{1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})} \left( -\frac{\partial h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})}{\partial \beta_j} \right) \right]\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial \text{cost}^{(i)}}{\partial \beta_j} = - \left[ \frac{y^{(i)}}{h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})} - \frac{1 - y^{(i)}}{1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})} \right] \frac{\partial h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})}{\partial \beta_j}\]</div>
<p>We know <span class="math notranslate nohighlight">\( \frac{\partial h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})}{\partial \beta_j} = \frac{\partial \sigma(z^{(i)})}{\partial z^{(i)}} \frac{\partial z^{(i)}}{\partial \beta_j} = \sigma(z^{(i)})(1 - \sigma(z^{(i)})) x_j^{(i)} = h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})(1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})) x_j^{(i)} \)</span>.</p>
<p>Substitute this back:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \text{cost}^{(i)}}{\partial \beta_j} = - \left[ \frac{y^{(i)}}{h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})} - \frac{1 - y^{(i)}}{1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})} \right] h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})(1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})) x_j^{(i)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial \text{cost}^{(i)}}{\partial \beta_j} = - \left[ y^{(i)}(1 - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)})) - (1 - y^{(i)})h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) \right] x_j^{(i)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial \text{cost}^{(i)}}{\partial \beta_j} = - \left[ y^{(i)} - y^{(i)}h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) + y^{(i)}h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) \right] x_j^{(i)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial \text{cost}^{(i)}}{\partial \beta_j} = - \left[ y^{(i)} - h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) \right] x_j^{(i)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial \text{cost}^{(i)}}{\partial \beta_j} = (h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}\]</div>
<p>So, the partial derivative of the total cost function <span class="math notranslate nohighlight">\(J(\boldsymbol{\beta})\)</span> with respect to <span class="math notranslate nohighlight">\(\beta_j\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\boldsymbol{\beta})}{\partial \beta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}\]</div>
<p>This is remarkably similar to the gradient descent update rule for linear regression, which is a nice feature of using the sigmoid function and the cross-entropy loss.</p>
<p>The gradient descent update rule becomes:</p>
<div class="math notranslate nohighlight">
\[\beta_j := \beta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\boldsymbol{\beta}}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}\]</div>
<p>We repeatedly apply this update rule for all <span class="math notranslate nohighlight">\(\beta_j\)</span> (simultaneously) until convergence (i.e., the parameters stop changing significantly or the cost function converges).</p>
</section>
<section id="simple-examples">
<h2>Simple Examples<a class="headerlink" href="#simple-examples" title="Link to this heading">#</a></h2>
<p>Let’s illustrate with some conceptual examples.</p>
<section id="example-1-admitting-to-college">
<h3>Example 1: Admitting to College<a class="headerlink" href="#example-1-admitting-to-college" title="Link to this heading">#</a></h3>
<p>Imagine we want to predict whether a student will be admitted to a prestigious college based on their SAT score.</p>
<ul class="simple">
<li><p><strong>Input Feature (x):</strong> SAT Score (e.g., 800 - 1600)</p></li>
<li><p><strong>Output (Y):</strong> 1 (Admitted), 0 (Not Admitted)</p></li>
</ul>
<p>The Logistic Regression model would learn a relationship like this:</p>
<div class="math notranslate nohighlight">
\[P(\text{Admitted}=1|\text{SAT Score}) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 \cdot \text{SAT Score})}}\]</div>
<p>Let’s say after training, we find <span class="math notranslate nohighlight">\(\beta_0 = -10\)</span> and <span class="math notranslate nohighlight">\(\beta_1 = 0.008\)</span>.</p>
<ul class="simple">
<li><p>If a student has an SAT score of 1200:
<span class="math notranslate nohighlight">\(z = -10 + 0.008 \cdot 1200 = -10 + 9.6 = -0.4\)</span>
<span class="math notranslate nohighlight">\(P(\text{Admitted}=1|\text{SAT}=1200) = \frac{1}{1 + e^{-(-0.4)}} = \frac{1}{1 + e^{0.4}} \approx \frac{1}{1 + 1.49} \approx 0.40\)</span>
With a threshold of 0.5, this student would likely not be admitted.</p></li>
<li><p>If a student has an SAT score of 1400:
<span class="math notranslate nohighlight">\(z = -10 + 0.008 \cdot 1400 = -10 + 11.2 = 1.2\)</span>
<span class="math notranslate nohighlight">\(P(\text{Admitted}=1|\text{SAT}=1400) = \frac{1}{1 + e^{-(1.2)}} = \frac{1}{1 + e^{-1.2}} \approx \frac{1}{1 + 0.30} \approx 0.77\)</span>
With a threshold of 0.5, this student would likely be admitted.</p></li>
</ul>
<p>The model learns that higher SAT scores lead to a higher probability of admission. The <span class="math notranslate nohighlight">\(\beta_1 = 0.008\)</span> signifies that for every one-point increase in SAT score, the log-odds of admission increase by 0.008.</p>
</section>
<section id="example-2-detecting-spam-email">
<h3>Example 2: Detecting Spam Email<a class="headerlink" href="#example-2-detecting-spam-email" title="Link to this heading">#</a></h3>
<p>Let’s say we have two features:</p>
<ul class="simple">
<li><p><strong>x1:</strong> Number of suspicious words (e.g., “win,” “prize,” “free”)</p></li>
<li><p><strong>x2:</strong> Sender’s reputation score (e.g., 0-10, lower is worse)</p></li>
<li><p><strong>Y:</strong> 1 (Spam), 0 (Not Spam)</p></li>
</ul>
<p>Our model would be:</p>
<div class="math notranslate nohighlight">
\[P(\text{Spam}=1|x_1, x_2) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}}\]</div>
<p>Suppose after training, <span class="math notranslate nohighlight">\( \beta_0 = -2 \)</span>, <span class="math notranslate nohighlight">\( \beta_1 = 0.5 \)</span>, and <span class="math notranslate nohighlight">\(\beta_2 = -0.3\)</span>.</p>
<ul class="simple">
<li><p><strong>Email A:</strong> 5 suspicious words (<span class="math notranslate nohighlight">\(x_1=5\)</span>), sender reputation 8 (<span class="math notranslate nohighlight">\(x_2=8\)</span>)
<span class="math notranslate nohighlight">\(z = -2 + (0.5 \cdot 5) + (-0.3 \cdot 8) = -2 + 2.5 - 2.4 = -1.9\)</span>
<span class="math notranslate nohighlight">\(P(\text{Spam}=1|\text{Email A}) = \frac{1}{1 + e^{-(-1.9)}} = \frac{1}{1 + e^{1.9}} \approx \frac{1}{1 + 6.68} \approx 0.13\)</span>
Likely not spam.</p></li>
<li><p><strong>Email B:</strong> 10 suspicious words (<span class="math notranslate nohighlight">\(x_1=10\)</span>), sender reputation 2 (<span class="math notranslate nohighlight">\(x_2=2\)</span>)
<span class="math notranslate nohighlight">\(z = -2 + (0.5 \cdot 10) + (-0.3 \cdot 2) = -2 + 5 - 0.6 = 2.4\)</span>
<span class="math notranslate nohighlight">\(P(\text{Spam}=1|\text{Email B}) = \frac{1}{1 + e^{-(2.4)}} = \frac{1}{1 + e^{-2.4}} \approx \frac{1}{1 + 0.09} \approx 0.91\)</span>
Likely spam.</p></li>
</ul>
<p>Here, a positive <span class="math notranslate nohighlight">\(\beta_1\)</span> means more suspicious words increase the probability of spam, and a negative <span class="math notranslate nohighlight">\(\beta_2\)</span> means a lower sender reputation (lower <span class="math notranslate nohighlight">\(x_2\)</span>) increases the probability of spam.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Logistic Regression is a fundamental machine learning algorithm for binary classification. It leverages the sigmoid function to map linear combinations of features to probabilities, which are then used for classification. Maximum Likelihood Estimation, typically optimized via Gradient Descent, is used to learn the model’s parameters. Its interpretability and simplicity make it a popular choice for many real-world classification problems.</p>
<p>Let me know if you’d like to delve into more advanced topics like regularization, multiclass logistic regression, or practical implementation details!</p>
</section>
<section id="additional-materials">
<h2>Additional Materials<a class="headerlink" href="#additional-materials" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.ibm.com/think/topics/logistic-regression">https://www.ibm.com/think/topics/logistic-regression</a></p></li>
<li><p><a class="reference external" href="https://developers.google.com/machine-learning/crash-course/logistic-regression/sigmoid-function?hl=en">https://developers.google.com/machine-learning/crash-course/logistic-regression/sigmoid-function?hl=en</a></p></li>
<li><p><a class="reference external" href="https://www.v7labs.com/blog/logistic-regression">https://www.v7labs.com/blog/logistic-regression</a></p></li>
<li><p><a class="reference external" href="https://www.geeksforgeeks.org/understanding-logistic-regression/">https://www.geeksforgeeks.org/understanding-logistic-regression/</a></p></li>
<li><p><a class="reference external" href="https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/">https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent">https://en.wikipedia.org/wiki/Gradient_descent</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Cross-entropy">https://en.wikipedia.org/wiki/Cross-entropy</a></p></li>
<li><p><a class="reference external" href="https://www.kellerbiostat.com/introregression/logisticinference">https://www.kellerbiostat.com/introregression/logisticinference</a></p></li>
<li><p><a class="reference external" href="https://www.datacamp.com/tutorial/understanding-logistic-regression-python">https://www.datacamp.com/tutorial/understanding-logistic-regression-python</a></p></li>
<li><p><a class="reference external" href="https://ai-ml-analytics.com/logistic-regression-practical-example">https://ai-ml-analytics.com/logistic-regression-practical-example</a></p></li>
<li><p><a class="reference external" href="https://www.simplilearn.com/tutorials/machine-learning-tutorial/logistic-regression-in-python">https://www.simplilearn.com/tutorials/machine-learning-tutorial/logistic-regression-in-python</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">https://en.wikipedia.org/wiki/Sigmoid_function</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./data-science"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-core-idea-modeling-probability">The Core Idea: Modeling Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-linear-part-the-log-odds">The Linear Part: The Log-Odds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-regression-model-equation">The Logistic Regression Model Equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-coefficients">Interpreting the Coefficients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary">Decision Boundary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-the-parameters-maximum-likelihood-estimation">Learning the Parameters: Maximum Likelihood Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-for-optimization">Gradient Descent for Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-examples">Simple Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-admitting-to-college">Example 1: Admitting to College</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-detecting-spam-email">Example 2: Detecting Spam Email</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional Materials</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Vladyslav Yakovliev (Ukraine)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>