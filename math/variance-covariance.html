
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Variance and Covariance &#8212; Quantopia&#39;:&#39; Physics, Python and Pi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'math/variance-covariance';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Variance and Covariance - Code Examples" href="variance-covariance-code.html" />
    <link rel="prev" title="Ordinary Least Squares (OLS) Regression - Code Example" href="ordinary-least-squares-code.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Quantopia':' Physics, Python and Pi - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Quantopia':' Physics, Python and Pi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Quantopia: Physics, Python, and Pi (Alpha Version)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">MATH</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gradient-operator.html">Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradient-directional-derivative.html">Directional Derivative</a></li>
<li class="toctree-l1"><a class="reference internal" href="divergence.html">Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier-transform-01.html">Fourier Transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="pmf-pdf-cdf.html">Random Variables, Probability Mass Function, Probability Density Function, Cumulative Distribution Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="pmf-pdf-cdf-code.html">PMF, PDF, CDF - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="bernoulli-and-binomial-distributions.html">Bernoulli Distribution and Binomial Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="bernoulli-and-binomial-distributions-code.html">Bernoulli Distribution and Binomial Distributions - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="uncertainties-introduction.html">Experimental Errors and Significant Figures</a></li>
<li class="toctree-l1"><a class="reference internal" href="least-squares-regression.html">Least Squares Regression, RSS, RMSE, R-squared</a></li>
<li class="toctree-l1"><a class="reference internal" href="least-squares-regression-code.html">Least Squares Regression - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares.html">Ordinary Least Squares (OLS) Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinary-least-squares-code.html">Ordinary Least Squares (OLS) Regression - Code Example</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Variance and Covariance</a></li>
<li class="toctree-l1"><a class="reference internal" href="variance-covariance-code.html">Variance and Covariance - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="correlation-coefficients.html">Correlation Coefficients</a></li>
<li class="toctree-l1"><a class="reference internal" href="correlation-coefficients-code.html">Correlation Coefficients - Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares.html">Weighted Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-1.html">WLS - Code Examples Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-2.html">WLS - Code Examples Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="goodness-of-fit-and-chi-squared.html">Goodness of Fit and Chi-Squared Statistic</a></li>
<li class="toctree-l1"><a class="reference internal" href="aic-and-bic.html">Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="weighted-least-squares-code-3.html">WLS - Code Examples Part 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="orthogonal-distance-regression.html">Orthogonal Distance Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="odr-code.html">ODR - Code Examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PHYSICS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../physics/continuity-equation-01.html">The Continuity Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/continuity-equation-02.html">The Continuity Equation: One-Dimensional Advection of a Density Profile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/ensemble.html">Statistical Ensembles and Liouville’s Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/microcanonical-ensemble.html">Microcanonical Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/inertial_vs_gravitational_mass.html">Mass: Inertial vs. Gravitational</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DATA ANALYSIS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../data-analysis/rate-of-return-metrics.html">Advanced Rate of Return Metrics (IRR, XIRR, MIRR, XMIRR, PV, FV, NPV, XNPV)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yakovliev/quantopia/issues/new?title=Issue%20on%20page%20%2Fmath/variance-covariance.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/math/variance-covariance.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Variance and Covariance</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-variance-and-covariance-matter-a-foundation-for-data-understanding">Why Variance and Covariance Matter: A Foundation for Data Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-average">Mean (Average)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-calculating-the-mean">Example: Calculating the Mean</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-deviation">Standard Deviation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#population-standard-deviation-sigma">Population Standard Deviation (<span class="math notranslate nohighlight">\(\sigma\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-standard-deviation-s">Sample Standard Deviation (<span class="math notranslate nohighlight">\(s\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#population-variance-sigma-2">Population Variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-variance-s-2">Sample Variance (<span class="math notranslate nohighlight">\(s^2\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-calculating-variance-and-standard-deviation">Example: Calculating Variance and Standard Deviation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-calculation-using-definitional-formula">Step-by-step Calculation using Definitional Formula:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#population-variance-sigma-2-and-standard-deviation-sigma">Population Variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>) and Standard Deviation (<span class="math notranslate nohighlight">\(\sigma\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-variance-s-2-and-standard-deviation-s">Sample Variance (<span class="math notranslate nohighlight">\(s^2\)</span>) and Standard Deviation (<span class="math notranslate nohighlight">\(s\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-and-limitations-of-variance-and-standard-deviation">Interpretation and Limitations of Variance and Standard Deviation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-variance">Properties of Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#population-covariance-text-cov-x-y-or-sigma-xy">Population Covariance (<span class="math notranslate nohighlight">\(\text{Cov}(X, Y)\)</span> or <span class="math notranslate nohighlight">\(\sigma_{XY}\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-covariance-s-xy">Sample Covariance (<span class="math notranslate nohighlight">\(s_{XY}\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-coefficient-pearson-s-r">Correlation Coefficient (Pearson’s r)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#population-correlation-coefficient-rho">Population Correlation Coefficient (<span class="math notranslate nohighlight">\(\rho\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-correlation-coefficient-r">Sample Correlation Coefficient (<span class="math notranslate nohighlight">\(r\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-and-limitations-of-covariance-and-correlation">Interpretation and Limitations of Covariance and Correlation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-covariance">Properties of Covariance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bessel-s-correction">Bessel’s Correction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#degrees-of-freedom-explanation">Degrees of Freedom Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-proof-sketch-for-variance">Mathematical Proof Sketch (for Variance)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional Materials</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="variance-and-covariance">
<h1>Variance and Covariance<a class="headerlink" href="#variance-and-covariance" title="Link to this heading">#</a></h1>
<blockquote>
<div><p>In statistics, we often use uppercase letters (e.g., <span class="math notranslate nohighlight">\(X\)</span>) to denote a <strong>random variable</strong>, which represents the abstract concept or process being measured (like the height of a person). Lowercase letters with an index (e.g., <span class="math notranslate nohighlight">\(x_i\)</span>) represent the specific <strong>observations</strong> or actual data points collected for that variable (e.g., <span class="math notranslate nohighlight">\(x_1 = 175\text{cm}\)</span>, <span class="math notranslate nohighlight">\(x_2 = 182\text{cm}\)</span>, etc.). While this document will primarily use the lowercase notation for calculations, it is helpful to remember this distinction.</p>
</div></blockquote>
<section id="why-variance-and-covariance-matter-a-foundation-for-data-understanding">
<h2>Why Variance and Covariance Matter: A Foundation for Data Understanding<a class="headerlink" href="#why-variance-and-covariance-matter-a-foundation-for-data-understanding" title="Link to this heading">#</a></h2>
<p>Variance and covariance are more than just statistical formulas; they are fundamental building blocks in statistics, data science, and quantitative analysis, offering profound insights into the nature of data. Mastering these concepts is indispensable for anyone working with data.</p>
<ul class="simple">
<li><p><strong>Quantifying Uncertainty and Dispersion (Variance &amp; Standard Deviation):</strong>
Variance, and its more interpretable cousin, standard deviation, quantify the spread, dispersion, or volatility of a single variable. This is critical in diverse fields for:</p>
<ul>
<li><p><strong>Finance:</strong> Measuring the risk or volatility of an investment portfolio.</p></li>
<li><p><strong>Quality Control:</strong> Assessing the consistency and precision of a manufacturing process to ensure product standards.</p></li>
<li><p><strong>Scientific Research:</strong> Understanding the inherent variability within experimental results and the reliability of measurements.</p></li>
<li><p><strong>Sports Analytics:</strong> Evaluating the consistency of athlete performance.</p></li>
<li><p><strong>Machine Learning:</strong> Informing feature scaling, understanding data distribution for model assumptions, and anomaly detection.</p></li>
</ul>
</li>
<li><p><strong>Revealing Relationships and Dependencies (Covariance &amp; Correlation):</strong>
Covariance and its standardized counterpart, the correlation coefficient, quantify how two variables move together. This understanding is vital for:</p>
<ul>
<li><p><strong>Economics:</strong> Analyzing the relationship between economic indicators like interest rates and inflation, or supply and demand.</p></li>
<li><p><strong>Healthcare:</strong> Investigating associations between lifestyle factors (e.g., diet, exercise) and health outcomes (e.g., disease incidence).</p></li>
<li><p><strong>Portfolio Management:</strong> Diversifying investments by selecting assets with low or negative covariance to reduce overall risk.</p></li>
<li><p><strong>Predictive Modeling:</strong> Identifying influential features in regression analysis, understanding multicollinearity, and building more robust predictive models.</p></li>
<li><p><strong>Social Sciences:</strong> Studying how different demographic factors relate to social behaviors or outcomes.</p></li>
</ul>
</li>
</ul>
<p>These measures form the bedrock for more advanced statistical concepts, including hypothesis testing, confidence intervals, linear regression, principal component analysis (PCA), factor analysis, and time series modeling. A deep understanding of variance and covariance empowers analysts to make informed decisions, draw valid conclusions, and build effective models from data.</p>
</section>
<hr class="docutils" />
<section id="mean-average">
<h2>Mean (Average)<a class="headerlink" href="#mean-average" title="Link to this heading">#</a></h2>
<p>Before diving into variance and covariance, we need to understand the concept of the mean, also known as the average. The mean is a measure of central tendency, representing the typical value in a dataset.</p>
<p>For a set of <span class="math notranslate nohighlight">\(N\)</span> data points (observations) <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_N\)</span>:</p>
<p><strong>Population Mean (<span class="math notranslate nohighlight">\(\mu\)</span>)</strong>: If we have the entire population, the mean is calculated as the sum of all observations divided by the number of observations.</p>
<div class="math notranslate nohighlight">
\[\mu = \frac{\sum_{i=1}^{N} x_i}{N}\]</div>
<p><strong>Sample Mean (<span class="math notranslate nohighlight">\(\bar{x}\)</span>)</strong>: If we have a sample from a larger population, the sample mean is calculated similarly.</p>
<div class="math notranslate nohighlight">
\[\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the number of observations in the sample.</p>
<section id="example-calculating-the-mean">
<h3>Example: Calculating the Mean<a class="headerlink" href="#example-calculating-the-mean" title="Link to this heading">#</a></h3>
<p>Let’s consider a small dataset of daily high temperatures (in Celsius) for a particular week: <span class="math notranslate nohighlight">\(x = \{20, 22, 19, 23, 21, 20, 24\}\)</span>.</p>
<p><strong>Calculation Steps:</strong></p>
<ol class="arabic simple">
<li><p><strong>Sum of observations:</strong> <span class="math notranslate nohighlight">\(\sum_{i=1}^{7} x_i = 20 + 22 + 19 + 23 + 21 + 20 + 24 = 149\)</span></p></li>
<li><p><strong>Number of observations:</strong> <span class="math notranslate nohighlight">\(N = 7\)</span> (if we consider this the entire population of temperatures for <em>that specific week</em>) or <span class="math notranslate nohighlight">\(n = 7\)</span> (if this is a sample from a larger set of daily temperatures).</p></li>
</ol>
<p><strong>Population Mean (<span class="math notranslate nohighlight">\(\mu\)</span>):</strong>
If this represents the entire population of interest (e.g., the high temperatures for <em>that exact week</em>), the population mean is:
$<span class="math notranslate nohighlight">\(\mu = \frac{\sum_{i=1}^{N} x_i}{N} = \frac{149}{7} \approx 21.286 \text{ C}\)</span>$</p>
<p><strong>Sample Mean (<span class="math notranslate nohighlight">\(\bar{x}\)</span>):</strong>
If we treat these 7 days as a <em>sample</em> drawn from a larger population (e.g., temperatures throughout a whole month or season), the sample mean is calculated identically:
$<span class="math notranslate nohighlight">\(\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n} = \frac{149}{7} \approx 21.286 \text{ C}\)</span>$
In this specific numerical example, the result for population and sample mean is the same because the calculation formula is identical. The distinction lies in the statistical context of whether the data represents the <em>entire</em> group or a <em>subset</em>.</p>
</section>
</section>
<hr class="docutils" />
<section id="variance">
<h2>Variance<a class="headerlink" href="#variance" title="Link to this heading">#</a></h2>
<p>Variance is a measure of how spread out a set of data is. It quantifies the average of the squared differences from the mean. A high variance indicates that data points are spread far from the mean and from each other, while a low variance indicates that data points are clustered closely around the mean.</p>
<p>Why squared differences?</p>
<ul class="simple">
<li><p><strong>To avoid cancellation:</strong> If we just summed the deviations <span class="math notranslate nohighlight">\((x_i - \mu)\)</span>, positive and negative deviations would cancel out, potentially leading to a sum of zero even for widely dispersed data.</p></li>
<li><p><strong>To penalize larger deviations more:</strong> Squaring exaggerates larger differences, giving more weight to data points that are further from the mean.</p></li>
</ul>
<p><strong>Units of Variance:</strong></p>
<p>It’s crucial to note that variance is always expressed in <strong>squared units</strong> of the original data. For example, if your data points represent measurements in meters (m), the variance will be in square meters (<span class="math notranslate nohighlight">\(m^2\)</span>). This squaring often makes variance less intuitive for direct interpretation in the context of the original measurements, which naturally leads us to the standard deviation.</p>
<section id="standard-deviation">
<h3>Standard Deviation<a class="headerlink" href="#standard-deviation" title="Link to this heading">#</a></h3>
<p>While variance provides a numerical value for data spread, its squared units can be difficult to relate directly to the original data. The <strong>standard deviation</strong> is defined as the <strong>square root of the variance</strong> and is expressed in the <strong>same units</strong> as the original data. This makes it a much more interpretable, intuitive, and widely used measure of data dispersion.</p>
<ul class="simple">
<li><p>A <strong>low standard deviation</strong> indicates that data points tend to be clustered closely around the mean, implying high consistency or low variability.</p></li>
<li><p>A <strong>high standard deviation</strong> indicates that data points are spread out over a wider range of values, implying greater variability or dispersion.</p></li>
</ul>
<section id="population-standard-deviation-sigma">
<h4>Population Standard Deviation (<span class="math notranslate nohighlight">\(\sigma\)</span>)<a class="headerlink" href="#population-standard-deviation-sigma" title="Link to this heading">#</a></h4>
<p>The population standard deviation is the square root of the population variance. It is typically denoted by the lowercase Greek letter sigma (<span class="math notranslate nohighlight">\(\sigma\)</span>).</p>
<div class="math notranslate nohighlight">
\[\sigma = \sqrt{\sigma^2} = \sqrt{\frac{\sum_{i=1}^{N} (x_i - \mu)^2}{N}}\]</div>
</section>
<section id="sample-standard-deviation-s">
<h4>Sample Standard Deviation (<span class="math notranslate nohighlight">\(s\)</span>)<a class="headerlink" href="#sample-standard-deviation-s" title="Link to this heading">#</a></h4>
<p>The sample standard deviation is the square root of the sample variance. It is typically denoted by the lowercase Latin letter <span class="math notranslate nohighlight">\(s\)</span>.</p>
<div class="math notranslate nohighlight">
\[s = \sqrt{s^2} = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}}\]</div>
</section>
</section>
<section id="population-variance-sigma-2">
<h3>Population Variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>)<a class="headerlink" href="#population-variance-sigma-2" title="Link to this heading">#</a></h3>
<p>The population variance is the true variance of an entire population. The population variance is the average of the squared differences between each data point and the population mean.</p>
<p>For a population with <span class="math notranslate nohighlight">\(N\)</span> data points <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_N\)</span> and population mean <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Calculate the deviation of each data point from the mean</strong>: <span class="math notranslate nohighlight">\((x_i - \mu)\)</span></p></li>
<li><p><strong>Square each deviation</strong>: <span class="math notranslate nohighlight">\((x_i - \mu)^2\)</span></p></li>
<li><p><strong>Sum all the squared deviations</strong>: <span class="math notranslate nohighlight">\(\sum_{i=1}^{N} (x_i - \mu)^2\)</span></p></li>
<li><p><strong>Divide by the total number of data points (<span class="math notranslate nohighlight">\(N\)</span>) to find the average squared deviation</strong>:</p></li>
</ol>
<p>Thus, the formula for population variance is:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{N}\]</div>
<p><strong>Alternative Form (Computational Formula)</strong>:</p>
<p>So, here is our main formula:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{N}\]</div>
<p>Expand the squared term:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i^2 - 2x_i\mu + \mu^2)\]</div>
<p>Distribute the summation:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{1}{N} \left( \sum_{i=1}^{N} x_i^2 - \sum_{i=1}^{N} 2x_i\mu + \sum_{i=1}^{N} \mu^2 \right)\]</div>
<p>Pull constants out of the summation:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{1}{N} \left( \sum_{i=1}^{N} x_i^2 - 2\mu \sum_{i=1}^{N} x_i + N\mu^2 \right)\]</div>
<p>Recall that <span class="math notranslate nohighlight">\(\mu = \frac{\sum_{i=1}^{N} x_i}{N}\)</span>, which means <span class="math notranslate nohighlight">\(\sum_{i=1}^{N} x_i = N\mu\)</span>. Substitute this into the equation:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{1}{N} \left( \sum_{i=1}^{N} x_i^2 - 2\mu (N\mu) + N\mu^2 \right)\]</div>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{1}{N} \left( \sum_{i=1}^{N} x_i^2 - 2N\mu^2 + N\mu^2 \right)\]</div>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{1}{N} \left( \sum_{i=1}^{N} x_i^2 - N\mu^2 \right)\]</div>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{\sum_{i=1}^{N} x_i^2}{N} - \mu^2\]</div>
<p>This is a convenient form often used for calculation.</p>
</section>
<section id="sample-variance-s-2">
<h3>Sample Variance (<span class="math notranslate nohighlight">\(s^2\)</span>)<a class="headerlink" href="#sample-variance-s-2" title="Link to this heading">#</a></h3>
<p>When we work with a sample from a larger population, we use sample variance to estimate the population variance.</p>
<p>The sample variance is an estimate of the population variance, calculated using the squared differences from the sample mean, divided by <span class="math notranslate nohighlight">\(n-1\)</span>.</p>
<p><strong>Derivation of the Formula</strong>:</p>
<p>If we were to use <span class="math notranslate nohighlight">\(n\)</span> in the denominator for sample variance, like we did for population variance, our estimate would consistently be <em>underestimated</em>. This is because the sample mean (<span class="math notranslate nohighlight">\(\bar{x}\)</span>) is calculated from the sample itself, and it will always be closer to the sample data points than the true population mean (<span class="math notranslate nohighlight">\(\mu\)</span>) would be. This makes the sum of squared deviations from <span class="math notranslate nohighlight">\(\bar{x}\)</span> smaller than the sum of squared deviations from <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>To correct for this bias and provide an <strong>unbiased estimator</strong> of the population variance, we divide by <span class="math notranslate nohighlight">\(n-1\)</span>, which is known as <strong>Bessel’s correction</strong>. The term <span class="math notranslate nohighlight">\(n-1\)</span> represents the <strong>degrees of freedom</strong>. We lose one degree of freedom because we used the sample mean (<span class="math notranslate nohighlight">\(\bar{x}\)</span>) (which is derived from the sample data) in the calculation.</p>
<p>So, for a sample with <span class="math notranslate nohighlight">\(n\)</span> data points <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_n\)</span> and sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Calculate the deviation of each data point from the sample mean</strong>: <span class="math notranslate nohighlight">\((x_i - \bar{x})\)</span></p></li>
<li><p><strong>Square each deviation</strong>: <span class="math notranslate nohighlight">\((x_i - \bar{x})^2\)</span></p></li>
<li><p><strong>Sum all the squared deviations</strong>: <span class="math notranslate nohighlight">\(\sum_{i=1}^{n} (x_i - \bar{x})^2\)</span></p></li>
<li><p><strong>Divide by <span class="math notranslate nohighlight">\(n-1\)</span></strong>:</p></li>
</ol>
<p>Thus, the formula for sample variance is:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}\]</div>
<p><strong>Alternative Form (Computational Formula)</strong>:</p>
<p>Similar to population variance, we can derive an alternative form:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}\]</div>
<p>Expand the squared term:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i^2 - 2x_i\bar{x} + \bar{x}^2)\]</div>
<p>Distribute the summation:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{n-1} \left( \sum_{i=1}^{n} x_i^2 - 2\bar{x} \sum_{i=1}^{n} x_i + \sum_{i=1}^{n} \bar{x}^2 \right)\]</div>
<p>Recall that <span class="math notranslate nohighlight">\(\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}\)</span>, which means <span class="math notranslate nohighlight">\(\sum_{i=1}^{n} x_i = n\bar{x}\)</span>. Substitute this into the equation:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{n-1} \left( \sum_{i=1}^{n} x_i^2 - 2\bar{x} (n\bar{x}) + n\bar{x}^2 \right)\]</div>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{n-1} \left( \sum_{i=1}^{n} x_i^2 - 2n\bar{x}^2 + n\bar{x}^2 \right)\]</div>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{n-1} \left( \sum_{i=1}^{n} x_i^2 - n\bar{x}^2 \right)\]</div>
<p>This is another common form for calculation.</p>
</section>
<section id="example-calculating-variance-and-standard-deviation">
<h3>Example: Calculating Variance and Standard Deviation<a class="headerlink" href="#example-calculating-variance-and-standard-deviation" title="Link to this heading">#</a></h3>
<p>Let’s use the same dataset of daily high temperatures (in Celsius) from our mean example: <span class="math notranslate nohighlight">\(x = \{20, 22, 19, 23, 21, 20, 24\}\)</span>.
We calculated the mean <span class="math notranslate nohighlight">\(\mu \approx \bar{x} \approx 21.286 \, ^{\circ}\text{C}\)</span>.</p>
<section id="step-by-step-calculation-using-definitional-formula">
<h4>Step-by-step Calculation using Definitional Formula:<a class="headerlink" href="#step-by-step-calculation-using-definitional-formula" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><span class="math notranslate nohighlight">\(x_i\)</span></p></th>
<th class="head text-left"><p><span class="math notranslate nohighlight">\((x_i - \bar{x})\)</span></p></th>
<th class="head text-left"><p><span class="math notranslate nohighlight">\((x_i - \bar{x})^2\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>20</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(20 - 21.286 = -1.286\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((-1.286)^2 \approx 1.654\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>22</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(22 - 21.286 = 0.714\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((0.714)^2 \approx 0.510\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>19</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(19 - 21.286 = -2.286\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((-2.286)^2 \approx 5.226\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>23</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(23 - 21.286 = 1.714\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((1.714)^2 \approx 2.938\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>21</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(21 - 21.286 = -0.286\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((-0.286)^2 \approx 0.082\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>20</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(20 - 21.286 = -1.286\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((-1.286)^2 \approx 1.654\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>24</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(24 - 21.286 = 2.714\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((2.714)^2 \approx 7.366\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Sum:</strong> <span class="math notranslate nohighlight">\(\approx 0\)</span></p></td>
<td class="text-left"><p><strong>Sum of Squared Deviations:</strong> <span class="math notranslate nohighlight">\(\sum (x_i - \bar{x})^2 \approx 19.43\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p><em>Note: Slight difference in sum of squared deviations compared to previous calculation due to carrying more decimal places for the mean.</em></p>
</section>
<section id="population-variance-sigma-2-and-standard-deviation-sigma">
<h4>Population Variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>) and Standard Deviation (<span class="math notranslate nohighlight">\(\sigma\)</span>)<a class="headerlink" href="#population-variance-sigma-2-and-standard-deviation-sigma" title="Link to this heading">#</a></h4>
<p>If this dataset represents the entire population (<span class="math notranslate nohighlight">\(N=7\)</span>):</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{N} = \frac{19.43}{7} \approx 2.776 \, ^{\circ}\text{C}^2\]</div>
<div class="math notranslate nohighlight">
\[\sigma = \sqrt{2.776} \approx 1.666 \, ^{\circ}\text{C}\]</div>
</section>
<section id="sample-variance-s-2-and-standard-deviation-s">
<h4>Sample Variance (<span class="math notranslate nohighlight">\(s^2\)</span>) and Standard Deviation (<span class="math notranslate nohighlight">\(s\)</span>)<a class="headerlink" href="#sample-variance-s-2-and-standard-deviation-s" title="Link to this heading">#</a></h4>
<p>If this dataset is a sample from a larger population (<span class="math notranslate nohighlight">\(n=7\)</span>):</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1} = \frac{19.43}{7-1} = \frac{19.43}{6} \approx 3.283 \, ^{\circ}\text{C}^2\]</div>
<div class="math notranslate nohighlight">
\[s = \sqrt{3.283} \approx 1.812 \, ^{\circ}\text{C}\]</div>
<p>Notice that the sample variance and standard deviation are slightly larger than their population counterparts due to Bessel’s correction, providing an unbiased estimate.</p>
</section>
</section>
<section id="interpretation-and-limitations-of-variance-and-standard-deviation">
<h3>Interpretation and Limitations of Variance and Standard Deviation<a class="headerlink" href="#interpretation-and-limitations-of-variance-and-standard-deviation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Interpretation of Magnitude:</strong> A larger variance or standard deviation signifies greater variability or dispersion in the data. For instance, a high standard deviation in daily stock returns implies greater volatility and risk, whereas a low standard deviation in manufactured product weights suggests high precision and consistent quality.</p></li>
<li><p><strong>Units and Intuition:</strong> Standard deviation is in the original units, making it directly comparable to the mean. This allows for more intuitive statements like “most data points fall within <span class="math notranslate nohighlight">\(\pm 1\)</span> standard deviation of the mean” (especially true for normally distributed data), which is crucial for constructing confidence intervals and understanding data distribution.</p></li>
<li><p><strong>Sensitivity to Outliers:</strong> Both variance and standard deviation involve squaring deviations. This mathematical property means they are highly sensitive to outliers. A single extreme value can disproportionately inflate these measures, potentially misrepresenting the spread of the majority of the data.</p></li>
<li><p><strong>Comparison:</strong> Standard deviation is generally preferred over variance for describing the spread of data because its units align with the data itself, making it more intuitive and practical for direct interpretation and communication. Variance, however, is mathematically more convenient for certain theoretical derivations and statistical tests (e.g., ANOVA).</p></li>
<li><p><strong>Relationship to Moments:</strong> For advanced readers, variance is formally the <strong>second central moment</strong> of a probability distribution, providing a fundamental measure of the distribution’s spread around its mean.</p></li>
</ul>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/f/f9/Comparison_standard_deviations.svg" alt="Comparison standard deviations.svg" height="453" width="612">
<br>
Fig. Example of samples from two populations with the same mean but different variances. The red population has mean 100 and variance 100 (SD=10) while the blue population has mean 100 and variance 2500 (SD=50) where SD stands for Standard Deviation.
By <a href="https://commons.wikimedia.org/w/index.php?title=User:JRBrown&amp;action=edit&amp;redlink=1" class="new" title="User:JRBrown (page does not exist)">JRBrown</a> - <span class="int-own-work" lang="en">Own work</span>, Public Domain, <a href="https://commons.wikimedia.org/w/index.php?curid=10777712">Link</a>. <a href="https://en.wikipedia.org/wiki/Variance">Wikipedia</a></p>
</section>
<section id="properties-of-variance">
<h3>Properties of Variance<a class="headerlink" href="#properties-of-variance" title="Link to this heading">#</a></h3>
<p>Understanding the mathematical properties of variance can simplify calculations and provide deeper insight into its behavior.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable and <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> be constants.</p>
<ol class="arabic">
<li><p><strong>Variance of a Constant</strong>: The variance of a constant is zero. A constant does not vary, so its spread is zero.</p>
<div class="math notranslate nohighlight">
\[Var(a) = 0\]</div>
</li>
<li><p><strong>Non-Negativity</strong>: Variance is always non-negative. Since it is an average of squared values, it cannot be negative.</p>
<div class="math notranslate nohighlight">
\[Var(X) \ge 0\]</div>
</li>
<li><p><strong>Effect of a Constant Multiplier</strong>: If you scale a random variable by a constant <span class="math notranslate nohighlight">\(a\)</span>, the variance is scaled by <span class="math notranslate nohighlight">\(a^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[Var(aX) = a^2 Var(X)\]</div>
<p><em>This is because the deviation from the mean <span class="math notranslate nohighlight">\((ax_i - a\mu)\)</span> becomes <span class="math notranslate nohighlight">\(a(x_i - \mu)\)</span>, and squaring this term gives <span class="math notranslate nohighlight">\(a^2(x_i - \mu)^2\)</span>.</em></p>
</li>
<li><p><strong>Effect of an Added Constant</strong>: Adding a constant to every data point shifts the mean by that constant, but it does not change the spread of the data. The variance remains unchanged.</p>
<div class="math notranslate nohighlight">
\[Var(X + b) = Var(X)\]</div>
</li>
<li><p><strong>Variance of a Sum of <em>Independent</em> Random Variables</strong>: If two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, the variance of their sum is the sum of their variances.</p>
<div class="math notranslate nohighlight">
\[Var(X + Y) = Var(X) + Var(Y)\]</div>
</li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="covariance">
<h2>Covariance<a class="headerlink" href="#covariance" title="Link to this heading">#</a></h2>
<p>While variance measures the spread of a single variable, <strong>covariance</strong> measures the <strong>joint variability</strong> of two random variables. In other words, it tells us how two variables change together.</p>
<ul class="simple">
<li><p><strong>Positive Covariance</strong>: Indicates that the two variables tend to move in the same direction. If one increases, the other tends to increase; if one decreases, the other tends to decrease.</p></li>
<li><p><strong>Negative Covariance</strong>: Indicates that the two variables tend to move in opposite directions. If one increases, the other tends to decrease.</p></li>
<li><p><strong>Covariance close to zero</strong>: Suggests there is no strong linear relationship between the two variables. (Note: Zero covariance does not necessarily mean independence, only that there’s no <em>linear</em> relationship.)</p></li>
</ul>
<section id="population-covariance-text-cov-x-y-or-sigma-xy">
<h3>Population Covariance (<span class="math notranslate nohighlight">\(\text{Cov}(X, Y)\)</span> or <span class="math notranslate nohighlight">\(\sigma_{XY}\)</span>)<a class="headerlink" href="#population-covariance-text-cov-x-y-or-sigma-xy" title="Link to this heading">#</a></h3>
<p>The population covariance is the true covariance between two variables for an entire population.</p>
<p>The population covariance is the average of the products of the deviations of each variable from their respective means.</p>
<p>Let <span class="math notranslate nohighlight">\(X = \{x_1, x_2, \ldots, x_N\}\)</span> and <span class="math notranslate nohighlight">\(Y = \{y_1, y_2, \ldots, y_N\}\)</span> be two sets of data points for a population, with population means <span class="math notranslate nohighlight">\(\mu_X\)</span> and <span class="math notranslate nohighlight">\(\mu_Y\)</span> respectively.</p>
<ol class="arabic simple">
<li><p><strong>Calculate the deviation of each <span class="math notranslate nohighlight">\(x\)</span> data point from its mean</strong>: <span class="math notranslate nohighlight">\((x_i - \mu_X)\)</span></p></li>
<li><p><strong>Calculate the deviation of each <span class="math notranslate nohighlight">\(y\)</span> data point from its mean</strong>: <span class="math notranslate nohighlight">\((y_i - \mu_Y)\)</span></p></li>
<li><p><strong>Multiply the corresponding deviations</strong>: <span class="math notranslate nohighlight">\((x_i - \mu_X)(y_i - \mu_Y)\)</span></p></li>
<li><p><strong>Sum all these products</strong>: <span class="math notranslate nohighlight">\(\sum_{i=1}^{N} (x_i - \mu_X)(y_i - \mu_Y)\)</span></p></li>
<li><p><strong>Divide by the total number of data points (<span class="math notranslate nohighlight">\(N\)</span>) to find the average product of deviations</strong>:</p></li>
</ol>
<p>Thus, the formula for population covariance is:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \sigma_{XY} = \frac{\sum_{i=1}^{N} (x_i - \mu_X)(y_i - \mu_Y)}{N}\]</div>
<p><strong>Alternative Form (Computational Formula)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu_X)(y_i - \mu_Y)\]</div>
<p>Expand the product:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \frac{1}{N} \sum_{i=1}^{N} (x_i y_i - x_i \mu_Y - y_i \mu_X + \mu_X \mu_Y)\]</div>
<p>Distribute the summation:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \frac{1}{N} \left( \sum_{i=1}^{N} x_i y_i - \sum_{i=1}^{N} x_i \mu_Y - \sum_{i=1}^{N} y_i \mu_X + \sum_{i=1}^{N} \mu_X \mu_Y \right)\]</div>
<p>Pull constants out of the summation:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \frac{1}{N} \left( \sum_{i=1}^{N} x_i y_i - \mu_Y \sum_{i=1}^{N} x_i - \mu_X \sum_{i=1}^{N} y_i + N\mu_X \mu_Y \right)\]</div>
<p>Recall that <span class="math notranslate nohighlight">\(\sum_{i=1}^{N} x_i = N\mu_X\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^{N} y_i = N\mu_Y\)</span>. Substitute these:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \frac{1}{N} \left( \sum_{i=1}^{N} x_i y_i - \mu_Y (N\mu_X) - \mu_X (N\mu_Y) + N\mu_X \mu_Y \right)\]</div>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \frac{1}{N} \left( \sum_{i=1}^{N} x_i y_i - N\mu_X \mu_Y - N\mu_X \mu_Y + N\mu_X \mu_Y \right)\]</div>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \frac{1}{N} \left( \sum_{i=1}^{N} x_i y_i - N\mu_X \mu_Y \right)\]</div>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \frac{\sum_{i=1}^{N} x_i y_i}{N} - \mu_X \mu_Y\]</div>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/a/a0/Covariance_trends.svg" alt="Covariance trends.svg" height="800" width="266.7">
<br>
Fig. The sign of the covariance of two random variables X and Y.
By <a href="https://commons.wikimedia.org/wiki/User:Cmglee" title="User:Cmglee">Cmglee</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0" title="Creative Commons Attribution-Share Alike 4.0">CC BY-SA 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=90452334">Link</a>. <a href="https://en.wikipedia.org/wiki/Covariance">Wikipedia</a></p>
</section>
<section id="sample-covariance-s-xy">
<h3>Sample Covariance (<span class="math notranslate nohighlight">\(s_{XY}\)</span>)<a class="headerlink" href="#sample-covariance-s-xy" title="Link to this heading">#</a></h3>
<p>When working with a sample from a larger population, we use sample covariance to estimate the population covariance.</p>
<p>The sample covariance is an estimate of the population covariance, calculated using the products of the deviations from the sample means, divided by <span class="math notranslate nohighlight">\(n-1\)</span>.</p>
<p>Similar to sample variance, we use <span class="math notranslate nohighlight">\(n-1\)</span> in the denominator to provide an unbiased estimate of the population covariance.</p>
<p>For a sample with <span class="math notranslate nohighlight">\(n\)</span> pairs of data points <span class="math notranslate nohighlight">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span>, and sample means <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(\bar{y}\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Calculate the deviation of each <span class="math notranslate nohighlight">\(x\)</span> data point from its sample mean</strong>: <span class="math notranslate nohighlight">\((x_i - \bar{x})\)</span></p></li>
<li><p><strong>Calculate the deviation of each <span class="math notranslate nohighlight">\(y\)</span> data point from its sample mean</strong>: <span class="math notranslate nohighlight">\((y_i - \bar{y})\)</span></p></li>
<li><p><strong>Multiply the corresponding deviations</strong>: <span class="math notranslate nohighlight">\((x_i - \bar{x})(y_i - \bar{y})\)</span></p></li>
<li><p><strong>Sum all these products</strong>: <span class="math notranslate nohighlight">\(\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})\)</span></p></li>
<li><p><strong>Divide by <span class="math notranslate nohighlight">\(n-1\)</span></strong>:</p></li>
</ol>
<p>Thus, the formula for sample covariance is:</p>
<div class="math notranslate nohighlight">
\[s_{XY} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{n-1}\]</div>
<p><strong>Alternative Form (Computational Formula)</strong>:</p>
<div class="math notranslate nohighlight">
\[s_{XY} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})\]</div>
<p>Expand the product:</p>
<div class="math notranslate nohighlight">
\[s_{XY} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i y_i - x_i \bar{y} - y_i \bar{x} + \bar{x}\bar{y})\]</div>
<p>Distribute the summation:</p>
<div class="math notranslate nohighlight">
\[s_{XY} = \frac{1}{n-1} \left( \sum_{i=1}^{n} x_i y_i - \sum_{i=1}^{n} x_i \bar{y} - \sum_{i=1}^{n} y_i \bar{x} + \sum_{i=1}^{n} \bar{x}\bar{y} \right)\]</div>
<p>Pull constants out of the summation:</p>
<div class="math notranslate nohighlight">
\[s_{XY} = \frac{1}{n-1} \left( \sum_{i=1}^{n} x_i y_i - \bar{y} \sum_{i=1}^{n} x_i - \bar{x} \sum_{i=1}^{n} y_i + n\bar{x}\bar{y} \right)\]</div>
<p>Recall that <span class="math notranslate nohighlight">\(\sum_{i=1}^{n} x_i = n\bar{x}\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^{n} y_i = n\bar{y}\)</span>. Substitute these:</p>
<div class="math notranslate nohighlight">
\[s_{XY} = \frac{1}{n-1} \left( \sum_{i=1}^{n} x_i y_i - \bar{y} (n\bar{x}) - \bar{x} (n\bar{y}) + n\bar{x}\bar{y} \right)\]</div>
<div class="math notranslate nohighlight">
\[s_{XY} = \frac{1}{n-1} \left( \sum_{i=1}^{n} x_i y_i - n\bar{x}\bar{y} - n\bar{x}\bar{y} + n\bar{x}\bar{y} \right)\]</div>
<div class="math notranslate nohighlight">
\[s_{XY} = \frac{1}{n-1} \left( \sum_{i=1}^{n} x_i y_i - n\bar{x}\bar{y} \right)\]</div>
</section>
<section id="correlation-coefficient-pearson-s-r">
<h3>Correlation Coefficient (Pearson’s r)<a class="headerlink" href="#correlation-coefficient-pearson-s-r" title="Link to this heading">#</a></h3>
<p>The units of covariance are the product of the units of the two variables involved. For example, if variable <span class="math notranslate nohighlight">\(X\)</span> is measured in kilograms (kg) and variable <span class="math notranslate nohighlight">\(Y\)</span> in centimeters (cm), then <span class="math notranslate nohighlight">\(\text{Cov}(X,Y)\)</span> will be in <span class="math notranslate nohighlight">\(\text{kg} \cdot \text{cm}\)</span>. This property makes the <em>magnitude</em> of covariance difficult to interpret on its own, as it depends entirely on the scales and units of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. It prevents direct comparison of the strength of relationships across different datasets or variable pairs. This limitation leads us to a more standardized measure: the correlation coefficient.</p>
<p>While covariance effectively indicates the <em>direction</em> (positive, negative, or none) of a linear relationship between two variables, its unstandardized magnitude makes it difficult to assess the <em>strength</em> of that relationship. The <strong>Pearson Correlation Coefficient (or Pearson’s r)</strong> is a standardized version of covariance that measures both the <strong>strength</strong> and <strong>direction</strong> of a <em>linear</em> relationship between two variables.</p>
<p>The Pearson correlation coefficient always ranges between -1 and +1, making it universally interpretable:</p>
<ul class="simple">
<li><p><strong>+1:</strong> Indicates a perfect positive linear relationship. As one variable increases, the other increases proportionally.</p></li>
<li><p><strong>-1:</strong> Indicates a perfect negative linear relationship. As one variable increases, the other decreases proportionally.</p></li>
<li><p><strong>0:</strong> Indicates no linear relationship between the two variables. This is a crucial point: it does not necessarily mean there is <em>no</em> relationship at all, just no <em>linear</em> one.</p></li>
</ul>
<p>Pearson’s correlation coefficient is the covariance of the two variables divided by the product of their standard deviations.</p>
<section id="population-correlation-coefficient-rho">
<h4>Population Correlation Coefficient (<span class="math notranslate nohighlight">\(\rho\)</span>)<a class="headerlink" href="#population-correlation-coefficient-rho" title="Link to this heading">#</a></h4>
<p>The population correlation coefficient, denoted by the lowercase Greek letter rho (<span class="math notranslate nohighlight">\(\rho\)</span>), is calculated by dividing the population covariance by the product of the population standard deviations of the two variables. This normalization process removes the units and standardizes the measure.</p>
<div class="math notranslate nohighlight">
\[\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}\]</div>
</section>
<section id="sample-correlation-coefficient-r">
<h4>Sample Correlation Coefficient (<span class="math notranslate nohighlight">\(r\)</span>)<a class="headerlink" href="#sample-correlation-coefficient-r" title="Link to this heading">#</a></h4>
<p>The sample correlation coefficient, denoted by the lowercase Latin letter <span class="math notranslate nohighlight">\(r\)</span>, is calculated similarly, using sample covariance and sample standard deviations.</p>
<div class="math notranslate nohighlight">
\[r_{XY} = \frac{s_{XY}}{s_X s_Y}\]</div>
<p>We will review Pearson’s correlation coefficient and other correlation coefficients in a separate lecture.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg" alt="Correlation examples2.svg" height="300" width="656.7">
<br>
Fig. Several sets of (x, y) points, with the Pearson correlation coefficient of x and y for each set. The correlation reflects the strength and direction of a linear relationship (top row), but not the slope of that relationship (middle row), nor many aspects of nonlinear relationships (bottom row). N.B.: the figure in the center has a slope of 0 but in that case the correlation coefficient is undefined because the variance of Y is zero.
By <a href="https://commons.wikimedia.org/w/index.php?title=User:DenisBoigelot&amp;action=edit&amp;redlink=1" class="new" title="User:DenisBoigelot (page does not exist)">DenisBoigelot</a>, <a href="https://creativecommons.org/publicdomain/zero/1.0/deed.en" title="Creative Commons Zero, Public Domain Dedication">CC0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=15165296">Link</a>. <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Wikipedia</a></p>
</section>
</section>
<section id="interpretation-and-limitations-of-covariance-and-correlation">
<h3>Interpretation and Limitations of Covariance and Correlation<a class="headerlink" href="#interpretation-and-limitations-of-covariance-and-correlation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Direction vs. Strength:</strong> Covariance indicates the <em>direction</em> of a linear relationship (positive, negative, or none). Correlation, being standardized, indicates both the <em>direction</em> and the <em>strength</em> of a linear relationship (from -1 to +1). Correlation is generally preferred for assessing strength because its value is bounded and unitless.</p></li>
<li><p><strong>Units and Comparability:</strong> Covariance has units that are the product of the units of the two variables, making its magnitude difficult to interpret or compare across different datasets. Correlation is unitless, making it universally comparable and interpretable.</p></li>
<li><p><strong>Linear Relationships Only:</strong> Both covariance and Pearson’s correlation coefficient only measure the <em>linear</em> association between variables. A correlation coefficient close to zero does not imply the absence of <em>any</em> relationship, only the absence of a <em>linear</em> one. Strong non-linear relationships (e.g., parabolic, exponential, cyclical) will yield low Pearson correlation coefficients. For such cases, other measures like <strong>Spearman’s rank correlation</strong> (which measures monotonic relationships) or visualizations are necessary.</p></li>
<li><p><strong>Sensitivity to Outliers:</strong> Like variance and standard deviation, both covariance and correlation are sensitive to outliers. A few extreme data points can significantly alter their values, potentially misrepresenting the overall relationship between variables.</p></li>
<li><p><strong>Correlation Does Not Imply Causation (A Critical Distinction):</strong> This is arguably the most crucial cautionary point in statistics. A strong correlation between two variables does not automatically mean that one causes the other. The observed relationship might be due to a confounding variable (a third, unobserved factor influencing both), pure coincidence, or even reverse causation. For example, ice cream sales and drowning incidents often increase at the same time, but ice cream doesn’t cause drowning; a third variable (temperature) influences both. Always be wary of inferring causation from correlation alone; controlled experiments or advanced causal inference techniques are required to establish causation.</p></li>
<li><p><strong>Relationship to Moments:</strong> For advanced readers, covariance is the <strong>second mixed central moment</strong> between two random variables, extending the concept of variance to multivariate settings.</p></li>
</ul>
</section>
<section id="properties-of-covariance">
<h3>Properties of Covariance<a class="headerlink" href="#properties-of-covariance" title="Link to this heading">#</a></h3>
<p>Covariance also has several important properties. Let <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, and <span class="math notranslate nohighlight">\(Z\)</span> be random variables and <span class="math notranslate nohighlight">\(a, b, c, d\)</span> be constants.</p>
<ol class="arabic">
<li><p><strong>Symmetry</strong>: The covariance of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is the same as the covariance of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>.</p>
<div class="math notranslate nohighlight">
\[Cov(X, Y) = Cov(Y, X)\]</div>
</li>
<li><p><strong>Covariance with a Constant</strong>: The covariance of a random variable with a constant is zero. A constant has no variability to share.</p>
<div class="math notranslate nohighlight">
\[Cov(X, a) = 0\]</div>
</li>
<li><p><strong>Covariance with Itself</strong>: The covariance of a random variable with itself is its variance.</p>
<div class="math notranslate nohighlight">
\[Cov(X, X) = Var(X)\]</div>
</li>
<li><p><strong>Effect of Scaling and Shifting</strong>:</p>
<div class="math notranslate nohighlight">
\[Cov(aX + b, cY + d) = ac \cdot Cov(X, Y)\]</div>
</li>
<li><p><strong>Distributive Property</strong>:</p>
<div class="math notranslate nohighlight">
\[Cov(X + Y, Z) = Cov(X, Z) + Cov(Y, Z)\]</div>
</li>
<li><p><strong>Variance of a Sum of <em>Dependent</em> Random Variables</strong>: The property for the variance of a sum can be generalized for dependent variables using covariance.</p>
<div class="math notranslate nohighlight">
\[Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)\]</div>
</li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="bessel-s-correction">
<h2>Bessel’s Correction<a class="headerlink" href="#bessel-s-correction" title="Link to this heading">#</a></h2>
<p>The use of <span class="math notranslate nohighlight">\(n-1\)</span> in the denominator for sample variance (and sample covariance) is not just a convention; it’s a crucial mathematical adjustment known as <strong>Bessel’s Correction</strong>. Its purpose is to ensure that the sample variance is an <strong>unbiased estimator</strong> of the population variance.</p>
<p>Let’s break down why this correction is necessary, both intuitively and with a conceptual sketch of the mathematical proof.</p>
<section id="intuitive-explanation">
<h3>Intuitive Explanation<a class="headerlink" href="#intuitive-explanation" title="Link to this heading">#</a></h3>
<p>Imagine you want to estimate the average height of all people in a large city. You take a sample of 100 people and calculate their average height (<span class="math notranslate nohighlight">\(\bar{x}\)</span>). You then want to estimate the variance of heights in the entire city (<span class="math notranslate nohighlight">\(\sigma^2\)</span>) using your sample.</p>
<ol class="arabic">
<li><p><strong>The problem of using the sample mean:</strong> When you calculate the variance, you’re looking at how much each data point deviates from the “true” mean. If you knew the <em>population mean</em> (<span class="math notranslate nohighlight">\(\mu\)</span>), you would calculate <span class="math notranslate nohighlight">\(\frac{\sum (x_i - \mu)^2}{N}\)</span>. However, you usually don’t know <span class="math notranslate nohighlight">\(\mu\)</span>. So, you use the <em>sample mean</em> (<span class="math notranslate nohighlight">\(\bar{x}\)</span>) as an estimate for <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p><strong>Sample mean is “closer” to the sample data:</strong> The crucial point is that the sample mean (<span class="math notranslate nohighlight">\(\bar{x}\)</span>) is calculated <em>from</em> the sample data itself. By its very nature, <span class="math notranslate nohighlight">\(\bar{x}\)</span> will always be the value that minimizes the sum of squared deviations for <em>that specific sample</em>. Think of it this way: if you try to fit a line to a set of points, the line you choose will be the one that minimizes the squared distances to <em>those specific points</em>. The sample mean is the “best fit” for your sample data.</p></li>
<li><p><strong>Underestimation:</strong> Because <span class="math notranslate nohighlight">\(\bar{x}\)</span> is the “best fit” for your sample, the sum of squared deviations from <span class="math notranslate nohighlight">\(\bar{x}\)</span> (<span class="math notranslate nohighlight">\(\sum (x_i - \bar{x})^2\)</span>) will always be <em>smaller than or equal to</em> the sum of squared deviations from the true population mean (<span class="math notranslate nohighlight">\(\sum (x_i - \mu)^2\)</span>). It will only be equal if your sample mean happens to perfectly match the population mean, which is highly unlikely in practice.</p>
<p>Therefore, if you divide by <span class="math notranslate nohighlight">\(n\)</span> (like you would for population variance), your sample variance would systematically <em>underestimate</em> the true population variance. It would be a biased estimator, always tending to be a little too small.</p>
</li>
<li><p><strong>The correction (<span class="math notranslate nohighlight">\(n-1\)</span>):</strong> To counteract this downward bias, we divide by a slightly smaller number, <span class="math notranslate nohighlight">\(n-1\)</span>. Dividing by a smaller number makes the overall result larger, thereby “inflating” the sample variance just enough to make it an unbiased estimate of the population variance.</p></li>
</ol>
</section>
<section id="degrees-of-freedom-explanation">
<h3>Degrees of Freedom Explanation<a class="headerlink" href="#degrees-of-freedom-explanation" title="Link to this heading">#</a></h3>
<p>Another way to understand <span class="math notranslate nohighlight">\(n-1\)</span> is through the concept of <strong>degrees of freedom</strong>.</p>
<ul>
<li><p>When calculating the population variance, you use the population mean (<span class="math notranslate nohighlight">\(\mu\)</span>), which is a fixed, known value. Each of your <span class="math notranslate nohighlight">\(N\)</span> data points <span class="math notranslate nohighlight">\((x_i - \mu)\)</span> provides independent information about the spread. You have <span class="math notranslate nohighlight">\(N\)</span> independent pieces of information, so you divide by <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p>When calculating sample variance, you first calculate the sample mean (<span class="math notranslate nohighlight">\(\bar{x}\)</span>) from your <span class="math notranslate nohighlight">\(n\)</span> data points. Once you know <span class="math notranslate nohighlight">\(\bar{x}\)</span>, you “lose” one degree of freedom. This means that if you know <span class="math notranslate nohighlight">\(n-1\)</span> of the deviations <span class="math notranslate nohighlight">\((x_i - \bar{x})\)</span> and you know <span class="math notranslate nohighlight">\(\bar{x}\)</span>, the last deviation is not independent; it’s determined by the others because <span class="math notranslate nohighlight">\(\sum (x_i - \bar{x}) = 0\)</span>.</p>
<p>For example, if you have a sample of three numbers and their mean is 5:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(x_1 - \bar{x} = 2\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(x_2 - \bar{x} = -3\)</span></p></li>
<li><p>Then <span class="math notranslate nohighlight">\(x_3 - \bar{x}\)</span> <em>must be</em> <span class="math notranslate nohighlight">\(1\)</span> (because <span class="math notranslate nohighlight">\(2 + (-3) + 1 = 0\)</span>).</p></li>
</ul>
<p>So, you only have <span class="math notranslate nohighlight">\(n-1\)</span> independent pieces of information contributing to the variability around the sample mean. Dividing by <span class="math notranslate nohighlight">\(n-1\)</span> accounts for this loss of a degree of freedom.</p>
</li>
</ul>
<p>However, now we can say:</p>
<ul class="simple">
<li><p>We have a formula for the population mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>Thus, the calculation of the population mean should also decrease the number of degrees of freedom. Or not?</p></li>
</ul>
<p>The key difference lies in whether the mean we are using in our calculation (<span class="math notranslate nohighlight">\(\bar{x}\)</span> or <span class="math notranslate nohighlight">\(\mu\)</span>) is <strong>estimated from the data itself</strong> or is a <strong>fixed, known value independent of the data</strong>.</p>
<p>Why <span class="math notranslate nohighlight">\(\bar{x}\)</span> reduces degrees of freedom by one (for sample variance/covariance)?</p>
<p>When you calculate the <strong>sample variance (<span class="math notranslate nohighlight">\(s^2\)</span>)</strong> using the formula <span class="math notranslate nohighlight">\(s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>You first calculate <span class="math notranslate nohighlight">\(\bar{x}\)</span> from the <span class="math notranslate nohighlight">\(n\)</span> data points in our sample.</strong></p></li>
<li><p><strong>This act of calculating <span class="math notranslate nohighlight">\(\bar{x}\)</span> imposes a constraint on the data points.</strong> Specifically, the sum of the deviations from the mean must always be zero: <span class="math notranslate nohighlight">\(\sum_{i=1}^{n} (x_i - \bar{x}) = 0\)</span>.</p></li>
<li><p>Because of this constraint, if you know <span class="math notranslate nohighlight">\(n-1\)</span> of the deviations <span class="math notranslate nohighlight">\((x_i - \bar{x})\)</span>, the <em>last</em> deviation is automatically determined. It’s not free to vary independently.</p>
<ul class="simple">
<li><p>For example, if you have three numbers, <span class="math notranslate nohighlight">\(x_1, x_2, x_3\)</span>, and their mean <span class="math notranslate nohighlight">\(\bar{x}\)</span>.</p></li>
<li><p>You calculate <span class="math notranslate nohighlight">\((x_1 - \bar{x})\)</span> and <span class="math notranslate nohighlight">\((x_2 - \bar{x})\)</span>.</p></li>
<li><p>Then, <span class="math notranslate nohighlight">\((x_3 - \bar{x})\)</span> <em>must be</em> equal to <span class="math notranslate nohighlight">\(-[(x_1 - \bar{x}) + (x_2 - \bar{x})]\)</span>.</p></li>
<li><p>So, only <span class="math notranslate nohighlight">\(n-1\)</span> of these deviations are truly independent pieces of information contributing to the variability around <span class="math notranslate nohighlight">\(\bar{x}\)</span>.</p></li>
</ul>
</li>
</ol>
<p>This “loss” of one piece of independent information is precisely why we say you lose one degree of freedom. You’ve used one piece of information from the sample (the sample mean) to define the center point around which you’re measuring variability.</p>
<p>Why <span class="math notranslate nohighlight">\(\mu\)</span> does NOT reduce degrees of freedom (for population variance/covariance)</p>
<p>When you calculate the <strong>population variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>)</strong> using the formula <span class="math notranslate nohighlight">\(\sigma^2 = \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{N}\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>The population mean (<span class="math notranslate nohighlight">\(\mu\)</span>) is a constant, a fixed parameter of the entire population.</strong></p></li>
<li><p><strong>You assume <span class="math notranslate nohighlight">\(\mu\)</span> is known.</strong> In theoretical scenarios where you are calculating population variance, you <em>hypothetically</em> have access to the true <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p><strong>No constraint is imposed on the deviations from <span class="math notranslate nohighlight">\(\mu\)</span> by the calculation of <span class="math notranslate nohighlight">\(\mu\)</span>.</strong> Each deviation <span class="math notranslate nohighlight">\((x_i - \mu)\)</span> is an independent piece of information about the spread from this <em>pre-defined</em> center point. You haven’t used any of the <span class="math notranslate nohighlight">\(x_i\)</span> values to determine <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
</ol>
<blockquote>
<div><p>Think of it like this: if you have a target (<span class="math notranslate nohighlight">\(\mu\)</span>) and you’re shooting arrows (<span class="math notranslate nohighlight">\(x_i\)</span>), and you want to know how spread out our shots are from the true center of the target, you just measure the distance of each shot from the target’s center. Each shot provides independent information.</p>
</div></blockquote>
<blockquote>
<div><p>However, if you don’t know the true target center, you fire 10 arrows, estimate the center of the target based on where our 10 arrows landed (<span class="math notranslate nohighlight">\(\bar{x}\)</span>), and then measure the spread from <em>that estimated center</em>. In this second scenario, our estimated center is influenced by the arrows themselves, meaning one of the arrow’s positions isn’t truly “free” if you know the other 9 and our estimated center.</p>
</div></blockquote>
<p>To find <span class="math notranslate nohighlight">\(\mu\)</span>, you perform a calculation as well as you perform a calculation to find <span class="math notranslate nohighlight">\(\bar{x}\)</span>. The distinction regarding degrees of freedom comes down to <em>when</em> that mean is known or estimated relative to the variance calculation.</p>
<p>When we discuss the <strong>population variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>)</strong> and its formula <span class="math notranslate nohighlight">\(\sigma^2 = \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{N}\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Hypothetical Knowledge or Definitive Calculation</strong>: In the context of population variance, we <em>assume</em> we either already know the true population mean (<span class="math notranslate nohighlight">\(\mu\)</span>) or we are in a theoretical scenario where we <em>have</em> access to the <em>entire population data</em> (<span class="math notranslate nohighlight">\(N\)</span> observations) to definitively calculate <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<ul class="simple">
<li><p>If you have the <em>entire population</em> of <span class="math notranslate nohighlight">\(N\)</span> data points, then calculating <span class="math notranslate nohighlight">\(\mu = \frac{\sum_{i=1}^{N} x_i}{N}\)</span> gives you the <em>true, definitive</em> population mean. It’s not an estimate; it’s the exact value for that population.</p></li>
<li><p>Once you have this definitive <span class="math notranslate nohighlight">\(\mu\)</span>, when you then calculate the variance, you are measuring the spread of each <span class="math notranslate nohighlight">\(x_i\)</span> from this <strong>fixed, true center</strong> (!!!).</p></li>
</ul>
</li>
<li><p><strong>No Estimation from a Subset</strong>: The crucial part is that when you calculate <span class="math notranslate nohighlight">\(\sigma^2\)</span>, you are <strong>not estimating <span class="math notranslate nohighlight">\(\mu\)</span> from a <em>subset</em> of the data that you’re simultaneously using to calculate the variance.</strong></p>
<ul class="simple">
<li><p>Each of the <span class="math notranslate nohighlight">\(N\)</span> deviations <span class="math notranslate nohighlight">\((x_i - \mu)\)</span> is indeed an independent piece of information about the spread around that fixed <span class="math notranslate nohighlight">\(\mu\)</span>. There’s no “loss” of information or constraint imposed on these deviations because the <span class="math notranslate nohighlight">\(\mu\)</span> is taken as the true center of the entire data set you’re working with.</p></li>
</ul>
</li>
</ol>
<p>Contrast with Sample Variance:</p>
<p>For <strong>sample variance (<span class="math notranslate nohighlight">\(s^2\)</span>)</strong>, when you use <span class="math notranslate nohighlight">\(\bar{x}\)</span> calculated from a <em>sample</em> of <span class="math notranslate nohighlight">\(n\)</span> observations:</p>
<ol class="arabic simple">
<li><p><strong>Estimation from a Subset</strong>: You are taking a <em>subset</em> (the sample) of the larger population. You don’t know the true population mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p><strong>Using <span class="math notranslate nohighlight">\(\bar{x}\)</span> as an Estimator</strong>: You calculate <span class="math notranslate nohighlight">\(\bar{x}\)</span> from <em>this very sample</em> of <span class="math notranslate nohighlight">\(n\)</span> data points to serve as an estimate of <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p><strong>Self-Fulfilling Constraint</strong>: Because <span class="math notranslate nohighlight">\(\bar{x}\)</span> is derived <em>from</em> the <span class="math notranslate nohighlight">\(n\)</span> sample points, it automatically minimizes the sum of squared deviations for <em>that specific sample</em>. This introduces a statistical dependency: if you know <span class="math notranslate nohighlight">\(n-1\)</span> deviations from <span class="math notranslate nohighlight">\(\bar{x}\)</span>, the last one is fixed. This is why you lose a degree of freedom.</p></li>
</ol>
<p>Another analogy:</p>
<ul class="simple">
<li><p><strong>Population Variance</strong>: You have a perfectly calibrated ruler (your true <span class="math notranslate nohighlight">\(\mu\)</span>) and you measure <span class="math notranslate nohighlight">\(N\)</span> items. Each measurement is independent with respect to the fixed ruler.</p></li>
<li><p><strong>Sample Variance</strong>: You have <span class="math notranslate nohighlight">\(n\)</span> items, and you first try to estimate the “average size” using these <span class="math notranslate nohighlight">\(n\)</span> items (your <span class="math notranslate nohighlight">\(\bar{x}\)</span>). Then you measure how much each item deviates from <em>your own estimate</em>. Because your estimate (<span class="math notranslate nohighlight">\(\bar{x}\)</span>) was influenced by the items themselves, there’s a slight dependency. One item’s deviation isn’t completely “free” once you’ve fixed the average from those <span class="math notranslate nohighlight">\(n\)</span> items.</p></li>
</ul>
<p>So, while you <em>do</em> calculate <span class="math notranslate nohighlight">\(\mu\)</span> for the population variance, that calculation uses the <em>entire</em> population data. This <span class="math notranslate nohighlight">\(\mu\)</span> then serves as the definitive, non-estimated center point from which deviations are measured, allowing all <span class="math notranslate nohighlight">\(N\)</span> deviations to contribute independently. For sample variance, <span class="math notranslate nohighlight">\(\bar{x}\)</span> is an estimate derived from a subset, which inherently “ties” one degree of freedom.</p>
<p>In summary:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Feature</p></th>
<th class="head text-left"><p>Sample Variance (<span class="math notranslate nohighlight">\(s^2\)</span>)</p></th>
<th class="head text-left"><p>Population Variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Mean Used</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\bar{x}\)</span> (Sample Mean)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\mu\)</span> (Population Mean)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Source of Mean</p></td>
<td class="text-left"><p>Calculated <em>from</em> the <span class="math notranslate nohighlight">\(n\)</span> data points in the sample.</p></td>
<td class="text-left"><p>A fixed parameter of the population, calculated using all <span class="math notranslate nohighlight">\(N\)</span> data points of the entire population.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Constraint</p></td>
<td class="text-left"><p>The sum of deviations from <span class="math notranslate nohighlight">\(\bar{x}\)</span> is constrained to zero.</p></td>
<td class="text-left"><p>No such constraint on deviations from <span class="math notranslate nohighlight">\(\mu\)</span>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Degrees of Freedom</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(n-1\)</span> (one degree lost because <span class="math notranslate nohighlight">\(\bar{x}\)</span> was estimated).</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(N\)</span> (all <span class="math notranslate nohighlight">\(N\)</span> deviations are independent).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Purpose</p></td>
<td class="text-left"><p>Unbiased estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></td>
<td class="text-left"><p>True variance of the population (if <span class="math notranslate nohighlight">\(\mu\)</span> is known).</p></td>
</tr>
</tbody>
</table>
</div>
<p>This distinction is fundamental in statistics, as it addresses the bias that arises when using sample statistics to infer properties of a larger population. The <span class="math notranslate nohighlight">\(n-1\)</span> in the denominator is Bessel’s correction, explicitly designed to create an unbiased estimator for population variance when only a sample is available.</p>
</section>
<section id="mathematical-proof-sketch-for-variance">
<h3>Mathematical Proof Sketch (for Variance)<a class="headerlink" href="#mathematical-proof-sketch-for-variance" title="Link to this heading">#</a></h3>
<p>The formal proof involves using the concept of <strong>expected value</strong>. An estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> for a parameter <span class="math notranslate nohighlight">\(\theta\)</span> is unbiased if <span class="math notranslate nohighlight">\(E[\hat{\theta}] = \theta\)</span>. We want <span class="math notranslate nohighlight">\(E[s^2] = \sigma^2\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be independent and identically distributed (i.i.d.) random variables from a population with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>We know:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[X_i] = \mu\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Var(X_i) = E[(X_i - \mu)^2] = \sigma^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(E[\bar{X}] = \mu\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Var(\bar{X}) = E[(\bar{X} - \mu)^2] = \frac{\sigma^2}{n}\)</span></p></li>
</ul>
<p>Let’s derive this last equation.</p>
<p>We start with the definition of the variance of <span class="math notranslate nohighlight">\(\bar{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[Var(\bar{X}) = E[(\bar{X} - E[\bar{X}])^2]\]</div>
<p>First, let’s find the expected value of the sample mean, <span class="math notranslate nohighlight">\(E[\bar{X}]\)</span>:</p>
<div class="math notranslate nohighlight">
\[E[\bar{X}] = E\left[ \frac{1}{n} \sum_{i=1}^{n} X_i \right]\]</div>
<div class="math notranslate nohighlight">
\[E[\bar{X}] = \frac{1}{n} E\left[ \sum_{i=1}^{n} X_i \right]\]</div>
<div class="math notranslate nohighlight">
\[E[\bar{X}] = \frac{1}{n} \sum_{i=1}^{n} E[X_i]\]</div>
<p>Since each <span class="math notranslate nohighlight">\(X_i\)</span> comes from the same population, <span class="math notranslate nohighlight">\(E[X_i] = \mu\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="math notranslate nohighlight">
\[E[\bar{X}] = \frac{1}{n} \sum_{i=1}^{n} \mu\]</div>
<div class="math notranslate nohighlight">
\[E[\bar{X}] = \frac{1}{n} (n\mu)\]</div>
<div class="math notranslate nohighlight">
\[E[\bar{X}] = \mu\]</div>
<p>So, the expected value of the sample mean is the population mean. This is why <span class="math notranslate nohighlight">\(\bar{X}\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>Now, substitute <span class="math notranslate nohighlight">\(E[\bar{X}] = \mu\)</span> back into the variance definition:</p>
<div class="math notranslate nohighlight">
\[Var(\bar{X}) = E[(\bar{X} - \mu)^2]\]</div>
<p>Next, substitute the definition of <span class="math notranslate nohighlight">\(\bar{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[Var(\bar{X}) = Var\left( \frac{1}{n} \sum_{i=1}^{n} X_i \right)\]</div>
<p>Using the property <span class="math notranslate nohighlight">\(Var(c Y) = c^2 Var(Y)\)</span>, where <span class="math notranslate nohighlight">\(c = \frac{1}{n}\)</span>:</p>
<div class="math notranslate nohighlight">
\[Var(\bar{X}) = \left( \frac{1}{n} \right)^2 Var\left( \sum_{i=1}^{n} X_i \right)\]</div>
<div class="math notranslate nohighlight">
\[Var(\bar{X}) = \frac{1}{n^2} Var\left( \sum_{i=1}^{n} X_i \right)\]</div>
<p>Since <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> are independent (a key assumption for simple random sampling), the variance of their sum is the sum of their variances:</p>
<div class="math notranslate nohighlight">
\[Var\left( \sum_{i=1}^{n} X_i \right) = \sum_{i=1}^{n} Var(X_i)\]</div>
<p>Since each <span class="math notranslate nohighlight">\(X_i\)</span> comes from the same population, <span class="math notranslate nohighlight">\(Var(X_i) = \sigma^2\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>:
$<span class="math notranslate nohighlight">\(\sum_{i=1}^{n} Var(X_i) = \sum_{i=1}^{n} \sigma^2 = n\sigma^2\)</span>$</p>
<p>Now, substitute this back into our expression for <span class="math notranslate nohighlight">\(Var(\bar{X})\)</span>:</p>
<div class="math notranslate nohighlight">
\[Var(\bar{X}) = \frac{1}{n^2} (n\sigma^2)\]</div>
<div class="math notranslate nohighlight">
\[Var(\bar{X}) = \frac{n\sigma^2}{n^2}\]</div>
<div class="math notranslate nohighlight">
\[Var(\bar{X}) = \frac{\sigma^2}{n}\]</div>
<p>This result is incredibly important because it quantifies how the precision of our estimate of the mean improves as we increase our sample size. A larger sample size (<span class="math notranslate nohighlight">\(n\)</span>) leads to a smaller variance for the sample mean, meaning <span class="math notranslate nohighlight">\(\bar{X}\)</span> is more likely to be closer to the true population mean <span class="math notranslate nohighlight">\(\mu\)</span>. This is the mathematical basis for why larger samples give more reliable estimates.</p>
<p><strong>Proof for Bessel’s Correction:</strong></p>
<p>Now, let’s consider the “biased” sample variance, <span class="math notranslate nohighlight">\(s^2_{\text{biased}} = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2\)</span>. We want to find its expected value:</p>
<div class="math notranslate nohighlight">
\[E[s^2_{\text{biased}}] = E\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \right] = \frac{1}{n} E\left[ \sum_{i=1}^n (X_i - \bar{X})^2 \right]\]</div>
<p>Let’s work with the sum of squares in the numerator: <span class="math notranslate nohighlight">\(\sum_{i=1}^n (X_i - \bar{X})^2\)</span>.</p>
<p>We can rewrite <span class="math notranslate nohighlight">\((X_i - \bar{X})\)</span> as <span class="math notranslate nohighlight">\((X_i - \mu) - (\bar{X} - \mu)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n (X_i - \bar{X})^2 = \sum_{i=1}^n [(X_i - \mu) - (\bar{X} - \mu)]^2\]</div>
<div class="math notranslate nohighlight">
\[= \sum_{i=1}^n [(X_i - \mu)^2 - 2(X_i - \mu)(\bar{X} - \mu) + (\bar{X} - \mu)^2]\]</div>
<div class="math notranslate nohighlight">
\[= \sum_{i=1}^n (X_i - \mu)^2 - 2(\bar{X} - \mu) \sum_{i=1}^n (X_i - \mu) + \sum_{i=1}^n (\bar{X} - \mu)^2\]</div>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n (X_i - \mu) = \sum X_i - n\mu = n\bar{X} - n\mu = n(\bar{X} - \mu)\]</div>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n (\bar{X} - \mu)^2 = n(\bar{X} - \mu)^2\]</div>
<p>Substitute these back:</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n (X_i - \bar{X})^2 = \sum_{i=1}^n (X_i - \mu)^2 - 2(\bar{X} - \mu) [n(\bar{X} - \mu)] + n(\bar{X} - \mu)^2\]</div>
<div class="math notranslate nohighlight">
\[= \sum_{i=1}^n (X_i - \mu)^2 - 2n(\bar{X} - \mu)^2 + n(\bar{X} - \mu)^2\]</div>
<div class="math notranslate nohighlight">
\[= \sum_{i=1}^n (X_i - \mu)^2 - n(\bar{X} - \mu)^2\]</div>
<p>Now, take the expectation of this expression:</p>
<div class="math notranslate nohighlight">
\[E\left[ \sum_{i=1}^n (X_i - \bar{X})^2 \right] = E\left[ \sum_{i=1}^n (X_i - \mu)^2 - n(\bar{X} - \mu)^2 \right]\]</div>
<p>By linearity of expectation:</p>
<div class="math notranslate nohighlight">
\[= \sum_{i=1}^n E[(X_i - \mu)^2] - n E[(\bar{X} - \mu)^2]\]</div>
<p>We know <span class="math notranslate nohighlight">\(E[(X_i - \mu)^2] = \sigma^2\)</span> (by definition of population variance) and <span class="math notranslate nohighlight">\(E[(\bar{X} - \mu)^2] = Var(\bar{X}) = \frac{\sigma^2}{n}\)</span>. Thus</p>
<div class="math notranslate nohighlight">
\[E\left[ \sum_{i=1}^n (X_i - \bar{X})^2 \right] = \sum_{i=1}^n \sigma^2 - n \frac{\sigma^2}{n}\]</div>
<div class="math notranslate nohighlight">
\[= n\sigma^2 - \sigma^2 = (n-1)\sigma^2\]</div>
<p>Therefore, the expected value of the numerator of the sample variance is <span class="math notranslate nohighlight">\((n-1)\sigma^2\)</span>. Therefore,</p>
<div class="math notranslate nohighlight">
\[E[s^2_{\text{biased}}] = \frac{1}{n} E\left[ \sum_{i=1}^n (X_i - \bar{X})^2 \right] = \frac{n-1}{n} \sigma^2\]</div>
<p>So, the expected value <span class="math notranslate nohighlight">\(E[s^2_{\text{biased}}]\)</span> is not equal to the population variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>Remember that we want <span class="math notranslate nohighlight">\(E[s^2] = \sigma^2\)</span>. Therefore, we will have for</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{\sum_{i=1}^n (X_i - \bar{X})^2}{n-1}\]</div>
<div class="math notranslate nohighlight">
\[E[s^2] = E\left[ \frac{\sum_{i=1}^n (X_i - \bar{X})^2}{n-1} \right] = \frac{1}{n-1} E\left[ \sum_{i=1}^n (X_i - \bar{X})^2 \right]\]</div>
<div class="math notranslate nohighlight">
\[= \frac{1}{n-1} (n-1)\sigma^2 = \sigma^2\]</div>
<p>This mathematical derivation shows that by dividing by <span class="math notranslate nohighlight">\(n-1\)</span>, the sample variance (<span class="math notranslate nohighlight">\(s^2\)</span>) indeed becomes an unbiased estimator of the population variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>). If we had divided by <span class="math notranslate nohighlight">\(n\)</span>, the expected value would have been <span class="math notranslate nohighlight">\(\frac{n-1}{n}\sigma^2\)</span>, confirming the downward bias.</p>
</section>
</section>
<hr class="docutils" />
<section id="additional-materials">
<h2>Additional Materials<a class="headerlink" href="#additional-materials" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Variance">https://en.wikipedia.org/wiki/Variance</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Covariance">https://en.wikipedia.org/wiki/Covariance</a></p></li>
<li><p><a class="reference external" href="https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/descriptive-statistics/variance-and-standard-deviation.html">https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/descriptive-statistics/variance-and-standard-deviation.html</a></p></li>
<li><p><a class="reference external" href="https://mathsathome.com/variance/">https://mathsathome.com/variance/</a></p></li>
<li><p><a class="reference external" href="https://online.stat.psu.edu/stat505/book/export/html/643">https://online.stat.psu.edu/stat505/book/export/html/643</a></p></li>
<li><p><a class="reference external" href="https://online.stat.psu.edu/stat505/book/export/html/653">https://online.stat.psu.edu/stat505/book/export/html/653</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Covariance_matrix">https://en.wikipedia.org/wiki/Covariance_matrix</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Mean">https://en.wikipedia.org/wiki/Mean</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Standard_deviation">https://en.wikipedia.org/wiki/Standard_deviation</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Sample_mean_and_covariance">https://en.wikipedia.org/wiki/Sample_mean_and_covariance</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_population">https://en.wikipedia.org/wiki/Statistical_population</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Bessel%27s_correction">https://en.wikipedia.org/wiki/Bessel’s_correction</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)">https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">https://en.wikipedia.org/wiki/Pearson_correlation_coefficient</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./math"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ordinary-least-squares-code.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Ordinary Least Squares (OLS) Regression - Code Example</p>
      </div>
    </a>
    <a class="right-next"
       href="variance-covariance-code.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Variance and Covariance - Code Examples</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-variance-and-covariance-matter-a-foundation-for-data-understanding">Why Variance and Covariance Matter: A Foundation for Data Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-average">Mean (Average)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-calculating-the-mean">Example: Calculating the Mean</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-deviation">Standard Deviation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#population-standard-deviation-sigma">Population Standard Deviation (<span class="math notranslate nohighlight">\(\sigma\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-standard-deviation-s">Sample Standard Deviation (<span class="math notranslate nohighlight">\(s\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#population-variance-sigma-2">Population Variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-variance-s-2">Sample Variance (<span class="math notranslate nohighlight">\(s^2\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-calculating-variance-and-standard-deviation">Example: Calculating Variance and Standard Deviation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-calculation-using-definitional-formula">Step-by-step Calculation using Definitional Formula:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#population-variance-sigma-2-and-standard-deviation-sigma">Population Variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>) and Standard Deviation (<span class="math notranslate nohighlight">\(\sigma\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-variance-s-2-and-standard-deviation-s">Sample Variance (<span class="math notranslate nohighlight">\(s^2\)</span>) and Standard Deviation (<span class="math notranslate nohighlight">\(s\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-and-limitations-of-variance-and-standard-deviation">Interpretation and Limitations of Variance and Standard Deviation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-variance">Properties of Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#population-covariance-text-cov-x-y-or-sigma-xy">Population Covariance (<span class="math notranslate nohighlight">\(\text{Cov}(X, Y)\)</span> or <span class="math notranslate nohighlight">\(\sigma_{XY}\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-covariance-s-xy">Sample Covariance (<span class="math notranslate nohighlight">\(s_{XY}\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-coefficient-pearson-s-r">Correlation Coefficient (Pearson’s r)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#population-correlation-coefficient-rho">Population Correlation Coefficient (<span class="math notranslate nohighlight">\(\rho\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-correlation-coefficient-r">Sample Correlation Coefficient (<span class="math notranslate nohighlight">\(r\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-and-limitations-of-covariance-and-correlation">Interpretation and Limitations of Covariance and Correlation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-covariance">Properties of Covariance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bessel-s-correction">Bessel’s Correction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#degrees-of-freedom-explanation">Degrees of Freedom Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-proof-sketch-for-variance">Mathematical Proof Sketch (for Variance)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-materials">Additional Materials</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Vladyslav Yakovliev (Ukraine)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>