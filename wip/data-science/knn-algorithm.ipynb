{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd914bf",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Algorithm (preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3c77e",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a non-parametric, supervised machine learning algorithm used for both classification and regression tasks. It's conceptually simple yet powerful, often serving as a good baseline model.\n",
    "\n",
    "## How KNN Works\n",
    "\n",
    "The core idea behind KNN is that similar things are near to each other. When you want to classify a new data point, KNN looks at its 'k' nearest neighbors (data points with known classifications) in the feature space and assigns the new point the class that is most common among those 'k' neighbors.\n",
    "\n",
    "Let's break down the steps:\n",
    "\n",
    "1.  **Choose the Number of Neighbors (K):** This is the most crucial parameter. You decide how many neighbors to consider.\n",
    "2.  **Calculate Distance:** For a new data point you want to classify, KNN calculates its distance to every other point in the training dataset. Common distance metrics include:\n",
    "    * **Euclidean Distance:** The most common. For two points $(x_1, y_1)$ and $(x_2, y_2)$ in 2D, it's $\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$. In N-dimensions, it generalizes to $\\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}$.\n",
    "    * **Manhattan Distance (Taxicab Distance):** Sum of the absolute differences of their Cartesian coordinates. For two points $(x_1, y_1)$ and $(x_2, y_2)$, it's $|x_2 - x_1| + |y_2 - y_1|$.\n",
    "    * **Minkowski Distance:** A generalization of Euclidean and Manhattan distances.\n",
    "3.  **Find K Nearest Neighbors:** After calculating all distances, the algorithm selects the 'k' data points from the training set that are closest to the new data point.\n",
    "4.  **Vote for Classification (for Classification problems):**\n",
    "    * For classification tasks, the algorithm looks at the classes of these 'k' nearest neighbors.\n",
    "    * The new data point is assigned the class that is most frequent among these 'k' neighbors. In case of a tie, different implementations handle it differently (e.g., choose the class of the closest neighbor, or random selection).\n",
    "5.  **Average for Regression (for Regression problems):**\n",
    "    * For regression tasks, the algorithm takes the average (or weighted average) of the target values of the 'k' nearest neighbors.\n",
    "\n",
    "## Simple Examples\n",
    "\n",
    "Let's illustrate with a very basic 2D classification example.\n",
    "\n",
    "Imagine we have two classes: \"Apples\" (A) and \"Oranges\" (O), based on two features: \"Sweetness\" and \"Crunchiness\".\n",
    "\n",
    "**Training Data:**\n",
    "\n",
    "| Sweetness | Crunchiness | Fruit  |\n",
    "| :-------- | :---------- | :----- |\n",
    "| 7         | 3           | Apple  |\n",
    "| 6         | 4           | Apple  |\n",
    "| 2         | 8           | Orange |\n",
    "| 3         | 7           | Orange |\n",
    "| 5         | 5           | Apple  |\n",
    "\n",
    "**New Data Point to Classify:** (Sweetness: 4, Crunchiness: 6) - Let's call this \"Mystery Fruit\".\n",
    "\n",
    "Let's set $K=3$.\n",
    "\n",
    "**Step 1: Calculate Euclidean Distances from Mystery Fruit (4, 6) to all training points:**\n",
    "\n",
    "* **To (7, 3) - Apple:** $\\sqrt{(7-4)^2 + (3-6)^2} = \\sqrt{3^2 + (-3)^2} = \\sqrt{9 + 9} = \\sqrt{18} \\approx 4.24$\n",
    "* **To (6, 4) - Apple:** $\\sqrt{(6-4)^2 + (4-6)^2} = \\sqrt{2^2 + (-2)^2} = \\sqrt{4 + 4} = \\sqrt{8} \\approx 2.83$\n",
    "* **To (2, 8) - Orange:** $\\sqrt{(2-4)^2 + (8-6)^2} = \\sqrt{(-2)^2 + 2^2} = \\sqrt{4 + 4} = \\sqrt{8} \\approx 2.83$\n",
    "* **To (3, 7) - Orange:** $\\sqrt{(3-4)^2 + (7-6)^2} = \\sqrt{(-1)^2 + 1^2} = \\sqrt{1 + 1} = \\sqrt{2} \\approx 1.41$\n",
    "* **To (5, 5) - Apple:** $\\sqrt{(5-4)^2 + (5-6)^2} = \\sqrt{1^2 + (-1)^2} = \\sqrt{1 + 1} = \\sqrt{2} \\approx 1.41$\n",
    "\n",
    "**Step 2: Find K=3 Nearest Neighbors (smallest distances):**\n",
    "\n",
    "1.  (3, 7) - Orange (Distance: 1.41)\n",
    "2.  (5, 5) - Apple (Distance: 1.41)\n",
    "3.  (2, 8) - Orange (Distance: 2.83) *OR* (6, 4) - Apple (Distance: 2.83) - Let's pick (2,8) for now.\n",
    "\n",
    "**Step 3: Vote for Classification:**\n",
    "\n",
    "The 3 nearest neighbors are:\n",
    "* Orange\n",
    "* Apple\n",
    "* Orange\n",
    "\n",
    "Two of the three neighbors are \"Orange\". Therefore, the \"Mystery Fruit\" is classified as an **Orange**.\n",
    "\n",
    "## KNN in Python (using scikit-learn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example 1: Simple Classification ---\n",
    "\n",
    "# Data: Sweetness, Crunchiness, Fruit (0 for Apple, 1 for Orange)\n",
    "X_train = np.array([\n",
    "    [7, 3],\n",
    "    [6, 4],\n",
    "    [2, 8],\n",
    "    [3, 7],\n",
    "    [5, 5]\n",
    "])\n",
    "y_train = np.array([0, 0, 1, 1, 0]) # 0: Apple, 1: Orange\n",
    "\n",
    "# New data point to classify\n",
    "mystery_fruit = np.array([[4, 6]])\n",
    "\n",
    "# Create a KNN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = knn.predict(mystery_fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb339832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Example 1: Simple Classification ---\")\n",
    "print(f\"Mystery Fruit (Sweetness: 4, Crunchiness: 6) is predicted to be: {'Orange' if prediction[0] == 1 else 'Apple'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualizing the decision boundary (more complex but illustrative) ---\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFaa']) # Light red/green\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00']) # Dark red/green\n",
    "\n",
    "# Plot the training data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold, edgecolor='k', s=100)\n",
    "plt.scatter(mystery_fruit[:, 0], mystery_fruit[:, 1], c='blue', marker='X', s=200, label='Mystery Fruit')\n",
    "plt.xlabel('Sweetness')\n",
    "plt.ylabel('Crunchiness')\n",
    "plt.title('KNN Classification of Fruits')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Create a mesh grid to plot decision boundaries\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6502f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example 2: KNN for Regression ---\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Let's say we want to predict a house price (target) based on size (feature)\n",
    "# Small dataset for demonstration\n",
    "house_sizes = np.array([\n",
    "    [50],  # 50 sq meters\n",
    "    [70],\n",
    "    [80],\n",
    "    [100],\n",
    "    [120],\n",
    "    [150]\n",
    "])\n",
    "house_prices = np.array([\n",
    "    100000, # 100k\n",
    "    130000,\n",
    "    150000,\n",
    "    180000,\n",
    "    200000,\n",
    "    250000\n",
    "])\n",
    "\n",
    "# New house size to predict price for\n",
    "new_house_size = np.array([[90]]) # 90 sq meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNN regressor with k=3\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=2)\n",
    "\n",
    "# Train the model\n",
    "knn_reg.fit(house_sizes, house_prices)\n",
    "\n",
    "# Make a prediction\n",
    "predicted_price = knn_reg.predict(new_house_size)\n",
    "\n",
    "print(f\"\\n--- Example 2: Simple Regression ---\")\n",
    "print(f\"Predicted price for a {new_house_size[0][0]} sq meter house: ${predicted_price[0]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for regression\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(house_sizes, house_prices, color='red', s=100, label='Training Data')\n",
    "plt.scatter(new_house_size, predicted_price, color='blue', marker='X', s=200, label='Predicted Point')\n",
    "plt.plot(house_sizes, knn_reg.predict(house_sizes), color='green', linestyle='--', label='KNN Regression Line (interpolated)')\n",
    "plt.xlabel('House Size (sq meters)')\n",
    "plt.ylabel('House Price ($)')\n",
    "plt.title('KNN Regression for House Prices')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d3917",
   "metadata": {},
   "source": [
    "## Key Considerations and Hyperparameters\n",
    "\n",
    "1.  **Choice of K:**\n",
    "    * **Small K (e.g., K=1):** The model becomes sensitive to noise in the training data and can lead to overfitting. The decision boundary will be more complex and irregular.\n",
    "    * **Large K:** The model becomes smoother and less sensitive to noise, but it might oversimplify the decision boundary, leading to underfitting. It might also cause the model to miss subtle patterns.\n",
    "    * **Odd K for Classification:** For binary classification, choosing an odd K avoids ties in voting.\n",
    "    * **How to choose K:** Cross-validation is the most common method to find an optimal K.\n",
    "\n",
    "2.  **Distance Metric:**\n",
    "    * Euclidean distance is the most common.\n",
    "    * Manhattan distance is useful when the difference between features is more important than their squared difference (e.g., grid-like movements).\n",
    "    * Minkowski distance allows you to experiment with different powers ($p=1$ for Manhattan, $p=2$ for Euclidean).\n",
    "\n",
    "3.  **Feature Scaling:**\n",
    "    * **Crucial for KNN!** KNN is a distance-based algorithm. If features have different scales (e.g., one feature ranges from 0-1000 and another from 0-1), the feature with the larger scale will disproportionately influence the distance calculation.\n",
    "    * You should normalize or standardize your features (e.g., using `StandardScaler` or `MinMaxScaler` from scikit-learn) before applying KNN.\n",
    "\n",
    "4.  **Curse of Dimensionality:**\n",
    "    * As the number of features (dimensions) increases, the concept of \"distance\" becomes less meaningful. Data points in high-dimensional space tend to be equidistant from each other, making it difficult for KNN to find true nearest neighbors.\n",
    "    * KNN performance can degrade significantly in very high-dimensional datasets. Dimensionality reduction techniques (like PCA) can sometimes help.\n",
    "\n",
    "### Advantages of KNN\n",
    "\n",
    "* **Simple to Understand and Implement:** Its intuitive nature makes it easy to grasp.\n",
    "* **No Training Phase (Lazy Learner):** KNN is a \"lazy learner\" because it doesn't build a model explicitly during the training phase. All it does is store the training data. The computation happens only when a prediction is requested.\n",
    "* **Non-parametric:** It makes no assumptions about the underlying data distribution.\n",
    "* **Adaptable:** Can be used for both classification and regression.\n",
    "\n",
    "### Disadvantages of KNN\n",
    "\n",
    "* **Computationally Expensive at Prediction Time:** For large datasets, finding the 'k' nearest neighbors for each new data point can be very slow, as it requires calculating distances to all training points.\n",
    "* **Memory Intensive:** Stores the entire training dataset in memory.\n",
    "* **Sensitive to Irrelevant Features:** Irrelevant features can negatively impact performance because they contribute to the distance calculation.\n",
    "* **Sensitive to Data Scaling:** As mentioned, features with larger scales can dominate the distance calculations.\n",
    "* **Poor Performance in High Dimensions (Curse of Dimensionality):** Performance degrades significantly with many features.\n",
    "\n",
    "### When to Use KNN\n",
    "\n",
    "KNN is often a good choice when:\n",
    "\n",
    "* Your dataset is relatively small.\n",
    "* The data is clean and doesn't have many noisy outliers.\n",
    "* You need a simple, interpretable model.\n",
    "* You don't have time to build more complex models and need a quick baseline.\n",
    "\n",
    "In summary, KNN is a fundamental and straightforward algorithm that provides a solid foundation for understanding instance-based learning. While it has limitations, especially with large or high-dimensional datasets, its simplicity and effectiveness make it a valuable tool in many machine learning scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
